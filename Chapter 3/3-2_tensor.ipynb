{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的每一对方括号 **<u>向内</u>** 是一个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的赋值、索引与形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(3)   # 创建大小为3的一维张量, 用1.0填充\n",
    "print(b)\n",
    "\n",
    "print(b[1])\n",
    "print(float(b[1]))  # 直接索引元素得到的还是张量, 需要转化成浮点型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[2] = 2.0\n",
    "print(b)\n",
    "b[2] = torch.tensor(3.)\n",
    "print(b)\n",
    "b[2] = torch.tensor([4.])     # 把 python list 转化成张量\n",
    "# ERROR: b[2] = torch.tensor([4., 5.])\n",
    "print(b)\n",
    "\n",
    "# 三种赋值方式等价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复习python列表索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_list = list(range(6))\n",
    "print(some_list[:])\n",
    "print(some_list[1:4])\n",
    "print(some_list[:4])\n",
    "print(some_list[:-1])\n",
    "print(some_list[1:5:2])   # 1到5, 左闭右开, 步长为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 2.0], [6.0, 3.0]])\n",
    "print('Get shape: ', points.shape)   # 3行, 2列\n",
    "\n",
    "print(points[0,1])   # 双索引, 0号列表里的1号元素, 从外往里数方括号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量维度的命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给维度命名\n",
    "weights_unnamed = torch.tensor([[[0.2], [0.7]], [[0.3], [0.6]]])\n",
    "weights_named = torch.tensor([[[0.2], [0.7]], [[0.3], [0.6]]], names = ['channels', 'rows', None])\n",
    "#print(weights_named.shape, weights_named)\n",
    "\n",
    "# 重命名\n",
    "weights_A = weights_unnamed.refine_names(..., 'cha', 'ro')\n",
    "#print(weights_A.shape, weights_A)\n",
    "\n",
    "# 移除名称\n",
    "weights_unnamed = weights_A.rename(None)\n",
    "\n",
    "# 重命名\n",
    "# ERROR: weights_B = weights_named.refine_names(..., 'cha', 'ro')\n",
    "weights_B = weights_named.refine_names(..., 'rows', 'columns')\n",
    "print(weights_B.shape, weights_B)\n",
    "weights_C = weights_B[1][0]\n",
    "print(weights_C.shape, weights_C)\n",
    "\n",
    "# 如果本来没有名称, 三点表示在末尾处赋予名称\n",
    "# 如果本来有名称, 三点后的第一个字符串应当是已经存在的名称, 改名从这里开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_aligned = weights_C.align_as(weights_B)\n",
    "print(weights_aligned.shape, weights_aligned)\n",
    "weights_mul = (weights_aligned * weights_B)\n",
    "print(weights_mul.shape, weights_mul)\n",
    "weights_sum = weights_mul.sum('rows')\n",
    "print(weights_sum.shape, weights_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量元素的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python 中数字是对象, 占用内存更多. PyTorch 和 NumPy 等库的数据结构更高效. \n",
    "\n",
    "对于张量构造函数, 使用 dtype 参数指定数据类型. 分为: torch.float32 / torch.float ; torch.float64 / torch.double ; torch.float16 / torch.half ; torch.int8 ; torch.uint8 ; torch.int16 / torch.short ; torch.int32 / torch.int ; torch.int64 / torch.long ; torch.bool\n",
    "默认为 float32, 切换至 double 不会提高精度, 但是占用更多内存. half 只在GPU中存在, 同样没有明显精度改变, 但可以减少内存占用.\n",
    "运算时如果类型不同, 会自动向较大类型转换.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.ones(5, 2, dtype=torch.double)\n",
    "short_points = torch.ones(7, 2, dtype=torch.short)\n",
    "print(short_points.dtype)\n",
    "\n",
    "double_points = short_points.to(torch.double)\n",
    "short_points = double_points.short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以不同方式索引张量时, 调用的是同一个存储, 赋到新变量时没有创建新存储. \n",
    "以一维数组形式存储. 可以用 torch.storage() 方法索引, 共享内存. \n",
    "\n",
    "以下划线结尾的方法, 如 zeros_(), 会改变母张量. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,2)\n",
    "#print(a)\n",
    "b = a.unsqueeze(1)\n",
    "b = a.unsqueeze_(1)  # a will change\n",
    "\n",
    "b.zero_()      # this will cause a to be zero\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "使用 torch.clone() 方法可以复制出一块独立的存储空间, 避免修改后的张量与原张量发生关联.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 连续性\n",
    "\n",
    "连续张量, 是指从前往后逐个访问元素, 得到的序列等同于它实际在存储中的序列. 例如 transpose() 方法就会使张量的连续性被破坏. 很多方法只对连续张量有效, 所以有时要调用 contiguous() 方法调整存储的顺序和索引步长, 这一过程中张量不变."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.randn(3,1,5)\n",
    "print(points.is_contiguous())\n",
    "a = points.unsqueeze(1)\n",
    "print(a.is_contiguous())\n",
    "b = a.squeeze()\n",
    "print(b.is_contiguous())\n",
    "c = b.transpose(0,1)\n",
    "print(c.is_contiguous())\n",
    "\n",
    "print(c.storage())\n",
    "d = c.contiguous()\n",
    "print(d.is_contiguous(), d.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归约操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 升维 UNSQUEEZE\n",
    "\n",
    "加下划线则改变母张量. 共享内存."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.unsqueeze(\n",
    "\tinput, \n",
    "\tdim, \t     # dim 从0算起, 将要扩增的维度. 将会在这个维度之前增加一个数量为1的维度.\n",
    "\tout = None)\n",
    "如果dim为负, 则将会被转化dim+input.dim()+1, 即从后往前数.\n",
    "\n",
    "unsqueeze_ 和 unsqueeze 实现一样的功能, \n",
    "区别在于 unsqueeze 不会对使用 unsqueeze 的 tensor 进行改变, 想要获取 unsqueeze 后的值必须赋予个新值, \n",
    "unsqueeze_ 则会对自己改变. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)        # Shape (channels, rows, columns)\n",
    "print(img_t.shape, img_t)\n",
    "batch_t = torch.randn(2, 3, 5, 5)   # Shape (batch, channels, rows, columns <- 这个叫作第四个维度的数量)\n",
    "#print('batch_t: ',batch_t)\n",
    "\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "# 消除倒数第三个维度. 假设这个维度的数量是3, 把这个维度上的每3个值叠起来用一个平均值代替. \n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "print(img_gray_naive.shape)\n",
    "# print(batch_gray_naive.shape, batch_gray_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 清理维度 SQUEEZE\n",
    "\n",
    "加下划线则改变母张量. 共享内存."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.squeeze(input, dim, out = None)\n",
    "For example, if input is of shape: (A, 1, B, C, 1, D) then the out tensor will be of shape: (A, B, C, D).\n",
    "\n",
    "When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A, 1, B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A, B).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.randn(3, 1, 2, 1, 2)\n",
    "g = f.squeeze()\n",
    "g.zero_()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维度求和 SUM\n",
    "\n",
    "复制内存."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.sum(input)\n",
    "打开所有维度, 把所有数求和, 输出单元素张量.\n",
    "\n",
    "torch.sum(input, dim, keepdim = False)\n",
    "把指定维度求和. keepdim = False 时对这个维度上的元素求和, 并消除这个维度, keepdim = True 时会保留这个维度.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.tensor([[[1], [2]], [[3], [4]]])\n",
    "print(k)\n",
    "j = k.sum()\n",
    "print(j)\n",
    "#j = k.squeeze(1)\n",
    "j = k.sum(0)\n",
    "print(j)\n",
    "j = k.sum(0, keepdim = True)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 2)\n",
    "b = a.sum(1)\n",
    "b.zero_()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维度转置 TRANSPOSE\n",
    "\n",
    "只改变索引机制. 共享存储."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "假设有形状为(a,b,c)的张量T. 访问 T[0] 时, 系统会数出存储区域的前 b*c 个元素.\n",
    "维度转置, 是交换两个维度的数量, 而不移动元素.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单二维转置 torch.t() 相当于把行列互换\n",
    "\n",
    "c = torch.tensor([[4.0, 1.0], [5.0, 2.0], [6.0, 3.0]])\n",
    "print(c.storage(), c)\n",
    "d = c.t()\n",
    "d[0,0] = 0.0\n",
    "print(d.storage(), d)\n",
    "print(c.storage(), c)       # no change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多维转置\n",
    "\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6]]])\n",
    "print(a.shape)\n",
    "b = a.transpose(0, 2)\n",
    "print(b.shape, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐点操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 点乘\n",
    "```\n",
    "张量点乘, 对于某个维度:\n",
    "若数量都不是1, 则必须数量相等才能相乘, 计算方法是: i位对i位相乘\n",
    "若数量有一个是1, 则可以相乘, 计算方法是: 多数量的每一个值分别去乘单数量的值, 类似乘法分配律\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)\n",
    "batch_t = torch.randn(2, 3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "# 一般对维度数相同的张量进行乘法运算, 可能需要提前对张量进行升维/降维. 若维度不同, 会自动在较短张量最外面补一维.\n",
    "\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)  # 在最后(即最小维度)升两维\n",
    "print(unsqueezed_weights.shape, unsqueezed_weights)\n",
    "\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "print(img_weights.shape, batch_t.shape, batch_weights.shape)\n",
    "img_gray_weighted = img_weights.sum(-3)     # 相当于squeeze\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "print(img_weights.shape, img_gray_weighted.shape)\n",
    "#batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor multiplication test\n",
    "\n",
    "def mult(A, B):\n",
    "    print(A, '<TIMES>', B, ' = ', A * B)\n",
    "\n",
    "x = torch.tensor([3, 5, 7, 9])\n",
    "# ERROR: x = torch.tensor([3, 5, 7])\n",
    "y = torch.tensor([13, 17, 19, 23])\n",
    "mult(x, y)\n",
    "\n",
    "x = torch.tensor([3])\n",
    "y = torch.tensor([13, 17, 19, 23])\n",
    "mult(x, y)\n",
    "\n",
    "x = torch.tensor([[3]])\n",
    "x = torch.tensor([[3], [27]])\n",
    "# ERROR: x = torch.tensor([[3], [27], [99]])\n",
    "y = torch.tensor([[13, 17, 19, 23], [1, 5, 7, 11]])\n",
    "mult(x, y)\n",
    "\n",
    "x = torch.tensor([[3, 4, 1, 2], [5, 6, 7, 8]])\n",
    "y = torch.tensor([[10, 11, 12, 13], [14, 15, 16, 17]])\n",
    "mult(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('disco-diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d154572db74bf447d8ef00b8088999b75aecc8ee0cff4dd3dcc88c51a55f698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

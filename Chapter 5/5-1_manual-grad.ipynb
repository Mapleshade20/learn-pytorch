{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorded temperature data, c - celsius, u - unknown\n",
    "t_cel = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_cel = torch.tensor(t_cel)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear model\n",
    "def model(t_u, w, b):\n",
    "    t_est = w * t_u + b\n",
    "    return t_est\n",
    "\n",
    "# use square loss function\n",
    "def loss_fn(t_cel, t_est):\n",
    "    sqr_dif = (t_est - t_cel)**2\n",
    "    return sqr_dif.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_est = model(t_u, w, b)\n",
    "loss = loss_fn(t_cel, t_est)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2964,   82.6000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain rule of gradient\n",
    "def dloss_model(t_cel, t_est):\n",
    "    dsqr_dif = (t_est - t_cel) / t_est.size(0) * 2  # 均值的导数, 因为后面return [dloss_dw.sum(), dloss_db.sum()]\n",
    "    return dsqr_dif\n",
    "def dmodel_w(t_u, w, b):\n",
    "    return t_u\n",
    "def dmodel_b(t_u, w, b):\n",
    "    return 1.0\n",
    "\n",
    "def grad_fn(t_u, t_cel, t_est, w, b):\n",
    "    dloss_dmodel = dloss_model(t_cel, t_est)\n",
    "    dloss_dw = dloss_dmodel * dmodel_w(t_u, w, b)\n",
    "    dloss_db = dloss_dmodel * dmodel_b(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "grad_fn(t_u, t_cel, t_est, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代以适应模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_cel):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params   # input tensor[w, b]\n",
    "        t_est = model(t_u, w, b)\n",
    "        loss = loss_fn(t_cel, t_est)\n",
    "        grad = grad_fn(t_u, t_cel, t_est, w, b)\n",
    "        params -= learning_rate * grad\n",
    "        print(f'Epoch %d, Loss %f, Params {params}, Grad {grad}' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766, Params tensor([-0.3552, -0.0248]), Grad tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 1044.412231, Params tensor([ 0.6816, -0.0074]), Grad tensor([-3455.9453,   -57.8471])\n",
      "Epoch 3, Loss 623.328979, Params tensor([-0.1115, -0.0223]), Grad tensor([2643.7959,   49.5984])\n",
      "Epoch 4, Loss 376.882294, Params tensor([ 0.4953, -0.0125]), Grad tensor([-2022.6680,   -32.6006])\n",
      "Epoch 5, Loss 232.644302, Params tensor([ 0.0311, -0.0216]), Grad tensor([1547.3005,   30.2835])\n",
      "Epoch 6, Loss 148.225693, Params tensor([ 0.3862, -0.0163]), Grad tensor([-1183.8201,   -17.8248])\n",
      "Epoch 7, Loss 98.817238, Params tensor([ 0.1145, -0.0220]), Grad tensor([905.5598,  18.9791])\n",
      "Epoch 8, Loss 69.899025, Params tensor([ 0.3224, -0.0192]), Grad tensor([-692.8713,   -9.1771])\n",
      "Epoch 9, Loss 52.973038, Params tensor([ 0.1634, -0.0229]), Grad tensor([529.9708,  12.3628])\n",
      "Epoch 10, Loss 43.065723, Params tensor([ 0.2851, -0.0217]), Grad tensor([-405.5359,   -4.1161])\n",
      "Epoch 11, Loss 37.266159, Params tensor([ 0.1920, -0.0242]), Grad tensor([310.1516,   8.4904])\n",
      "Epoch 12, Loss 33.870735, Params tensor([ 0.2632, -0.0239]), Grad tensor([-237.3684,   -1.1542])\n",
      "Epoch 13, Loss 31.882368, Params tensor([ 0.2088, -0.0257]), Grad tensor([181.4989,   6.2239])\n",
      "Epoch 14, Loss 30.717512, Params tensor([ 0.2505, -0.0259]), Grad tensor([-138.9455,    0.5792])\n",
      "Epoch 15, Loss 30.034624, Params tensor([ 0.2186, -0.0274]), Grad tensor([106.2030,   4.8973])\n",
      "Epoch 16, Loss 29.633808, Params tensor([ 0.2430, -0.0279]), Grad tensor([-81.3423,   1.5936])\n",
      "Epoch 17, Loss 29.398085, Params tensor([ 0.2244, -0.0291]), Grad tensor([62.1350,  4.1207])\n",
      "Epoch 18, Loss 29.258989, Params tensor([ 0.2387, -0.0298]), Grad tensor([-47.6289,   2.1871])\n",
      "Epoch 19, Loss 29.176441, Params tensor([ 0.2278, -0.0309]), Grad tensor([36.3433,  3.6661])\n",
      "Epoch 20, Loss 29.126989, Params tensor([ 0.2361, -0.0316]), Grad tensor([-27.8976,   2.5343])\n",
      "Epoch 21, Loss 29.096901, Params tensor([ 0.2298, -0.0326]), Grad tensor([21.2485,  3.3999])\n",
      "Epoch 22, Loss 29.078163, Params tensor([ 0.2347, -0.0335]), Grad tensor([-16.3497,   2.7374])\n",
      "Epoch 23, Loss 29.066053, Params tensor([ 0.2309, -0.0344]), Grad tensor([12.4139,  3.2439])\n",
      "Epoch 24, Loss 29.057833, Params tensor([ 0.2338, -0.0353]), Grad tensor([-9.5910,  2.8562])\n",
      "Epoch 25, Loss 29.051880, Params tensor([ 0.2316, -0.0362]), Grad tensor([7.2433, 3.1525])\n",
      "Epoch 26, Loss 29.047260, Params tensor([ 0.2333, -0.0371]), Grad tensor([-5.6353,  2.9255])\n",
      "Epoch 27, Loss 29.043415, Params tensor([ 0.2321, -0.0380]), Grad tensor([4.2172, 3.0989])\n",
      "Epoch 28, Loss 29.040031, Params tensor([ 0.2331, -0.0389]), Grad tensor([-3.3203,  2.9660])\n",
      "Epoch 29, Loss 29.036909, Params tensor([ 0.2323, -0.0399]), Grad tensor([2.4462, 3.0674])\n",
      "Epoch 30, Loss 29.033949, Params tensor([ 0.2329, -0.0408]), Grad tensor([-1.9654,  2.9895])\n",
      "Epoch 31, Loss 29.031073, Params tensor([ 0.2325, -0.0417]), Grad tensor([1.4096, 3.0488])\n",
      "Epoch 32, Loss 29.028259, Params tensor([ 0.2328, -0.0426]), Grad tensor([-1.1723,  3.0032])\n",
      "Epoch 33, Loss 29.025471, Params tensor([ 0.2326, -0.0435]), Grad tensor([0.8028, 3.0378])\n",
      "Epoch 34, Loss 29.022705, Params tensor([ 0.2328, -0.0444]), Grad tensor([-0.7082,  3.0111])\n",
      "Epoch 35, Loss 29.019945, Params tensor([ 0.2327, -0.0453]), Grad tensor([0.4478, 3.0313])\n",
      "Epoch 36, Loss 29.017195, Params tensor([ 0.2328, -0.0462]), Grad tensor([-0.4366,  3.0155])\n",
      "Epoch 37, Loss 29.014446, Params tensor([ 0.2327, -0.0471]), Grad tensor([0.2402, 3.0273])\n",
      "Epoch 38, Loss 29.011707, Params tensor([ 0.2328, -0.0480]), Grad tensor([-0.2778,  3.0180])\n",
      "Epoch 39, Loss 29.008961, Params tensor([ 0.2328, -0.0489]), Grad tensor([0.1186, 3.0248])\n",
      "Epoch 40, Loss 29.006220, Params tensor([ 0.2328, -0.0498]), Grad tensor([-0.1848,  3.0193])\n",
      "Epoch 41, Loss 29.003487, Params tensor([ 0.2328, -0.0507]), Grad tensor([0.0475, 3.0233])\n",
      "Epoch 42, Loss 29.000744, Params tensor([ 0.2329, -0.0516]), Grad tensor([-0.1302,  3.0200])\n",
      "Epoch 43, Loss 28.998003, Params tensor([ 0.2329, -0.0525]), Grad tensor([0.0057, 3.0222])\n",
      "Epoch 44, Loss 28.995264, Params tensor([ 0.2329, -0.0534]), Grad tensor([-0.0983,  3.0202])\n",
      "Epoch 45, Loss 28.992525, Params tensor([ 0.2329, -0.0544]), Grad tensor([-0.0187,  3.0215])\n",
      "Epoch 46, Loss 28.989784, Params tensor([ 0.2329, -0.0553]), Grad tensor([-0.0797,  3.0202])\n",
      "Epoch 47, Loss 28.987047, Params tensor([ 0.2329, -0.0562]), Grad tensor([-0.0330,  3.0209])\n",
      "Epoch 48, Loss 28.984308, Params tensor([ 0.2330, -0.0571]), Grad tensor([-0.0686,  3.0201])\n",
      "Epoch 49, Loss 28.981573, Params tensor([ 0.2330, -0.0580]), Grad tensor([-0.0414,  3.0204])\n",
      "Epoch 50, Loss 28.978832, Params tensor([ 0.2330, -0.0589]), Grad tensor([-0.0622,  3.0199])\n",
      "Epoch 51, Loss 28.976099, Params tensor([ 0.2330, -0.0598]), Grad tensor([-0.0463,  3.0200])\n",
      "Epoch 52, Loss 28.973360, Params tensor([ 0.2330, -0.0607]), Grad tensor([-0.0585,  3.0196])\n",
      "Epoch 53, Loss 28.970625, Params tensor([ 0.2330, -0.0616]), Grad tensor([-0.0491,  3.0196])\n",
      "Epoch 54, Loss 28.967888, Params tensor([ 0.2331, -0.0625]), Grad tensor([-0.0563,  3.0194])\n",
      "Epoch 55, Loss 28.965151, Params tensor([ 0.2331, -0.0634]), Grad tensor([-0.0509,  3.0193])\n",
      "Epoch 56, Loss 28.962416, Params tensor([ 0.2331, -0.0643]), Grad tensor([-0.0550,  3.0191])\n",
      "Epoch 57, Loss 28.959686, Params tensor([ 0.2331, -0.0652]), Grad tensor([-0.0517,  3.0190])\n",
      "Epoch 58, Loss 28.956947, Params tensor([ 0.2331, -0.0661]), Grad tensor([-0.0543,  3.0188])\n",
      "Epoch 59, Loss 28.954212, Params tensor([ 0.2331, -0.0670]), Grad tensor([-0.0523,  3.0186])\n",
      "Epoch 60, Loss 28.951479, Params tensor([ 0.2331, -0.0679]), Grad tensor([-0.0538,  3.0185])\n",
      "Epoch 61, Loss 28.948742, Params tensor([ 0.2332, -0.0688]), Grad tensor([-0.0527,  3.0183])\n",
      "Epoch 62, Loss 28.946012, Params tensor([ 0.2332, -0.0698]), Grad tensor([-0.0535,  3.0181])\n",
      "Epoch 63, Loss 28.943275, Params tensor([ 0.2332, -0.0707]), Grad tensor([-0.0530,  3.0180])\n",
      "Epoch 64, Loss 28.940546, Params tensor([ 0.2332, -0.0716]), Grad tensor([-0.0533,  3.0178])\n",
      "Epoch 65, Loss 28.937813, Params tensor([ 0.2332, -0.0725]), Grad tensor([-0.0530,  3.0177])\n",
      "Epoch 66, Loss 28.935076, Params tensor([ 0.2332, -0.0734]), Grad tensor([-0.0533,  3.0175])\n",
      "Epoch 67, Loss 28.932348, Params tensor([ 0.2333, -0.0743]), Grad tensor([-0.0530,  3.0174])\n",
      "Epoch 68, Loss 28.929613, Params tensor([ 0.2333, -0.0752]), Grad tensor([-0.0532,  3.0172])\n",
      "Epoch 69, Loss 28.926882, Params tensor([ 0.2333, -0.0761]), Grad tensor([-0.0531,  3.0170])\n",
      "Epoch 70, Loss 28.924150, Params tensor([ 0.2333, -0.0770]), Grad tensor([-0.0532,  3.0169])\n",
      "Epoch 71, Loss 28.921419, Params tensor([ 0.2333, -0.0779]), Grad tensor([-0.0531,  3.0167])\n",
      "Epoch 72, Loss 28.918690, Params tensor([ 0.2333, -0.0788]), Grad tensor([-0.0532,  3.0166])\n",
      "Epoch 73, Loss 28.915956, Params tensor([ 0.2334, -0.0797]), Grad tensor([-0.0531,  3.0164])\n",
      "Epoch 74, Loss 28.913231, Params tensor([ 0.2334, -0.0806]), Grad tensor([-0.0532,  3.0162])\n",
      "Epoch 75, Loss 28.910498, Params tensor([ 0.2334, -0.0815]), Grad tensor([-0.0531,  3.0161])\n",
      "Epoch 76, Loss 28.907768, Params tensor([ 0.2334, -0.0824]), Grad tensor([-0.0531,  3.0159])\n",
      "Epoch 77, Loss 28.905035, Params tensor([ 0.2334, -0.0833]), Grad tensor([-0.0530,  3.0158])\n",
      "Epoch 78, Loss 28.902311, Params tensor([ 0.2334, -0.0842]), Grad tensor([-0.0532,  3.0156])\n",
      "Epoch 79, Loss 28.899580, Params tensor([ 0.2335, -0.0851]), Grad tensor([-0.0531,  3.0155])\n",
      "Epoch 80, Loss 28.896852, Params tensor([ 0.2335, -0.0860]), Grad tensor([-0.0531,  3.0153])\n",
      "Epoch 81, Loss 28.894121, Params tensor([ 0.2335, -0.0869]), Grad tensor([-0.0532,  3.0151])\n",
      "Epoch 82, Loss 28.891396, Params tensor([ 0.2335, -0.0879]), Grad tensor([-0.0530,  3.0150])\n",
      "Epoch 83, Loss 28.888666, Params tensor([ 0.2335, -0.0888]), Grad tensor([-0.0532,  3.0148])\n",
      "Epoch 84, Loss 28.885942, Params tensor([ 0.2335, -0.0897]), Grad tensor([-0.0531,  3.0147])\n",
      "Epoch 85, Loss 28.883211, Params tensor([ 0.2335, -0.0906]), Grad tensor([-0.0530,  3.0145])\n",
      "Epoch 86, Loss 28.880487, Params tensor([ 0.2336, -0.0915]), Grad tensor([-0.0532,  3.0143])\n",
      "Epoch 87, Loss 28.877758, Params tensor([ 0.2336, -0.0924]), Grad tensor([-0.0530,  3.0142])\n",
      "Epoch 88, Loss 28.875036, Params tensor([ 0.2336, -0.0933]), Grad tensor([-0.0532,  3.0140])\n",
      "Epoch 89, Loss 28.872307, Params tensor([ 0.2336, -0.0942]), Grad tensor([-0.0530,  3.0139])\n",
      "Epoch 90, Loss 28.869581, Params tensor([ 0.2336, -0.0951]), Grad tensor([-0.0531,  3.0137])\n",
      "Epoch 91, Loss 28.866854, Params tensor([ 0.2336, -0.0960]), Grad tensor([-0.0531,  3.0136])\n",
      "Epoch 92, Loss 28.864130, Params tensor([ 0.2337, -0.0969]), Grad tensor([-0.0531,  3.0134])\n",
      "Epoch 93, Loss 28.861403, Params tensor([ 0.2337, -0.0978]), Grad tensor([-0.0531,  3.0132])\n",
      "Epoch 94, Loss 28.858681, Params tensor([ 0.2337, -0.0987]), Grad tensor([-0.0532,  3.0131])\n",
      "Epoch 95, Loss 28.855955, Params tensor([ 0.2337, -0.0996]), Grad tensor([-0.0530,  3.0129])\n",
      "Epoch 96, Loss 28.853233, Params tensor([ 0.2337, -0.1005]), Grad tensor([-0.0532,  3.0128])\n",
      "Epoch 97, Loss 28.850506, Params tensor([ 0.2337, -0.1014]), Grad tensor([-0.0530,  3.0126])\n",
      "Epoch 98, Loss 28.847784, Params tensor([ 0.2338, -0.1023]), Grad tensor([-0.0531,  3.0124])\n",
      "Epoch 99, Loss 28.845057, Params tensor([ 0.2338, -0.1032]), Grad tensor([-0.0531,  3.0123])\n",
      "Epoch 100, Loss 28.842335, Params tensor([ 0.2338, -0.1041]), Grad tensor([-0.0530,  3.0121])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2338, -0.1041])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 3e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u, t_cel = t_cel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化输入\n",
    "\n",
    "目的是保证同一个学习率适用于每个参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342, Params tensor([3.3284, 0.3192]), Grad tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 63.488392, Params tensor([ 1.4492, -0.1044]), Grad tensor([62.6412, 14.1208])\n",
      "Epoch 3, Loss 52.070919, Params tensor([3.0175, 0.0814]), Grad tensor([-52.2767,  -6.1953])\n",
      "Epoch 4, Loss 44.319126, Params tensor([ 1.7608, -0.2313]), Grad tensor([41.8899, 10.4240])\n",
      "Epoch 5, Loss 39.029301, Params tensor([ 2.8187, -0.1347]), Grad tensor([-35.2638,  -3.2208])\n",
      "Epoch 6, Loss 35.393250, Params tensor([ 1.9799, -0.3726]), Grad tensor([27.9594,  7.9324])\n",
      "Epoch 7, Loss 32.868412, Params tensor([ 2.6951, -0.3356]), Grad tensor([-23.8398,  -1.2333])\n",
      "Epoch 8, Loss 31.090471, Params tensor([ 2.1369, -0.5231]), Grad tensor([18.6083,  6.2501])\n",
      "Epoch 9, Loss 29.814831, Params tensor([ 2.6219, -0.5259]), Grad tensor([-16.1682,   0.0916])\n",
      "Epoch 10, Loss 28.877258, Params tensor([ 2.2520, -0.6792]), Grad tensor([12.3318,  5.1112])\n",
      "Epoch 11, Loss 28.167492, Params tensor([ 2.5824, -0.7084]), Grad tensor([-11.0159,   0.9718])\n",
      "Epoch 12, Loss 27.611483, Params tensor([ 2.3388, -0.8385]), Grad tensor([8.1194, 4.3372])\n",
      "Epoch 13, Loss 27.159517, Params tensor([ 2.5655, -0.8851]), Grad tensor([-7.5552,  1.5535])\n",
      "Epoch 14, Loss 26.778198, Params tensor([ 2.4067, -0.9993]), Grad tensor([5.2929, 3.8084])\n",
      "Epoch 15, Loss 26.445110, Params tensor([ 2.5636, -1.0574]), Grad tensor([-5.2300,  1.9349])\n",
      "Epoch 16, Loss 26.145184, Params tensor([ 2.4617, -1.1607]), Grad tensor([3.3969, 3.4443])\n",
      "Epoch 17, Loss 25.868292, Params tensor([ 2.5717, -1.2262]), Grad tensor([-3.6673,  2.1819])\n",
      "Epoch 18, Loss 25.607647, Params tensor([ 2.5080, -1.3219]), Grad tensor([2.1254, 3.1907])\n",
      "Epoch 19, Loss 25.358654, Params tensor([ 2.5865, -1.3921]), Grad tensor([-2.6165,  2.3387])\n",
      "Epoch 20, Loss 25.118250, Params tensor([ 2.5483, -1.4824]), Grad tensor([1.2734, 3.0116])\n",
      "Epoch 21, Loss 24.884344, Params tensor([ 2.6055, -1.5555]), Grad tensor([-1.9095,  2.4351])\n",
      "Epoch 22, Loss 24.655550, Params tensor([ 2.5845, -1.6419]), Grad tensor([0.7029, 2.8825])\n",
      "Epoch 23, Loss 24.430910, Params tensor([ 2.6275, -1.7167]), Grad tensor([-1.4333,  2.4911])\n",
      "Epoch 24, Loss 24.209784, Params tensor([ 2.6178, -1.8003]), Grad tensor([0.3215, 2.7871])\n",
      "Epoch 25, Loss 23.991732, Params tensor([ 2.6512, -1.8759]), Grad tensor([-1.1120,  2.5199])\n",
      "Epoch 26, Loss 23.776451, Params tensor([ 2.6492, -1.9573]), Grad tensor([0.0669, 2.7143])\n",
      "Epoch 27, Loss 23.563726, Params tensor([ 2.6760, -2.0332]), Grad tensor([-0.8947,  2.5307])\n",
      "Epoch 28, Loss 23.353416, Params tensor([ 2.6791, -2.1129]), Grad tensor([-0.1025,  2.6569])\n",
      "Epoch 29, Loss 23.145412, Params tensor([ 2.7015, -2.1888]), Grad tensor([-0.7474,  2.5294])\n",
      "Epoch 30, Loss 22.939638, Params tensor([ 2.7079, -2.2671]), Grad tensor([-0.2148,  2.6099])\n",
      "Epoch 31, Loss 22.736027, Params tensor([ 2.7274, -2.3427]), Grad tensor([-0.6470,  2.5201])\n",
      "Epoch 32, Loss 22.534544, Params tensor([ 2.7360, -2.4198]), Grad tensor([-0.2886,  2.5699])\n",
      "Epoch 33, Loss 22.335136, Params tensor([ 2.7534, -2.4950]), Grad tensor([-0.5781,  2.5055])\n",
      "Epoch 34, Loss 22.137783, Params tensor([ 2.7635, -2.5710]), Grad tensor([-0.3368,  2.5348])\n",
      "Epoch 35, Loss 21.942450, Params tensor([ 2.7794, -2.6456]), Grad tensor([-0.5304,  2.4874])\n",
      "Epoch 36, Loss 21.749111, Params tensor([ 2.7904, -2.7207]), Grad tensor([-0.3676,  2.5030])\n",
      "Epoch 37, Loss 21.557747, Params tensor([ 2.8053, -2.7947]), Grad tensor([-0.4969,  2.4670])\n",
      "Epoch 38, Loss 21.368334, Params tensor([ 2.8169, -2.8690]), Grad tensor([-0.3869,  2.4735])\n",
      "Epoch 39, Loss 21.180845, Params tensor([ 2.8311, -2.9423]), Grad tensor([-0.4730,  2.4453])\n",
      "Epoch 40, Loss 20.995274, Params tensor([ 2.8431, -3.0157]), Grad tensor([-0.3984,  2.4456])\n",
      "Epoch 41, Loss 20.811579, Params tensor([ 2.8567, -3.0884]), Grad tensor([-0.4555,  2.4227])\n",
      "Epoch 42, Loss 20.629763, Params tensor([ 2.8689, -3.1609]), Grad tensor([-0.4048,  2.4189])\n",
      "Epoch 43, Loss 20.449785, Params tensor([ 2.8821, -3.2329]), Grad tensor([-0.4424,  2.3995])\n",
      "Epoch 44, Loss 20.271646, Params tensor([ 2.8944, -3.3047]), Grad tensor([-0.4076,  2.3931])\n",
      "Epoch 45, Loss 20.095320, Params tensor([ 2.9073, -3.3760]), Grad tensor([-0.4322,  2.3762])\n",
      "Epoch 46, Loss 19.920778, Params tensor([ 2.9196, -3.4470]), Grad tensor([-0.4081,  2.3679])\n",
      "Epoch 47, Loss 19.748016, Params tensor([ 2.9323, -3.5176]), Grad tensor([-0.4239,  2.3527])\n",
      "Epoch 48, Loss 19.577013, Params tensor([ 2.9445, -3.5879]), Grad tensor([-0.4071,  2.3433])\n",
      "Epoch 49, Loss 19.407743, Params tensor([ 2.9570, -3.6578]), Grad tensor([-0.4170,  2.3292])\n",
      "Epoch 50, Loss 19.240196, Params tensor([ 2.9692, -3.7274]), Grad tensor([-0.4051,  2.3191])\n",
      "Epoch 51, Loss 19.074356, Params tensor([ 2.9815, -3.7965]), Grad tensor([-0.4111,  2.3058])\n",
      "Epoch 52, Loss 18.910194, Params tensor([ 2.9936, -3.8654]), Grad tensor([-0.4024,  2.2953])\n",
      "Epoch 53, Loss 18.747709, Params tensor([ 3.0057, -3.9339]), Grad tensor([-0.4057,  2.2826])\n",
      "Epoch 54, Loss 18.586870, Params tensor([ 3.0177, -4.0020]), Grad tensor([-0.3992,  2.2718])\n",
      "Epoch 55, Loss 18.427673, Params tensor([ 3.0297, -4.0698]), Grad tensor([-0.4008,  2.2595])\n",
      "Epoch 56, Loss 18.270088, Params tensor([ 3.0416, -4.1373]), Grad tensor([-0.3958,  2.2485])\n",
      "Epoch 57, Loss 18.114107, Params tensor([ 3.0535, -4.2044]), Grad tensor([-0.3963,  2.2367])\n",
      "Epoch 58, Loss 17.959715, Params tensor([ 3.0653, -4.2711]), Grad tensor([-0.3922,  2.2256])\n",
      "Epoch 59, Loss 17.806890, Params tensor([ 3.0770, -4.3375]), Grad tensor([-0.3919,  2.2140])\n",
      "Epoch 60, Loss 17.655619, Params tensor([ 3.0887, -4.4036]), Grad tensor([-0.3885,  2.2029])\n",
      "Epoch 61, Loss 17.505886, Params tensor([ 3.1003, -4.4694]), Grad tensor([-0.3877,  2.1915])\n",
      "Epoch 62, Loss 17.357674, Params tensor([ 3.1119, -4.5348]), Grad tensor([-0.3848,  2.1805])\n",
      "Epoch 63, Loss 17.210966, Params tensor([ 3.1234, -4.5999]), Grad tensor([-0.3836,  2.1693])\n",
      "Epoch 64, Loss 17.065754, Params tensor([ 3.1348, -4.6646]), Grad tensor([-0.3810,  2.1583])\n",
      "Epoch 65, Loss 16.922016, Params tensor([ 3.1462, -4.7290]), Grad tensor([-0.3796,  2.1472])\n",
      "Epoch 66, Loss 16.779743, Params tensor([ 3.1575, -4.7931]), Grad tensor([-0.3772,  2.1364])\n",
      "Epoch 67, Loss 16.638914, Params tensor([ 3.1688, -4.8569]), Grad tensor([-0.3756,  2.1254])\n",
      "Epoch 68, Loss 16.499516, Params tensor([ 3.1800, -4.9203]), Grad tensor([-0.3734,  2.1146])\n",
      "Epoch 69, Loss 16.361542, Params tensor([ 3.1911, -4.9834]), Grad tensor([-0.3718,  2.1038])\n",
      "Epoch 70, Loss 16.224964, Params tensor([ 3.2022, -5.0462]), Grad tensor([-0.3697,  2.0931])\n",
      "Epoch 71, Loss 16.089771, Params tensor([ 3.2132, -5.1087]), Grad tensor([-0.3679,  2.0824])\n",
      "Epoch 72, Loss 15.955956, Params tensor([ 3.2242, -5.1709]), Grad tensor([-0.3659,  2.0718])\n",
      "Epoch 73, Loss 15.823505, Params tensor([ 3.2352, -5.2327]), Grad tensor([-0.3642,  2.0613])\n",
      "Epoch 74, Loss 15.692397, Params tensor([ 3.2460, -5.2942]), Grad tensor([-0.3622,  2.0508])\n",
      "Epoch 75, Loss 15.562622, Params tensor([ 3.2568, -5.3554]), Grad tensor([-0.3605,  2.0403])\n",
      "Epoch 76, Loss 15.434170, Params tensor([ 3.2676, -5.4163]), Grad tensor([-0.3585,  2.0299])\n",
      "Epoch 77, Loss 15.307022, Params tensor([ 3.2783, -5.4769]), Grad tensor([-0.3568,  2.0196])\n",
      "Epoch 78, Loss 15.181168, Params tensor([ 3.2889, -5.5372]), Grad tensor([-0.3549,  2.0093])\n",
      "Epoch 79, Loss 15.056588, Params tensor([ 3.2995, -5.5972]), Grad tensor([-0.3532,  1.9990])\n",
      "Epoch 80, Loss 14.933277, Params tensor([ 3.3101, -5.6568]), Grad tensor([-0.3513,  1.9888])\n",
      "Epoch 81, Loss 14.811221, Params tensor([ 3.3206, -5.7162]), Grad tensor([-0.3496,  1.9787])\n",
      "Epoch 82, Loss 14.690405, Params tensor([ 3.3310, -5.7753]), Grad tensor([-0.3477,  1.9686])\n",
      "Epoch 83, Loss 14.570819, Params tensor([ 3.3414, -5.8340]), Grad tensor([-0.3460,  1.9586])\n",
      "Epoch 84, Loss 14.452445, Params tensor([ 3.3517, -5.8925]), Grad tensor([-0.3442,  1.9486])\n",
      "Epoch 85, Loss 14.335278, Params tensor([ 3.3620, -5.9506]), Grad tensor([-0.3425,  1.9387])\n",
      "Epoch 86, Loss 14.219302, Params tensor([ 3.3722, -6.0085]), Grad tensor([-0.3407,  1.9288])\n",
      "Epoch 87, Loss 14.104504, Params tensor([ 3.3824, -6.0661]), Grad tensor([-0.3390,  1.9190])\n",
      "Epoch 88, Loss 13.990872, Params tensor([ 3.3925, -6.1233]), Grad tensor([-0.3373,  1.9092])\n",
      "Epoch 89, Loss 13.878399, Params tensor([ 3.4026, -6.1803]), Grad tensor([-0.3355,  1.8995])\n",
      "Epoch 90, Loss 13.767065, Params tensor([ 3.4126, -6.2370]), Grad tensor([-0.3338,  1.8898])\n",
      "Epoch 91, Loss 13.656866, Params tensor([ 3.4225, -6.2934]), Grad tensor([-0.3321,  1.8801])\n",
      "Epoch 92, Loss 13.547787, Params tensor([ 3.4324, -6.3495]), Grad tensor([-0.3304,  1.8706])\n",
      "Epoch 93, Loss 13.439814, Params tensor([ 3.4423, -6.4054]), Grad tensor([-0.3288,  1.8610])\n",
      "Epoch 94, Loss 13.332941, Params tensor([ 3.4521, -6.4609]), Grad tensor([-0.3271,  1.8516])\n",
      "Epoch 95, Loss 13.227152, Params tensor([ 3.4619, -6.5162]), Grad tensor([-0.3254,  1.8421])\n",
      "Epoch 96, Loss 13.122443, Params tensor([ 3.4716, -6.5712]), Grad tensor([-0.3238,  1.8327])\n",
      "Epoch 97, Loss 13.018798, Params tensor([ 3.4813, -6.6259]), Grad tensor([-0.3221,  1.8234])\n",
      "Epoch 98, Loss 12.916205, Params tensor([ 3.4909, -6.6803]), Grad tensor([-0.3204,  1.8141])\n",
      "Epoch 99, Loss 12.814655, Params tensor([ 3.5004, -6.7344]), Grad tensor([-0.3188,  1.8048])\n",
      "Epoch 100, Loss 12.714138, Params tensor([ 3.5100, -6.7883]), Grad tensor([-0.3172,  1.7957])\n",
      "Epoch 101, Loss 12.614642, Params tensor([ 3.5194, -6.8419]), Grad tensor([-0.3156,  1.7865])\n",
      "Epoch 102, Loss 12.516159, Params tensor([ 3.5288, -6.8952]), Grad tensor([-0.3140,  1.7774])\n",
      "Epoch 103, Loss 12.418674, Params tensor([ 3.5382, -6.9483]), Grad tensor([-0.3124,  1.7683])\n",
      "Epoch 104, Loss 12.322186, Params tensor([ 3.5475, -7.0010]), Grad tensor([-0.3108,  1.7593])\n",
      "Epoch 105, Loss 12.226673, Params tensor([ 3.5568, -7.0536]), Grad tensor([-0.3092,  1.7504])\n",
      "Epoch 106, Loss 12.132133, Params tensor([ 3.5660, -7.1058]), Grad tensor([-0.3076,  1.7414])\n",
      "Epoch 107, Loss 12.038555, Params tensor([ 3.5752, -7.1578]), Grad tensor([-0.3061,  1.7326])\n",
      "Epoch 108, Loss 11.945929, Params tensor([ 3.5844, -7.2095]), Grad tensor([-0.3045,  1.7237])\n",
      "Epoch 109, Loss 11.854245, Params tensor([ 3.5934, -7.2609]), Grad tensor([-0.3030,  1.7150])\n",
      "Epoch 110, Loss 11.763493, Params tensor([ 3.6025, -7.3121]), Grad tensor([-0.3014,  1.7062])\n",
      "Epoch 111, Loss 11.673663, Params tensor([ 3.6115, -7.3631]), Grad tensor([-0.2999,  1.6975])\n",
      "Epoch 112, Loss 11.584744, Params tensor([ 3.6204, -7.4137]), Grad tensor([-0.2983,  1.6889])\n",
      "Epoch 113, Loss 11.496731, Params tensor([ 3.6293, -7.4641]), Grad tensor([-0.2968,  1.6803])\n",
      "Epoch 114, Loss 11.409615, Params tensor([ 3.6382, -7.5143]), Grad tensor([-0.2953,  1.6717])\n",
      "Epoch 115, Loss 11.323382, Params tensor([ 3.6470, -7.5642]), Grad tensor([-0.2938,  1.6632])\n",
      "Epoch 116, Loss 11.238025, Params tensor([ 3.6558, -7.6138]), Grad tensor([-0.2923,  1.6547])\n",
      "Epoch 117, Loss 11.153535, Params tensor([ 3.6645, -7.6632]), Grad tensor([-0.2908,  1.6463])\n",
      "Epoch 118, Loss 11.069906, Params tensor([ 3.6732, -7.7123]), Grad tensor([-0.2893,  1.6379])\n",
      "Epoch 119, Loss 10.987130, Params tensor([ 3.6818, -7.7612]), Grad tensor([-0.2879,  1.6295])\n",
      "Epoch 120, Loss 10.905190, Params tensor([ 3.6904, -7.8099]), Grad tensor([-0.2864,  1.6212])\n",
      "Epoch 121, Loss 10.824085, Params tensor([ 3.6990, -7.8582]), Grad tensor([-0.2849,  1.6130])\n",
      "Epoch 122, Loss 10.743807, Params tensor([ 3.7075, -7.9064]), Grad tensor([-0.2835,  1.6047])\n",
      "Epoch 123, Loss 10.664343, Params tensor([ 3.7159, -7.9543]), Grad tensor([-0.2820,  1.5966])\n",
      "Epoch 124, Loss 10.585686, Params tensor([ 3.7243, -8.0019]), Grad tensor([-0.2806,  1.5884])\n",
      "Epoch 125, Loss 10.507832, Params tensor([ 3.7327, -8.0493]), Grad tensor([-0.2791,  1.5803])\n",
      "Epoch 126, Loss 10.430767, Params tensor([ 3.7411, -8.0965]), Grad tensor([-0.2778,  1.5723])\n",
      "Epoch 127, Loss 10.354488, Params tensor([ 3.7493, -8.1434]), Grad tensor([-0.2763,  1.5643])\n",
      "Epoch 128, Loss 10.278978, Params tensor([ 3.7576, -8.1901]), Grad tensor([-0.2749,  1.5563])\n",
      "Epoch 129, Loss 10.204241, Params tensor([ 3.7658, -8.2366]), Grad tensor([-0.2735,  1.5484])\n",
      "Epoch 130, Loss 10.130263, Params tensor([ 3.7740, -8.2828]), Grad tensor([-0.2721,  1.5405])\n",
      "Epoch 131, Loss 10.057037, Params tensor([ 3.7821, -8.3288]), Grad tensor([-0.2708,  1.5326])\n",
      "Epoch 132, Loss 9.984555, Params tensor([ 3.7902, -8.3745]), Grad tensor([-0.2694,  1.5248])\n",
      "Epoch 133, Loss 9.912810, Params tensor([ 3.7982, -8.4200]), Grad tensor([-0.2680,  1.5170])\n",
      "Epoch 134, Loss 9.841795, Params tensor([ 3.8062, -8.4653]), Grad tensor([-0.2666,  1.5093])\n",
      "Epoch 135, Loss 9.771501, Params tensor([ 3.8142, -8.5104]), Grad tensor([-0.2653,  1.5016])\n",
      "Epoch 136, Loss 9.701923, Params tensor([ 3.8221, -8.5552]), Grad tensor([-0.2639,  1.4940])\n",
      "Epoch 137, Loss 9.633053, Params tensor([ 3.8300, -8.5998]), Grad tensor([-0.2626,  1.4863])\n",
      "Epoch 138, Loss 9.564880, Params tensor([ 3.8378, -8.6441]), Grad tensor([-0.2612,  1.4788])\n",
      "Epoch 139, Loss 9.497405, Params tensor([ 3.8456, -8.6883]), Grad tensor([-0.2599,  1.4712])\n",
      "Epoch 140, Loss 9.430611, Params tensor([ 3.8533, -8.7322]), Grad tensor([-0.2586,  1.4637])\n",
      "Epoch 141, Loss 9.364498, Params tensor([ 3.8611, -8.7759]), Grad tensor([-0.2573,  1.4563])\n",
      "Epoch 142, Loss 9.299057, Params tensor([ 3.8687, -8.8193]), Grad tensor([-0.2559,  1.4489])\n",
      "Epoch 143, Loss 9.234282, Params tensor([ 3.8764, -8.8626]), Grad tensor([-0.2546,  1.4415])\n",
      "Epoch 144, Loss 9.170167, Params tensor([ 3.8840, -8.9056]), Grad tensor([-0.2534,  1.4341])\n",
      "Epoch 145, Loss 9.106698, Params tensor([ 3.8915, -8.9484]), Grad tensor([-0.2520,  1.4268])\n",
      "Epoch 146, Loss 9.043880, Params tensor([ 3.8991, -8.9910]), Grad tensor([-0.2508,  1.4195])\n",
      "Epoch 147, Loss 8.981699, Params tensor([ 3.9065, -9.0334]), Grad tensor([-0.2495,  1.4123])\n",
      "Epoch 148, Loss 8.920147, Params tensor([ 3.9140, -9.0755]), Grad tensor([-0.2482,  1.4051])\n",
      "Epoch 149, Loss 8.859223, Params tensor([ 3.9214, -9.1175]), Grad tensor([-0.2469,  1.3980])\n",
      "Epoch 150, Loss 8.798923, Params tensor([ 3.9288, -9.1592]), Grad tensor([-0.2457,  1.3908])\n",
      "Epoch 151, Loss 8.739231, Params tensor([ 3.9361, -9.2007]), Grad tensor([-0.2444,  1.3837])\n",
      "Epoch 152, Loss 8.680145, Params tensor([ 3.9434, -9.2420]), Grad tensor([-0.2432,  1.3767])\n",
      "Epoch 153, Loss 8.621661, Params tensor([ 3.9507, -9.2831]), Grad tensor([-0.2420,  1.3697])\n",
      "Epoch 154, Loss 8.563772, Params tensor([ 3.9579, -9.3240]), Grad tensor([-0.2407,  1.3627])\n",
      "Epoch 155, Loss 8.506474, Params tensor([ 3.9651, -9.3646]), Grad tensor([-0.2395,  1.3557])\n",
      "Epoch 156, Loss 8.449754, Params tensor([ 3.9722, -9.4051]), Grad tensor([-0.2383,  1.3488])\n",
      "Epoch 157, Loss 8.393615, Params tensor([ 3.9793, -9.4454]), Grad tensor([-0.2371,  1.3420])\n",
      "Epoch 158, Loss 8.338042, Params tensor([ 3.9864, -9.4854]), Grad tensor([-0.2358,  1.3351])\n",
      "Epoch 159, Loss 8.283039, Params tensor([ 3.9934, -9.5253]), Grad tensor([-0.2347,  1.3283])\n",
      "Epoch 160, Loss 8.228594, Params tensor([ 4.0004, -9.5649]), Grad tensor([-0.2334,  1.3216])\n",
      "Epoch 161, Loss 8.174701, Params tensor([ 4.0074, -9.6044]), Grad tensor([-0.2323,  1.3148])\n",
      "Epoch 162, Loss 8.121355, Params tensor([ 4.0143, -9.6436]), Grad tensor([-0.2311,  1.3081])\n",
      "Epoch 163, Loss 8.068550, Params tensor([ 4.0212, -9.6826]), Grad tensor([-0.2299,  1.3015])\n",
      "Epoch 164, Loss 8.016290, Params tensor([ 4.0281, -9.7215]), Grad tensor([-0.2287,  1.2948])\n",
      "Epoch 165, Loss 7.964552, Params tensor([ 4.0349, -9.7601]), Grad tensor([-0.2276,  1.2882])\n",
      "Epoch 166, Loss 7.913343, Params tensor([ 4.0417, -9.7986]), Grad tensor([-0.2264,  1.2817])\n",
      "Epoch 167, Loss 7.862658, Params tensor([ 4.0485, -9.8368]), Grad tensor([-0.2253,  1.2751])\n",
      "Epoch 168, Loss 7.812482, Params tensor([ 4.0552, -9.8749]), Grad tensor([-0.2241,  1.2686])\n",
      "Epoch 169, Loss 7.762820, Params tensor([ 4.0619, -9.9128]), Grad tensor([-0.2230,  1.2622])\n",
      "Epoch 170, Loss 7.713665, Params tensor([ 4.0686, -9.9504]), Grad tensor([-0.2218,  1.2557])\n",
      "Epoch 171, Loss 7.665009, Params tensor([ 4.0752, -9.9879]), Grad tensor([-0.2207,  1.2493])\n",
      "Epoch 172, Loss 7.616845, Params tensor([  4.0818, -10.0252]), Grad tensor([-0.2196,  1.2430])\n",
      "Epoch 173, Loss 7.569171, Params tensor([  4.0883, -10.0623]), Grad tensor([-0.2185,  1.2366])\n",
      "Epoch 174, Loss 7.521982, Params tensor([  4.0948, -10.0992]), Grad tensor([-0.2173,  1.2303])\n",
      "Epoch 175, Loss 7.475275, Params tensor([  4.1013, -10.1359]), Grad tensor([-0.2163,  1.2241])\n",
      "Epoch 176, Loss 7.429037, Params tensor([  4.1078, -10.1725]), Grad tensor([-0.2151,  1.2178])\n",
      "Epoch 177, Loss 7.383276, Params tensor([  4.1142, -10.2088]), Grad tensor([-0.2141,  1.2116])\n",
      "Epoch 178, Loss 7.337975, Params tensor([  4.1206, -10.2450]), Grad tensor([-0.2129,  1.2054])\n",
      "Epoch 179, Loss 7.293137, Params tensor([  4.1269, -10.2810]), Grad tensor([-0.2119,  1.1993])\n",
      "Epoch 180, Loss 7.248756, Params tensor([  4.1333, -10.3168]), Grad tensor([-0.2107,  1.1932])\n",
      "Epoch 181, Loss 7.204824, Params tensor([  4.1396, -10.3524]), Grad tensor([-0.2097,  1.1871])\n",
      "Epoch 182, Loss 7.161342, Params tensor([  4.1458, -10.3878]), Grad tensor([-0.2086,  1.1810])\n",
      "Epoch 183, Loss 7.118299, Params tensor([  4.1520, -10.4231]), Grad tensor([-0.2076,  1.1750])\n",
      "Epoch 184, Loss 7.075693, Params tensor([  4.1582, -10.4581]), Grad tensor([-0.2065,  1.1690])\n",
      "Epoch 185, Loss 7.033525, Params tensor([  4.1644, -10.4930]), Grad tensor([-0.2055,  1.1631])\n",
      "Epoch 186, Loss 6.991783, Params tensor([  4.1705, -10.5277]), Grad tensor([-0.2044,  1.1572])\n",
      "Epoch 187, Loss 6.950461, Params tensor([  4.1766, -10.5623]), Grad tensor([-0.2034,  1.1513])\n",
      "Epoch 188, Loss 6.909565, Params tensor([  4.1827, -10.5966]), Grad tensor([-0.2023,  1.1454])\n",
      "Epoch 189, Loss 6.869082, Params tensor([  4.1887, -10.6308]), Grad tensor([-0.2013,  1.1396])\n",
      "Epoch 190, Loss 6.829012, Params tensor([  4.1948, -10.6648]), Grad tensor([-0.2003,  1.1338])\n",
      "Epoch 191, Loss 6.789349, Params tensor([  4.2007, -10.6987]), Grad tensor([-0.1993,  1.1280])\n",
      "Epoch 192, Loss 6.750085, Params tensor([  4.2067, -10.7323]), Grad tensor([-0.1982,  1.1222])\n",
      "Epoch 193, Loss 6.711227, Params tensor([  4.2126, -10.7658]), Grad tensor([-0.1973,  1.1165])\n",
      "Epoch 194, Loss 6.672758, Params tensor([  4.2185, -10.7992]), Grad tensor([-0.1962,  1.1108])\n",
      "Epoch 195, Loss 6.634685, Params tensor([  4.2243, -10.8323]), Grad tensor([-0.1952,  1.1052])\n",
      "Epoch 196, Loss 6.596995, Params tensor([  4.2302, -10.8653]), Grad tensor([-0.1942,  1.0995])\n",
      "Epoch 197, Loss 6.559691, Params tensor([  4.2360, -10.8981]), Grad tensor([-0.1932,  1.0939])\n",
      "Epoch 198, Loss 6.522764, Params tensor([  4.2417, -10.9308]), Grad tensor([-0.1923,  1.0883])\n",
      "Epoch 199, Loss 6.486216, Params tensor([  4.2475, -10.9632]), Grad tensor([-0.1913,  1.0828])\n",
      "Epoch 200, Loss 6.450037, Params tensor([  4.2532, -10.9956]), Grad tensor([-0.1903,  1.0773])\n",
      "Epoch 201, Loss 6.414227, Params tensor([  4.2589, -11.0277]), Grad tensor([-0.1893,  1.0718])\n",
      "Epoch 202, Loss 6.378783, Params tensor([  4.2645, -11.0597]), Grad tensor([-0.1884,  1.0663])\n",
      "Epoch 203, Loss 6.343697, Params tensor([  4.2701, -11.0915]), Grad tensor([-0.1874,  1.0609])\n",
      "Epoch 204, Loss 6.308961, Params tensor([  4.2757, -11.1232]), Grad tensor([-0.1865,  1.0555])\n",
      "Epoch 205, Loss 6.274588, Params tensor([  4.2813, -11.1547]), Grad tensor([-0.1855,  1.0501])\n",
      "Epoch 206, Loss 6.240561, Params tensor([  4.2868, -11.1860]), Grad tensor([-0.1846,  1.0448])\n",
      "Epoch 207, Loss 6.206879, Params tensor([  4.2923, -11.2172]), Grad tensor([-0.1836,  1.0394])\n",
      "Epoch 208, Loss 6.173543, Params tensor([  4.2978, -11.2483]), Grad tensor([-0.1827,  1.0341])\n",
      "Epoch 209, Loss 6.140542, Params tensor([  4.3033, -11.2791]), Grad tensor([-0.1817,  1.0289])\n",
      "Epoch 210, Loss 6.107875, Params tensor([  4.3087, -11.3098]), Grad tensor([-0.1808,  1.0236])\n",
      "Epoch 211, Loss 6.075548, Params tensor([  4.3141, -11.3404]), Grad tensor([-0.1799,  1.0184])\n",
      "Epoch 212, Loss 6.043541, Params tensor([  4.3195, -11.3708]), Grad tensor([-0.1790,  1.0132])\n",
      "Epoch 213, Loss 6.011864, Params tensor([  4.3248, -11.4010]), Grad tensor([-0.1781,  1.0080])\n",
      "Epoch 214, Loss 5.980505, Params tensor([  4.3301, -11.4311]), Grad tensor([-0.1772,  1.0029])\n",
      "Epoch 215, Loss 5.949472, Params tensor([  4.3354, -11.4610]), Grad tensor([-0.1763,  0.9978])\n",
      "Epoch 216, Loss 5.918749, Params tensor([  4.3407, -11.4908]), Grad tensor([-0.1754,  0.9927])\n",
      "Epoch 217, Loss 5.888339, Params tensor([  4.3459, -11.5204]), Grad tensor([-0.1745,  0.9877])\n",
      "Epoch 218, Loss 5.858240, Params tensor([  4.3511, -11.5499]), Grad tensor([-0.1736,  0.9826])\n",
      "Epoch 219, Loss 5.828447, Params tensor([  4.3563, -11.5793]), Grad tensor([-0.1727,  0.9776])\n",
      "Epoch 220, Loss 5.798953, Params tensor([  4.3614, -11.6084]), Grad tensor([-0.1718,  0.9726])\n",
      "Epoch 221, Loss 5.769760, Params tensor([  4.3666, -11.6375]), Grad tensor([-0.1710,  0.9677])\n",
      "Epoch 222, Loss 5.740866, Params tensor([  4.3717, -11.6663]), Grad tensor([-0.1701,  0.9627])\n",
      "Epoch 223, Loss 5.712265, Params tensor([  4.3767, -11.6951]), Grad tensor([-0.1692,  0.9578])\n",
      "Epoch 224, Loss 5.683956, Params tensor([  4.3818, -11.7237]), Grad tensor([-0.1683,  0.9530])\n",
      "Epoch 225, Loss 5.655935, Params tensor([  4.3868, -11.7521]), Grad tensor([-0.1675,  0.9481])\n",
      "Epoch 226, Loss 5.628197, Params tensor([  4.3918, -11.7804]), Grad tensor([-0.1666,  0.9433])\n",
      "Epoch 227, Loss 5.600739, Params tensor([  4.3968, -11.8086]), Grad tensor([-0.1658,  0.9385])\n",
      "Epoch 228, Loss 5.573566, Params tensor([  4.4017, -11.8366]), Grad tensor([-0.1649,  0.9337])\n",
      "Epoch 229, Loss 5.546664, Params tensor([  4.4067, -11.8644]), Grad tensor([-0.1641,  0.9289])\n",
      "Epoch 230, Loss 5.520037, Params tensor([  4.4116, -11.8922]), Grad tensor([-0.1633,  0.9242])\n",
      "Epoch 231, Loss 5.493684, Params tensor([  4.4164, -11.9198]), Grad tensor([-0.1624,  0.9195])\n",
      "Epoch 232, Loss 5.467593, Params tensor([  4.4213, -11.9472]), Grad tensor([-0.1616,  0.9148])\n",
      "Epoch 233, Loss 5.441773, Params tensor([  4.4261, -11.9745]), Grad tensor([-0.1608,  0.9101])\n",
      "Epoch 234, Loss 5.416213, Params tensor([  4.4309, -12.0017]), Grad tensor([-0.1599,  0.9055])\n",
      "Epoch 235, Loss 5.390913, Params tensor([  4.4357, -12.0287]), Grad tensor([-0.1591,  0.9009])\n",
      "Epoch 236, Loss 5.365871, Params tensor([  4.4404, -12.0556]), Grad tensor([-0.1583,  0.8963])\n",
      "Epoch 237, Loss 5.341080, Params tensor([  4.4452, -12.0823]), Grad tensor([-0.1575,  0.8917])\n",
      "Epoch 238, Loss 5.316545, Params tensor([  4.4499, -12.1089]), Grad tensor([-0.1567,  0.8872])\n",
      "Epoch 239, Loss 5.292256, Params tensor([  4.4545, -12.1354]), Grad tensor([-0.1559,  0.8826])\n",
      "Epoch 240, Loss 5.268219, Params tensor([  4.4592, -12.1618]), Grad tensor([-0.1551,  0.8782])\n",
      "Epoch 241, Loss 5.244423, Params tensor([  4.4638, -12.1880]), Grad tensor([-0.1543,  0.8737])\n",
      "Epoch 242, Loss 5.220867, Params tensor([  4.4684, -12.2141]), Grad tensor([-0.1535,  0.8692])\n",
      "Epoch 243, Loss 5.197554, Params tensor([  4.4730, -12.2400]), Grad tensor([-0.1528,  0.8648])\n",
      "Epoch 244, Loss 5.174477, Params tensor([  4.4776, -12.2658]), Grad tensor([-0.1520,  0.8604])\n",
      "Epoch 245, Loss 5.151632, Params tensor([  4.4821, -12.2915]), Grad tensor([-0.1512,  0.8560])\n",
      "Epoch 246, Loss 5.129023, Params tensor([  4.4866, -12.3170]), Grad tensor([-0.1505,  0.8516])\n",
      "Epoch 247, Loss 5.106644, Params tensor([  4.4911, -12.3425]), Grad tensor([-0.1496,  0.8473])\n",
      "Epoch 248, Loss 5.084490, Params tensor([  4.4956, -12.3677]), Grad tensor([-0.1489,  0.8430])\n",
      "Epoch 249, Loss 5.062563, Params tensor([  4.5000, -12.3929]), Grad tensor([-0.1481,  0.8387])\n",
      "Epoch 250, Loss 5.040857, Params tensor([  4.5044, -12.4179]), Grad tensor([-0.1474,  0.8344])\n",
      "Epoch 251, Loss 5.019374, Params tensor([  4.5088, -12.4428]), Grad tensor([-0.1466,  0.8302])\n",
      "Epoch 252, Loss 4.998107, Params tensor([  4.5132, -12.4676]), Grad tensor([-0.1459,  0.8259])\n",
      "Epoch 253, Loss 4.977060, Params tensor([  4.5176, -12.4923]), Grad tensor([-0.1451,  0.8217])\n",
      "Epoch 254, Loss 4.956222, Params tensor([  4.5219, -12.5168]), Grad tensor([-0.1444,  0.8175])\n",
      "Epoch 255, Loss 4.935598, Params tensor([  4.5262, -12.5412]), Grad tensor([-0.1437,  0.8134])\n",
      "Epoch 256, Loss 4.915185, Params tensor([  4.5305, -12.5655]), Grad tensor([-0.1429,  0.8092])\n",
      "Epoch 257, Loss 4.894979, Params tensor([  4.5348, -12.5896]), Grad tensor([-0.1422,  0.8051])\n",
      "Epoch 258, Loss 4.874976, Params tensor([  4.5390, -12.6137]), Grad tensor([-0.1415,  0.8010])\n",
      "Epoch 259, Loss 4.855181, Params tensor([  4.5432, -12.6376]), Grad tensor([-0.1408,  0.7969])\n",
      "Epoch 260, Loss 4.835583, Params tensor([  4.5474, -12.6614]), Grad tensor([-0.1400,  0.7929])\n",
      "Epoch 261, Loss 4.816185, Params tensor([  4.5516, -12.6850]), Grad tensor([-0.1394,  0.7888])\n",
      "Epoch 262, Loss 4.796985, Params tensor([  4.5558, -12.7086]), Grad tensor([-0.1386,  0.7848])\n",
      "Epoch 263, Loss 4.777980, Params tensor([  4.5599, -12.7320]), Grad tensor([-0.1379,  0.7808])\n",
      "Epoch 264, Loss 4.759169, Params tensor([  4.5640, -12.7553]), Grad tensor([-0.1372,  0.7768])\n",
      "Epoch 265, Loss 4.740550, Params tensor([  4.5681, -12.7785]), Grad tensor([-0.1365,  0.7729])\n",
      "Epoch 266, Loss 4.722119, Params tensor([  4.5722, -12.8015]), Grad tensor([-0.1358,  0.7689])\n",
      "Epoch 267, Loss 4.703876, Params tensor([  4.5763, -12.8245]), Grad tensor([-0.1352,  0.7650])\n",
      "Epoch 268, Loss 4.685817, Params tensor([  4.5803, -12.8473]), Grad tensor([-0.1344,  0.7611])\n",
      "Epoch 269, Loss 4.667943, Params tensor([  4.5843, -12.8700]), Grad tensor([-0.1338,  0.7572])\n",
      "Epoch 270, Loss 4.650248, Params tensor([  4.5883, -12.8926]), Grad tensor([-0.1331,  0.7534])\n",
      "Epoch 271, Loss 4.632734, Params tensor([  4.5923, -12.9151]), Grad tensor([-0.1324,  0.7495])\n",
      "Epoch 272, Loss 4.615400, Params tensor([  4.5962, -12.9375]), Grad tensor([-0.1317,  0.7457])\n",
      "Epoch 273, Loss 4.598243, Params tensor([  4.6002, -12.9598]), Grad tensor([-0.1311,  0.7419])\n",
      "Epoch 274, Loss 4.581256, Params tensor([  4.6041, -12.9819]), Grad tensor([-0.1304,  0.7381])\n",
      "Epoch 275, Loss 4.564445, Params tensor([  4.6080, -13.0039]), Grad tensor([-0.1297,  0.7344])\n",
      "Epoch 276, Loss 4.547804, Params tensor([  4.6118, -13.0258]), Grad tensor([-0.1291,  0.7306])\n",
      "Epoch 277, Loss 4.531332, Params tensor([  4.6157, -13.0477]), Grad tensor([-0.1284,  0.7269])\n",
      "Epoch 278, Loss 4.515030, Params tensor([  4.6195, -13.0694]), Grad tensor([-0.1277,  0.7232])\n",
      "Epoch 279, Loss 4.498890, Params tensor([  4.6233, -13.0909]), Grad tensor([-0.1271,  0.7195])\n",
      "Epoch 280, Loss 4.482915, Params tensor([  4.6271, -13.1124]), Grad tensor([-0.1265,  0.7158])\n",
      "Epoch 281, Loss 4.467106, Params tensor([  4.6309, -13.1338]), Grad tensor([-0.1258,  0.7122])\n",
      "Epoch 282, Loss 4.451456, Params tensor([  4.6347, -13.1550]), Grad tensor([-0.1252,  0.7086])\n",
      "Epoch 283, Loss 4.435961, Params tensor([  4.6384, -13.1762]), Grad tensor([-0.1245,  0.7049])\n",
      "Epoch 284, Loss 4.420627, Params tensor([  4.6421, -13.1972]), Grad tensor([-0.1239,  0.7014])\n",
      "Epoch 285, Loss 4.405449, Params tensor([  4.6458, -13.2182]), Grad tensor([-0.1233,  0.6978])\n",
      "Epoch 286, Loss 4.390425, Params tensor([  4.6495, -13.2390]), Grad tensor([-0.1226,  0.6942])\n",
      "Epoch 287, Loss 4.375554, Params tensor([  4.6531, -13.2597]), Grad tensor([-0.1220,  0.6907])\n",
      "Epoch 288, Loss 4.360835, Params tensor([  4.6568, -13.2803]), Grad tensor([-0.1214,  0.6872])\n",
      "Epoch 289, Loss 4.346262, Params tensor([  4.6604, -13.3008]), Grad tensor([-0.1208,  0.6837])\n",
      "Epoch 290, Loss 4.331840, Params tensor([  4.6640, -13.3212]), Grad tensor([-0.1201,  0.6802])\n",
      "Epoch 291, Loss 4.317563, Params tensor([  4.6676, -13.3415]), Grad tensor([-0.1195,  0.6767])\n",
      "Epoch 292, Loss 4.303434, Params tensor([  4.6712, -13.3617]), Grad tensor([-0.1189,  0.6733])\n",
      "Epoch 293, Loss 4.289448, Params tensor([  4.6747, -13.3818]), Grad tensor([-0.1183,  0.6698])\n",
      "Epoch 294, Loss 4.275601, Params tensor([  4.6782, -13.4018]), Grad tensor([-0.1177,  0.6664])\n",
      "Epoch 295, Loss 4.261896, Params tensor([  4.6818, -13.4217]), Grad tensor([-0.1171,  0.6630])\n",
      "Epoch 296, Loss 4.248333, Params tensor([  4.6853, -13.4415]), Grad tensor([-0.1165,  0.6596])\n",
      "Epoch 297, Loss 4.234904, Params tensor([  4.6887, -13.4612]), Grad tensor([-0.1159,  0.6563])\n",
      "Epoch 298, Loss 4.221617, Params tensor([  4.6922, -13.4808]), Grad tensor([-0.1154,  0.6529])\n",
      "Epoch 299, Loss 4.208459, Params tensor([  4.6956, -13.5003]), Grad tensor([-0.1147,  0.6496])\n",
      "Epoch 300, Loss 4.195440, Params tensor([  4.6991, -13.5197]), Grad tensor([-0.1142,  0.6463])\n",
      "Epoch 301, Loss 4.182549, Params tensor([  4.7025, -13.5389]), Grad tensor([-0.1136,  0.6430])\n",
      "Epoch 302, Loss 4.169793, Params tensor([  4.7059, -13.5581]), Grad tensor([-0.1130,  0.6397])\n",
      "Epoch 303, Loss 4.157165, Params tensor([  4.7092, -13.5772]), Grad tensor([-0.1124,  0.6365])\n",
      "Epoch 304, Loss 4.144664, Params tensor([  4.7126, -13.5962]), Grad tensor([-0.1119,  0.6332])\n",
      "Epoch 305, Loss 4.132293, Params tensor([  4.7159, -13.6151]), Grad tensor([-0.1113,  0.6300])\n",
      "Epoch 306, Loss 4.120041, Params tensor([  4.7193, -13.6339]), Grad tensor([-0.1107,  0.6268])\n",
      "Epoch 307, Loss 4.107919, Params tensor([  4.7226, -13.6526]), Grad tensor([-0.1102,  0.6236])\n",
      "Epoch 308, Loss 4.095920, Params tensor([  4.7258, -13.6712]), Grad tensor([-0.1096,  0.6204])\n",
      "Epoch 309, Loss 4.084046, Params tensor([  4.7291, -13.6898]), Grad tensor([-0.1091,  0.6172])\n",
      "Epoch 310, Loss 4.072288, Params tensor([  4.7324, -13.7082]), Grad tensor([-0.1085,  0.6141])\n",
      "Epoch 311, Loss 4.060652, Params tensor([  4.7356, -13.7265]), Grad tensor([-0.1080,  0.6110])\n",
      "Epoch 312, Loss 4.049130, Params tensor([  4.7388, -13.7448]), Grad tensor([-0.1074,  0.6079])\n",
      "Epoch 313, Loss 4.037732, Params tensor([  4.7420, -13.7629]), Grad tensor([-0.1069,  0.6048])\n",
      "Epoch 314, Loss 4.026444, Params tensor([  4.7452, -13.7809]), Grad tensor([-0.1063,  0.6017])\n",
      "Epoch 315, Loss 4.015271, Params tensor([  4.7484, -13.7989]), Grad tensor([-0.1058,  0.5986])\n",
      "Epoch 316, Loss 4.004213, Params tensor([  4.7516, -13.8168]), Grad tensor([-0.1052,  0.5956])\n",
      "Epoch 317, Loss 3.993269, Params tensor([  4.7547, -13.8345]), Grad tensor([-0.1047,  0.5925])\n",
      "Epoch 318, Loss 3.982439, Params tensor([  4.7578, -13.8522]), Grad tensor([-0.1041,  0.5895])\n",
      "Epoch 319, Loss 3.971714, Params tensor([  4.7609, -13.8698]), Grad tensor([-0.1036,  0.5865])\n",
      "Epoch 320, Loss 3.961097, Params tensor([  4.7640, -13.8873]), Grad tensor([-0.1031,  0.5835])\n",
      "Epoch 321, Loss 3.950590, Params tensor([  4.7671, -13.9047]), Grad tensor([-0.1026,  0.5805])\n",
      "Epoch 322, Loss 3.940193, Params tensor([  4.7702, -13.9221]), Grad tensor([-0.1021,  0.5776])\n",
      "Epoch 323, Loss 3.929900, Params tensor([  4.7732, -13.9393]), Grad tensor([-0.1015,  0.5746])\n",
      "Epoch 324, Loss 3.919708, Params tensor([  4.7762, -13.9565]), Grad tensor([-0.1010,  0.5717])\n",
      "Epoch 325, Loss 3.909621, Params tensor([  4.7792, -13.9735]), Grad tensor([-0.1005,  0.5688])\n",
      "Epoch 326, Loss 3.899641, Params tensor([  4.7822, -13.9905]), Grad tensor([-0.1000,  0.5659])\n",
      "Epoch 327, Loss 3.889756, Params tensor([  4.7852, -14.0074]), Grad tensor([-0.0994,  0.5630])\n",
      "Epoch 328, Loss 3.879977, Params tensor([  4.7882, -14.0242]), Grad tensor([-0.0990,  0.5601])\n",
      "Epoch 329, Loss 3.870295, Params tensor([  4.7911, -14.0409]), Grad tensor([-0.0985,  0.5573])\n",
      "Epoch 330, Loss 3.860710, Params tensor([  4.7941, -14.0576]), Grad tensor([-0.0979,  0.5545])\n",
      "Epoch 331, Loss 3.851223, Params tensor([  4.7970, -14.0741]), Grad tensor([-0.0975,  0.5516])\n",
      "Epoch 332, Loss 3.841836, Params tensor([  4.7999, -14.0906]), Grad tensor([-0.0969,  0.5488])\n",
      "Epoch 333, Loss 3.832540, Params tensor([  4.8028, -14.1070]), Grad tensor([-0.0965,  0.5460])\n",
      "Epoch 334, Loss 3.823341, Params tensor([  4.8057, -14.1232]), Grad tensor([-0.0960,  0.5432])\n",
      "Epoch 335, Loss 3.814235, Params tensor([  4.8086, -14.1395]), Grad tensor([-0.0955,  0.5405])\n",
      "Epoch 336, Loss 3.805219, Params tensor([  4.8114, -14.1556]), Grad tensor([-0.0950,  0.5377])\n",
      "Epoch 337, Loss 3.796299, Params tensor([  4.8142, -14.1716]), Grad tensor([-0.0945,  0.5350])\n",
      "Epoch 338, Loss 3.787466, Params tensor([  4.8171, -14.1876]), Grad tensor([-0.0940,  0.5322])\n",
      "Epoch 339, Loss 3.778726, Params tensor([  4.8199, -14.2035]), Grad tensor([-0.0936,  0.5295])\n",
      "Epoch 340, Loss 3.770072, Params tensor([  4.8227, -14.2193]), Grad tensor([-0.0931,  0.5268])\n",
      "Epoch 341, Loss 3.761509, Params tensor([  4.8254, -14.2350]), Grad tensor([-0.0926,  0.5242])\n",
      "Epoch 342, Loss 3.753032, Params tensor([  4.8282, -14.2507]), Grad tensor([-0.0921,  0.5215])\n",
      "Epoch 343, Loss 3.744640, Params tensor([  4.8309, -14.2662]), Grad tensor([-0.0916,  0.5188])\n",
      "Epoch 344, Loss 3.736333, Params tensor([  4.8337, -14.2817]), Grad tensor([-0.0912,  0.5162])\n",
      "Epoch 345, Loss 3.728111, Params tensor([  4.8364, -14.2971]), Grad tensor([-0.0907,  0.5136])\n",
      "Epoch 346, Loss 3.719974, Params tensor([  4.8391, -14.3125]), Grad tensor([-0.0903,  0.5109])\n",
      "Epoch 347, Loss 3.711919, Params tensor([  4.8418, -14.3277]), Grad tensor([-0.0898,  0.5083])\n",
      "Epoch 348, Loss 3.703946, Params tensor([  4.8445, -14.3429]), Grad tensor([-0.0894,  0.5057])\n",
      "Epoch 349, Loss 3.696052, Params tensor([  4.8472, -14.3580]), Grad tensor([-0.0889,  0.5032])\n",
      "Epoch 350, Loss 3.688240, Params tensor([  4.8498, -14.3730]), Grad tensor([-0.0884,  0.5006])\n",
      "Epoch 351, Loss 3.680509, Params tensor([  4.8524, -14.3879]), Grad tensor([-0.0880,  0.4980])\n",
      "Epoch 352, Loss 3.672855, Params tensor([  4.8551, -14.4028]), Grad tensor([-0.0875,  0.4955])\n",
      "Epoch 353, Loss 3.665278, Params tensor([  4.8577, -14.4176]), Grad tensor([-0.0871,  0.4930])\n",
      "Epoch 354, Loss 3.657779, Params tensor([  4.8603, -14.4323]), Grad tensor([-0.0866,  0.4905])\n",
      "Epoch 355, Loss 3.650357, Params tensor([  4.8629, -14.4469]), Grad tensor([-0.0862,  0.4880])\n",
      "Epoch 356, Loss 3.643008, Params tensor([  4.8654, -14.4615]), Grad tensor([-0.0857,  0.4855])\n",
      "Epoch 357, Loss 3.635734, Params tensor([  4.8680, -14.4760]), Grad tensor([-0.0853,  0.4830])\n",
      "Epoch 358, Loss 3.628538, Params tensor([  4.8706, -14.4904]), Grad tensor([-0.0849,  0.4805])\n",
      "Epoch 359, Loss 3.621411, Params tensor([  4.8731, -14.5048]), Grad tensor([-0.0844,  0.4781])\n",
      "Epoch 360, Loss 3.614361, Params tensor([  4.8756, -14.5190]), Grad tensor([-0.0841,  0.4757])\n",
      "Epoch 361, Loss 3.607376, Params tensor([  4.8781, -14.5332]), Grad tensor([-0.0836,  0.4732])\n",
      "Epoch 362, Loss 3.600464, Params tensor([  4.8806, -14.5473]), Grad tensor([-0.0832,  0.4708])\n",
      "Epoch 363, Loss 3.593625, Params tensor([  4.8831, -14.5614]), Grad tensor([-0.0827,  0.4684])\n",
      "Epoch 364, Loss 3.586854, Params tensor([  4.8856, -14.5754]), Grad tensor([-0.0824,  0.4660])\n",
      "Epoch 365, Loss 3.580153, Params tensor([  4.8880, -14.5893]), Grad tensor([-0.0819,  0.4637])\n",
      "Epoch 366, Loss 3.573518, Params tensor([  4.8905, -14.6031]), Grad tensor([-0.0815,  0.4613])\n",
      "Epoch 367, Loss 3.566952, Params tensor([  4.8929, -14.6169]), Grad tensor([-0.0811,  0.4589])\n",
      "Epoch 368, Loss 3.560452, Params tensor([  4.8953, -14.6306]), Grad tensor([-0.0807,  0.4566])\n",
      "Epoch 369, Loss 3.554020, Params tensor([  4.8977, -14.6442]), Grad tensor([-0.0803,  0.4543])\n",
      "Epoch 370, Loss 3.547652, Params tensor([  4.9001, -14.6578]), Grad tensor([-0.0798,  0.4520])\n",
      "Epoch 371, Loss 3.541347, Params tensor([  4.9025, -14.6713]), Grad tensor([-0.0794,  0.4497])\n",
      "Epoch 372, Loss 3.535110, Params tensor([  4.9049, -14.6847]), Grad tensor([-0.0790,  0.4474])\n",
      "Epoch 373, Loss 3.528933, Params tensor([  4.9072, -14.6980]), Grad tensor([-0.0786,  0.4451])\n",
      "Epoch 374, Loss 3.522819, Params tensor([  4.9096, -14.7113]), Grad tensor([-0.0782,  0.4428])\n",
      "Epoch 375, Loss 3.516769, Params tensor([  4.9119, -14.7245]), Grad tensor([-0.0779,  0.4406])\n",
      "Epoch 376, Loss 3.510779, Params tensor([  4.9142, -14.7377]), Grad tensor([-0.0774,  0.4383])\n",
      "Epoch 377, Loss 3.504851, Params tensor([  4.9165, -14.7508]), Grad tensor([-0.0771,  0.4361])\n",
      "Epoch 378, Loss 3.498984, Params tensor([  4.9188, -14.7638]), Grad tensor([-0.0766,  0.4339])\n",
      "Epoch 379, Loss 3.493176, Params tensor([  4.9211, -14.7767]), Grad tensor([-0.0763,  0.4317])\n",
      "Epoch 380, Loss 3.487425, Params tensor([  4.9234, -14.7896]), Grad tensor([-0.0758,  0.4295])\n",
      "Epoch 381, Loss 3.481734, Params tensor([  4.9257, -14.8024]), Grad tensor([-0.0755,  0.4273])\n",
      "Epoch 382, Loss 3.476101, Params tensor([  4.9279, -14.8152]), Grad tensor([-0.0750,  0.4251])\n",
      "Epoch 383, Loss 3.470524, Params tensor([  4.9302, -14.8279]), Grad tensor([-0.0748,  0.4229])\n",
      "Epoch 384, Loss 3.465006, Params tensor([  4.9324, -14.8405]), Grad tensor([-0.0743,  0.4208])\n",
      "Epoch 385, Loss 3.459544, Params tensor([  4.9346, -14.8531]), Grad tensor([-0.0740,  0.4186])\n",
      "Epoch 386, Loss 3.454135, Params tensor([  4.9368, -14.8656]), Grad tensor([-0.0736,  0.4165])\n",
      "Epoch 387, Loss 3.448783, Params tensor([  4.9390, -14.8780]), Grad tensor([-0.0732,  0.4144])\n",
      "Epoch 388, Loss 3.443484, Params tensor([  4.9412, -14.8904]), Grad tensor([-0.0728,  0.4123])\n",
      "Epoch 389, Loss 3.438241, Params tensor([  4.9434, -14.9027]), Grad tensor([-0.0725,  0.4101])\n",
      "Epoch 390, Loss 3.433049, Params tensor([  4.9455, -14.9149]), Grad tensor([-0.0721,  0.4081])\n",
      "Epoch 391, Loss 3.427909, Params tensor([  4.9477, -14.9271]), Grad tensor([-0.0718,  0.4060])\n",
      "Epoch 392, Loss 3.422825, Params tensor([  4.9498, -14.9392]), Grad tensor([-0.0713,  0.4039])\n",
      "Epoch 393, Loss 3.417789, Params tensor([  4.9520, -14.9513]), Grad tensor([-0.0710,  0.4018])\n",
      "Epoch 394, Loss 3.412806, Params tensor([  4.9541, -14.9633]), Grad tensor([-0.0706,  0.3998])\n",
      "Epoch 395, Loss 3.407875, Params tensor([  4.9562, -14.9752]), Grad tensor([-0.0703,  0.3978])\n",
      "Epoch 396, Loss 3.402994, Params tensor([  4.9583, -14.9871]), Grad tensor([-0.0699,  0.3957])\n",
      "Epoch 397, Loss 3.398159, Params tensor([  4.9604, -14.9989]), Grad tensor([-0.0696,  0.3937])\n",
      "Epoch 398, Loss 3.393376, Params tensor([  4.9624, -15.0106]), Grad tensor([-0.0692,  0.3917])\n",
      "Epoch 399, Loss 3.388641, Params tensor([  4.9645, -15.0223]), Grad tensor([-0.0689,  0.3897])\n",
      "Epoch 400, Loss 3.383954, Params tensor([  4.9666, -15.0339]), Grad tensor([-0.0685,  0.3877])\n",
      "Epoch 401, Loss 3.379317, Params tensor([  4.9686, -15.0455]), Grad tensor([-0.0682,  0.3858])\n",
      "Epoch 402, Loss 3.374724, Params tensor([  4.9706, -15.0570]), Grad tensor([-0.0678,  0.3838])\n",
      "Epoch 403, Loss 3.370180, Params tensor([  4.9727, -15.0685]), Grad tensor([-0.0675,  0.3818])\n",
      "Epoch 404, Loss 3.365680, Params tensor([  4.9747, -15.0799]), Grad tensor([-0.0671,  0.3799])\n",
      "Epoch 405, Loss 3.361227, Params tensor([  4.9767, -15.0912]), Grad tensor([-0.0668,  0.3780])\n",
      "Epoch 406, Loss 3.356819, Params tensor([  4.9787, -15.1025]), Grad tensor([-0.0664,  0.3760])\n",
      "Epoch 407, Loss 3.352455, Params tensor([  4.9807, -15.1137]), Grad tensor([-0.0661,  0.3741])\n",
      "Epoch 408, Loss 3.348135, Params tensor([  4.9826, -15.1249]), Grad tensor([-0.0658,  0.3722])\n",
      "Epoch 409, Loss 3.343861, Params tensor([  4.9846, -15.1360]), Grad tensor([-0.0654,  0.3703])\n",
      "Epoch 410, Loss 3.339629, Params tensor([  4.9866, -15.1471]), Grad tensor([-0.0651,  0.3684])\n",
      "Epoch 411, Loss 3.335441, Params tensor([  4.9885, -15.1581]), Grad tensor([-0.0647,  0.3666])\n",
      "Epoch 412, Loss 3.331297, Params tensor([  4.9904, -15.1690]), Grad tensor([-0.0644,  0.3647])\n",
      "Epoch 413, Loss 3.327191, Params tensor([  4.9923, -15.1799]), Grad tensor([-0.0641,  0.3628])\n",
      "Epoch 414, Loss 3.323131, Params tensor([  4.9943, -15.1907]), Grad tensor([-0.0638,  0.3610])\n",
      "Epoch 415, Loss 3.319109, Params tensor([  4.9962, -15.2015]), Grad tensor([-0.0634,  0.3591])\n",
      "Epoch 416, Loss 3.315130, Params tensor([  4.9981, -15.2122]), Grad tensor([-0.0631,  0.3573])\n",
      "Epoch 417, Loss 3.311190, Params tensor([  4.9999, -15.2229]), Grad tensor([-0.0628,  0.3555])\n",
      "Epoch 418, Loss 3.307288, Params tensor([  5.0018, -15.2335]), Grad tensor([-0.0625,  0.3537])\n",
      "Epoch 419, Loss 3.303431, Params tensor([  5.0037, -15.2440]), Grad tensor([-0.0621,  0.3519])\n",
      "Epoch 420, Loss 3.299610, Params tensor([  5.0055, -15.2545]), Grad tensor([-0.0619,  0.3501])\n",
      "Epoch 421, Loss 3.295828, Params tensor([  5.0074, -15.2650]), Grad tensor([-0.0615,  0.3483])\n",
      "Epoch 422, Loss 3.292087, Params tensor([  5.0092, -15.2754]), Grad tensor([-0.0612,  0.3465])\n",
      "Epoch 423, Loss 3.288380, Params tensor([  5.0110, -15.2857]), Grad tensor([-0.0609,  0.3448])\n",
      "Epoch 424, Loss 3.284710, Params tensor([  5.0129, -15.2960]), Grad tensor([-0.0606,  0.3430])\n",
      "Epoch 425, Loss 3.281082, Params tensor([  5.0147, -15.3062]), Grad tensor([-0.0603,  0.3412])\n",
      "Epoch 426, Loss 3.277489, Params tensor([  5.0165, -15.3164]), Grad tensor([-0.0600,  0.3395])\n",
      "Epoch 427, Loss 3.273932, Params tensor([  5.0183, -15.3266]), Grad tensor([-0.0597,  0.3378])\n",
      "Epoch 428, Loss 3.270414, Params tensor([  5.0200, -15.3366]), Grad tensor([-0.0594,  0.3360])\n",
      "Epoch 429, Loss 3.266927, Params tensor([  5.0218, -15.3467]), Grad tensor([-0.0590,  0.3343])\n",
      "Epoch 430, Loss 3.263478, Params tensor([  5.0236, -15.3567]), Grad tensor([-0.0588,  0.3326])\n",
      "Epoch 431, Loss 3.260062, Params tensor([  5.0253, -15.3666]), Grad tensor([-0.0584,  0.3309])\n",
      "Epoch 432, Loss 3.256685, Params tensor([  5.0271, -15.3765]), Grad tensor([-0.0582,  0.3292])\n",
      "Epoch 433, Loss 3.253338, Params tensor([  5.0288, -15.3863]), Grad tensor([-0.0578,  0.3276])\n",
      "Epoch 434, Loss 3.250028, Params tensor([  5.0305, -15.3961]), Grad tensor([-0.0576,  0.3259])\n",
      "Epoch 435, Loss 3.246748, Params tensor([  5.0323, -15.4058]), Grad tensor([-0.0573,  0.3243])\n",
      "Epoch 436, Loss 3.243505, Params tensor([  5.0340, -15.4155]), Grad tensor([-0.0570,  0.3226])\n",
      "Epoch 437, Loss 3.240295, Params tensor([  5.0357, -15.4251]), Grad tensor([-0.0567,  0.3210])\n",
      "Epoch 438, Loss 3.237115, Params tensor([  5.0374, -15.4347]), Grad tensor([-0.0564,  0.3193])\n",
      "Epoch 439, Loss 3.233969, Params tensor([  5.0390, -15.4442]), Grad tensor([-0.0561,  0.3177])\n",
      "Epoch 440, Loss 3.230855, Params tensor([  5.0407, -15.4537]), Grad tensor([-0.0558,  0.3161])\n",
      "Epoch 441, Loss 3.227773, Params tensor([  5.0424, -15.4631]), Grad tensor([-0.0555,  0.3145])\n",
      "Epoch 442, Loss 3.224721, Params tensor([  5.0440, -15.4725]), Grad tensor([-0.0553,  0.3129])\n",
      "Epoch 443, Loss 3.221700, Params tensor([  5.0457, -15.4818]), Grad tensor([-0.0550,  0.3113])\n",
      "Epoch 444, Loss 3.218711, Params tensor([  5.0473, -15.4911]), Grad tensor([-0.0547,  0.3097])\n",
      "Epoch 445, Loss 3.215752, Params tensor([  5.0490, -15.5004]), Grad tensor([-0.0544,  0.3081])\n",
      "Epoch 446, Loss 3.212824, Params tensor([  5.0506, -15.5096]), Grad tensor([-0.0542,  0.3065])\n",
      "Epoch 447, Loss 3.209924, Params tensor([  5.0522, -15.5187]), Grad tensor([-0.0538,  0.3050])\n",
      "Epoch 448, Loss 3.207054, Params tensor([  5.0538, -15.5278]), Grad tensor([-0.0536,  0.3034])\n",
      "Epoch 449, Loss 3.204212, Params tensor([  5.0554, -15.5369]), Grad tensor([-0.0533,  0.3019])\n",
      "Epoch 450, Loss 3.201401, Params tensor([  5.0570, -15.5459]), Grad tensor([-0.0531,  0.3003])\n",
      "Epoch 451, Loss 3.198619, Params tensor([  5.0586, -15.5549]), Grad tensor([-0.0528,  0.2988])\n",
      "Epoch 452, Loss 3.195864, Params tensor([  5.0602, -15.5638]), Grad tensor([-0.0525,  0.2973])\n",
      "Epoch 453, Loss 3.193138, Params tensor([  5.0617, -15.5726]), Grad tensor([-0.0523,  0.2958])\n",
      "Epoch 454, Loss 3.190438, Params tensor([  5.0633, -15.5815]), Grad tensor([-0.0520,  0.2943])\n",
      "Epoch 455, Loss 3.187767, Params tensor([  5.0648, -15.5903]), Grad tensor([-0.0517,  0.2928])\n",
      "Epoch 456, Loss 3.185123, Params tensor([  5.0664, -15.5990]), Grad tensor([-0.0515,  0.2913])\n",
      "Epoch 457, Loss 3.182503, Params tensor([  5.0679, -15.6077]), Grad tensor([-0.0512,  0.2898])\n",
      "Epoch 458, Loss 3.179913, Params tensor([  5.0695, -15.6163]), Grad tensor([-0.0510,  0.2883])\n",
      "Epoch 459, Loss 3.177347, Params tensor([  5.0710, -15.6249]), Grad tensor([-0.0506,  0.2868])\n",
      "Epoch 460, Loss 3.174809, Params tensor([  5.0725, -15.6335]), Grad tensor([-0.0504,  0.2854])\n",
      "Epoch 461, Loss 3.172296, Params tensor([  5.0740, -15.6420]), Grad tensor([-0.0501,  0.2839])\n",
      "Epoch 462, Loss 3.169807, Params tensor([  5.0755, -15.6505]), Grad tensor([-0.0499,  0.2825])\n",
      "Epoch 463, Loss 3.167348, Params tensor([  5.0770, -15.6589]), Grad tensor([-0.0497,  0.2810])\n",
      "Epoch 464, Loss 3.164912, Params tensor([  5.0785, -15.6673]), Grad tensor([-0.0494,  0.2796])\n",
      "Epoch 465, Loss 3.162499, Params tensor([  5.0799, -15.6757]), Grad tensor([-0.0491,  0.2782])\n",
      "Epoch 466, Loss 3.160110, Params tensor([  5.0814, -15.6840]), Grad tensor([-0.0489,  0.2768])\n",
      "Epoch 467, Loss 3.157747, Params tensor([  5.0829, -15.6922]), Grad tensor([-0.0486,  0.2753])\n",
      "Epoch 468, Loss 3.155409, Params tensor([  5.0843, -15.7004]), Grad tensor([-0.0484,  0.2739])\n",
      "Epoch 469, Loss 3.153093, Params tensor([  5.0858, -15.7086]), Grad tensor([-0.0481,  0.2725])\n",
      "Epoch 470, Loss 3.150800, Params tensor([  5.0872, -15.7168]), Grad tensor([-0.0479,  0.2712])\n",
      "Epoch 471, Loss 3.148533, Params tensor([  5.0886, -15.7248]), Grad tensor([-0.0477,  0.2698])\n",
      "Epoch 472, Loss 3.146286, Params tensor([  5.0900, -15.7329]), Grad tensor([-0.0474,  0.2684])\n",
      "Epoch 473, Loss 3.144064, Params tensor([  5.0915, -15.7409]), Grad tensor([-0.0472,  0.2670])\n",
      "Epoch 474, Loss 3.141862, Params tensor([  5.0929, -15.7489]), Grad tensor([-0.0469,  0.2657])\n",
      "Epoch 475, Loss 3.139684, Params tensor([  5.0943, -15.7568]), Grad tensor([-0.0467,  0.2643])\n",
      "Epoch 476, Loss 3.137530, Params tensor([  5.0957, -15.7647]), Grad tensor([-0.0464,  0.2630])\n",
      "Epoch 477, Loss 3.135397, Params tensor([  5.0970, -15.7725]), Grad tensor([-0.0462,  0.2616])\n",
      "Epoch 478, Loss 3.133283, Params tensor([  5.0984, -15.7804]), Grad tensor([-0.0460,  0.2603])\n",
      "Epoch 479, Loss 3.131192, Params tensor([  5.0998, -15.7881]), Grad tensor([-0.0458,  0.2590])\n",
      "Epoch 480, Loss 3.129125, Params tensor([  5.1012, -15.7958]), Grad tensor([-0.0455,  0.2576])\n",
      "Epoch 481, Loss 3.127077, Params tensor([  5.1025, -15.8035]), Grad tensor([-0.0453,  0.2563])\n",
      "Epoch 482, Loss 3.125048, Params tensor([  5.1039, -15.8112]), Grad tensor([-0.0450,  0.2550])\n",
      "Epoch 483, Loss 3.123042, Params tensor([  5.1052, -15.8188]), Grad tensor([-0.0448,  0.2537])\n",
      "Epoch 484, Loss 3.121057, Params tensor([  5.1066, -15.8264]), Grad tensor([-0.0446,  0.2524])\n",
      "Epoch 485, Loss 3.119088, Params tensor([  5.1079, -15.8339]), Grad tensor([-0.0444,  0.2511])\n",
      "Epoch 486, Loss 3.117142, Params tensor([  5.1092, -15.8414]), Grad tensor([-0.0441,  0.2499])\n",
      "Epoch 487, Loss 3.115216, Params tensor([  5.1105, -15.8489]), Grad tensor([-0.0439,  0.2486])\n",
      "Epoch 488, Loss 3.113309, Params tensor([  5.1118, -15.8563]), Grad tensor([-0.0437,  0.2473])\n",
      "Epoch 489, Loss 3.111421, Params tensor([  5.1131, -15.8637]), Grad tensor([-0.0435,  0.2461])\n",
      "Epoch 490, Loss 3.109552, Params tensor([  5.1144, -15.8710]), Grad tensor([-0.0433,  0.2448])\n",
      "Epoch 491, Loss 3.107704, Params tensor([  5.1157, -15.8783]), Grad tensor([-0.0430,  0.2436])\n",
      "Epoch 492, Loss 3.105872, Params tensor([  5.1170, -15.8856]), Grad tensor([-0.0428,  0.2423])\n",
      "Epoch 493, Loss 3.104062, Params tensor([  5.1183, -15.8928]), Grad tensor([-0.0426,  0.2411])\n",
      "Epoch 494, Loss 3.102270, Params tensor([  5.1196, -15.9000]), Grad tensor([-0.0424,  0.2399])\n",
      "Epoch 495, Loss 3.100491, Params tensor([  5.1208, -15.9072]), Grad tensor([-0.0421,  0.2386])\n",
      "Epoch 496, Loss 3.098735, Params tensor([  5.1221, -15.9143]), Grad tensor([-0.0420,  0.2374])\n",
      "Epoch 497, Loss 3.096996, Params tensor([  5.1233, -15.9214]), Grad tensor([-0.0417,  0.2362])\n",
      "Epoch 498, Loss 3.095274, Params tensor([  5.1246, -15.9284]), Grad tensor([-0.0415,  0.2350])\n",
      "Epoch 499, Loss 3.093569, Params tensor([  5.1258, -15.9354]), Grad tensor([-0.0413,  0.2338])\n",
      "Epoch 500, Loss 3.091884, Params tensor([  5.1271, -15.9424]), Grad tensor([-0.0411,  0.2326])\n",
      "Epoch 501, Loss 3.090213, Params tensor([  5.1283, -15.9494]), Grad tensor([-0.0408,  0.2314])\n",
      "Epoch 502, Loss 3.088560, Params tensor([  5.1295, -15.9563]), Grad tensor([-0.0407,  0.2303])\n",
      "Epoch 503, Loss 3.086925, Params tensor([  5.1307, -15.9631]), Grad tensor([-0.0405,  0.2291])\n",
      "Epoch 504, Loss 3.085305, Params tensor([  5.1319, -15.9700]), Grad tensor([-0.0403,  0.2279])\n",
      "Epoch 505, Loss 3.083704, Params tensor([  5.1331, -15.9768]), Grad tensor([-0.0401,  0.2267])\n",
      "Epoch 506, Loss 3.082116, Params tensor([  5.1343, -15.9836]), Grad tensor([-0.0398,  0.2256])\n",
      "Epoch 507, Loss 3.080545, Params tensor([  5.1355, -15.9903]), Grad tensor([-0.0397,  0.2244])\n",
      "Epoch 508, Loss 3.078990, Params tensor([  5.1367, -15.9970]), Grad tensor([-0.0394,  0.2233])\n",
      "Epoch 509, Loss 3.077451, Params tensor([  5.1379, -16.0037]), Grad tensor([-0.0393,  0.2222])\n",
      "Epoch 510, Loss 3.075929, Params tensor([  5.1390, -16.0103]), Grad tensor([-0.0390,  0.2210])\n",
      "Epoch 511, Loss 3.074422, Params tensor([  5.1402, -16.0169]), Grad tensor([-0.0389,  0.2199])\n",
      "Epoch 512, Loss 3.072929, Params tensor([  5.1414, -16.0234]), Grad tensor([-0.0386,  0.2188])\n",
      "Epoch 513, Loss 3.071451, Params tensor([  5.1425, -16.0300]), Grad tensor([-0.0385,  0.2177])\n",
      "Epoch 514, Loss 3.069990, Params tensor([  5.1437, -16.0365]), Grad tensor([-0.0383,  0.2166])\n",
      "Epoch 515, Loss 3.068544, Params tensor([  5.1448, -16.0429]), Grad tensor([-0.0380,  0.2155])\n",
      "Epoch 516, Loss 3.067110, Params tensor([  5.1459, -16.0494]), Grad tensor([-0.0379,  0.2144])\n",
      "Epoch 517, Loss 3.065691, Params tensor([  5.1471, -16.0558]), Grad tensor([-0.0376,  0.2133])\n",
      "Epoch 518, Loss 3.064289, Params tensor([  5.1482, -16.0621]), Grad tensor([-0.0375,  0.2122])\n",
      "Epoch 519, Loss 3.062900, Params tensor([  5.1493, -16.0685]), Grad tensor([-0.0373,  0.2111])\n",
      "Epoch 520, Loss 3.061525, Params tensor([  5.1504, -16.0748]), Grad tensor([-0.0371,  0.2100])\n",
      "Epoch 521, Loss 3.060164, Params tensor([  5.1515, -16.0810]), Grad tensor([-0.0369,  0.2090])\n",
      "Epoch 522, Loss 3.058817, Params tensor([  5.1526, -16.0873]), Grad tensor([-0.0368,  0.2079])\n",
      "Epoch 523, Loss 3.057483, Params tensor([  5.1537, -16.0935]), Grad tensor([-0.0365,  0.2068])\n",
      "Epoch 524, Loss 3.056162, Params tensor([  5.1548, -16.0996]), Grad tensor([-0.0364,  0.2058])\n",
      "Epoch 525, Loss 3.054857, Params tensor([  5.1559, -16.1058]), Grad tensor([-0.0361,  0.2047])\n",
      "Epoch 526, Loss 3.053564, Params tensor([  5.1570, -16.1119]), Grad tensor([-0.0360,  0.2037])\n",
      "Epoch 527, Loss 3.052283, Params tensor([  5.1581, -16.1180]), Grad tensor([-0.0358,  0.2026])\n",
      "Epoch 528, Loss 3.051016, Params tensor([  5.1591, -16.1240]), Grad tensor([-0.0356,  0.2016])\n",
      "Epoch 529, Loss 3.049762, Params tensor([  5.1602, -16.1300]), Grad tensor([-0.0354,  0.2006])\n",
      "Epoch 530, Loss 3.048521, Params tensor([  5.1613, -16.1360]), Grad tensor([-0.0353,  0.1996])\n",
      "Epoch 531, Loss 3.047292, Params tensor([  5.1623, -16.1420]), Grad tensor([-0.0351,  0.1985])\n",
      "Epoch 532, Loss 3.046074, Params tensor([  5.1634, -16.1479]), Grad tensor([-0.0349,  0.1975])\n",
      "Epoch 533, Loss 3.044870, Params tensor([  5.1644, -16.1538]), Grad tensor([-0.0347,  0.1965])\n",
      "Epoch 534, Loss 3.043678, Params tensor([  5.1654, -16.1597]), Grad tensor([-0.0345,  0.1955])\n",
      "Epoch 535, Loss 3.042501, Params tensor([  5.1665, -16.1655]), Grad tensor([-0.0344,  0.1945])\n",
      "Epoch 536, Loss 3.041331, Params tensor([  5.1675, -16.1713]), Grad tensor([-0.0342,  0.1935])\n",
      "Epoch 537, Loss 3.040175, Params tensor([  5.1685, -16.1771]), Grad tensor([-0.0340,  0.1926])\n",
      "Epoch 538, Loss 3.039032, Params tensor([  5.1695, -16.1828]), Grad tensor([-0.0338,  0.1916])\n",
      "Epoch 539, Loss 3.037900, Params tensor([  5.1705, -16.1886]), Grad tensor([-0.0336,  0.1906])\n",
      "Epoch 540, Loss 3.036779, Params tensor([  5.1715, -16.1942]), Grad tensor([-0.0335,  0.1896])\n",
      "Epoch 541, Loss 3.035670, Params tensor([  5.1725, -16.1999]), Grad tensor([-0.0333,  0.1887])\n",
      "Epoch 542, Loss 3.034573, Params tensor([  5.1735, -16.2055]), Grad tensor([-0.0332,  0.1877])\n",
      "Epoch 543, Loss 3.033482, Params tensor([  5.1745, -16.2111]), Grad tensor([-0.0330,  0.1867])\n",
      "Epoch 544, Loss 3.032409, Params tensor([  5.1755, -16.2167]), Grad tensor([-0.0328,  0.1858])\n",
      "Epoch 545, Loss 3.031343, Params tensor([  5.1765, -16.2223]), Grad tensor([-0.0326,  0.1848])\n",
      "Epoch 546, Loss 3.030288, Params tensor([  5.1775, -16.2278]), Grad tensor([-0.0325,  0.1839])\n",
      "Epoch 547, Loss 3.029246, Params tensor([  5.1784, -16.2333]), Grad tensor([-0.0323,  0.1830])\n",
      "Epoch 548, Loss 3.028211, Params tensor([  5.1794, -16.2387]), Grad tensor([-0.0322,  0.1820])\n",
      "Epoch 549, Loss 3.027190, Params tensor([  5.1804, -16.2442]), Grad tensor([-0.0320,  0.1811])\n",
      "Epoch 550, Loss 3.026178, Params tensor([  5.1813, -16.2496]), Grad tensor([-0.0318,  0.1802])\n",
      "Epoch 551, Loss 3.025177, Params tensor([  5.1823, -16.2549]), Grad tensor([-0.0317,  0.1793])\n",
      "Epoch 552, Loss 3.024187, Params tensor([  5.1832, -16.2603]), Grad tensor([-0.0315,  0.1783])\n",
      "Epoch 553, Loss 3.023204, Params tensor([  5.1841, -16.2656]), Grad tensor([-0.0314,  0.1774])\n",
      "Epoch 554, Loss 3.022232, Params tensor([  5.1851, -16.2709]), Grad tensor([-0.0312,  0.1765])\n",
      "Epoch 555, Loss 3.021271, Params tensor([  5.1860, -16.2762]), Grad tensor([-0.0311,  0.1756])\n",
      "Epoch 556, Loss 3.020320, Params tensor([  5.1869, -16.2814]), Grad tensor([-0.0308,  0.1747])\n",
      "Epoch 557, Loss 3.019377, Params tensor([  5.1879, -16.2866]), Grad tensor([-0.0307,  0.1738])\n",
      "Epoch 558, Loss 3.018445, Params tensor([  5.1888, -16.2918]), Grad tensor([-0.0305,  0.1730])\n",
      "Epoch 559, Loss 3.017521, Params tensor([  5.1897, -16.2970]), Grad tensor([-0.0304,  0.1721])\n",
      "Epoch 560, Loss 3.016608, Params tensor([  5.1906, -16.3021]), Grad tensor([-0.0302,  0.1712])\n",
      "Epoch 561, Loss 3.015702, Params tensor([  5.1915, -16.3072]), Grad tensor([-0.0301,  0.1703])\n",
      "Epoch 562, Loss 3.014807, Params tensor([  5.1924, -16.3123]), Grad tensor([-0.0299,  0.1695])\n",
      "Epoch 563, Loss 3.013921, Params tensor([  5.1933, -16.3174]), Grad tensor([-0.0298,  0.1686])\n",
      "Epoch 564, Loss 3.013043, Params tensor([  5.1942, -16.3224]), Grad tensor([-0.0296,  0.1677])\n",
      "Epoch 565, Loss 3.012176, Params tensor([  5.1951, -16.3274]), Grad tensor([-0.0295,  0.1669])\n",
      "Epoch 566, Loss 3.011318, Params tensor([  5.1959, -16.3324]), Grad tensor([-0.0293,  0.1660])\n",
      "Epoch 567, Loss 3.010467, Params tensor([  5.1968, -16.3373]), Grad tensor([-0.0292,  0.1652])\n",
      "Epoch 568, Loss 3.009625, Params tensor([  5.1977, -16.3423]), Grad tensor([-0.0290,  0.1643])\n",
      "Epoch 569, Loss 3.008790, Params tensor([  5.1986, -16.3472]), Grad tensor([-0.0289,  0.1635])\n",
      "Epoch 570, Loss 3.007966, Params tensor([  5.1994, -16.3521]), Grad tensor([-0.0287,  0.1627])\n",
      "Epoch 571, Loss 3.007149, Params tensor([  5.2003, -16.3569]), Grad tensor([-0.0286,  0.1618])\n",
      "Epoch 572, Loss 3.006339, Params tensor([  5.2011, -16.3617]), Grad tensor([-0.0285,  0.1610])\n",
      "Epoch 573, Loss 3.005542, Params tensor([  5.2020, -16.3666]), Grad tensor([-0.0283,  0.1602])\n",
      "Epoch 574, Loss 3.004749, Params tensor([  5.2028, -16.3713]), Grad tensor([-0.0282,  0.1594])\n",
      "Epoch 575, Loss 3.003964, Params tensor([  5.2037, -16.3761]), Grad tensor([-0.0280,  0.1586])\n",
      "Epoch 576, Loss 3.003188, Params tensor([  5.2045, -16.3808]), Grad tensor([-0.0279,  0.1578])\n",
      "Epoch 577, Loss 3.002421, Params tensor([  5.2053, -16.3855]), Grad tensor([-0.0277,  0.1570])\n",
      "Epoch 578, Loss 3.001660, Params tensor([  5.2062, -16.3902]), Grad tensor([-0.0276,  0.1562])\n",
      "Epoch 579, Loss 3.000908, Params tensor([  5.2070, -16.3949]), Grad tensor([-0.0274,  0.1554])\n",
      "Epoch 580, Loss 3.000162, Params tensor([  5.2078, -16.3995]), Grad tensor([-0.0273,  0.1546])\n",
      "Epoch 581, Loss 2.999426, Params tensor([  5.2086, -16.4041]), Grad tensor([-0.0272,  0.1538])\n",
      "Epoch 582, Loss 2.998697, Params tensor([  5.2094, -16.4087]), Grad tensor([-0.0271,  0.1530])\n",
      "Epoch 583, Loss 2.997974, Params tensor([  5.2102, -16.4133]), Grad tensor([-0.0269,  0.1522])\n",
      "Epoch 584, Loss 2.997259, Params tensor([  5.2110, -16.4178]), Grad tensor([-0.0268,  0.1514])\n",
      "Epoch 585, Loss 2.996552, Params tensor([  5.2118, -16.4223]), Grad tensor([-0.0266,  0.1507])\n",
      "Epoch 586, Loss 2.995851, Params tensor([  5.2126, -16.4268]), Grad tensor([-0.0265,  0.1499])\n",
      "Epoch 587, Loss 2.995157, Params tensor([  5.2134, -16.4313]), Grad tensor([-0.0263,  0.1491])\n",
      "Epoch 588, Loss 2.994471, Params tensor([  5.2142, -16.4358]), Grad tensor([-0.0262,  0.1484])\n",
      "Epoch 589, Loss 2.993790, Params tensor([  5.2150, -16.4402]), Grad tensor([-0.0261,  0.1476])\n",
      "Epoch 590, Loss 2.993118, Params tensor([  5.2158, -16.4446]), Grad tensor([-0.0260,  0.1469])\n",
      "Epoch 591, Loss 2.992454, Params tensor([  5.2165, -16.4490]), Grad tensor([-0.0258,  0.1461])\n",
      "Epoch 592, Loss 2.991794, Params tensor([  5.2173, -16.4534]), Grad tensor([-0.0257,  0.1454])\n",
      "Epoch 593, Loss 2.991142, Params tensor([  5.2181, -16.4577]), Grad tensor([-0.0256,  0.1446])\n",
      "Epoch 594, Loss 2.990498, Params tensor([  5.2188, -16.4620]), Grad tensor([-0.0254,  0.1439])\n",
      "Epoch 595, Loss 2.989858, Params tensor([  5.2196, -16.4663]), Grad tensor([-0.0253,  0.1432])\n",
      "Epoch 596, Loss 2.989225, Params tensor([  5.2204, -16.4706]), Grad tensor([-0.0252,  0.1424])\n",
      "Epoch 597, Loss 2.988600, Params tensor([  5.2211, -16.4748]), Grad tensor([-0.0250,  0.1417])\n",
      "Epoch 598, Loss 2.987980, Params tensor([  5.2219, -16.4791]), Grad tensor([-0.0249,  0.1410])\n",
      "Epoch 599, Loss 2.987366, Params tensor([  5.2226, -16.4833]), Grad tensor([-0.0248,  0.1403])\n",
      "Epoch 600, Loss 2.986758, Params tensor([  5.2233, -16.4875]), Grad tensor([-0.0246,  0.1396])\n",
      "Epoch 601, Loss 2.986158, Params tensor([  5.2241, -16.4916]), Grad tensor([-0.0245,  0.1388])\n",
      "Epoch 602, Loss 2.985562, Params tensor([  5.2248, -16.4958]), Grad tensor([-0.0244,  0.1381])\n",
      "Epoch 603, Loss 2.984974, Params tensor([  5.2255, -16.4999]), Grad tensor([-0.0243,  0.1374])\n",
      "Epoch 604, Loss 2.984392, Params tensor([  5.2263, -16.5040]), Grad tensor([-0.0241,  0.1367])\n",
      "Epoch 605, Loss 2.983813, Params tensor([  5.2270, -16.5081]), Grad tensor([-0.0240,  0.1360])\n",
      "Epoch 606, Loss 2.983243, Params tensor([  5.2277, -16.5121]), Grad tensor([-0.0239,  0.1353])\n",
      "Epoch 607, Loss 2.982678, Params tensor([  5.2284, -16.5162]), Grad tensor([-0.0238,  0.1347])\n",
      "Epoch 608, Loss 2.982118, Params tensor([  5.2291, -16.5202]), Grad tensor([-0.0237,  0.1340])\n",
      "Epoch 609, Loss 2.981565, Params tensor([  5.2298, -16.5242]), Grad tensor([-0.0235,  0.1333])\n",
      "Epoch 610, Loss 2.981017, Params tensor([  5.2305, -16.5282]), Grad tensor([-0.0234,  0.1326])\n",
      "Epoch 611, Loss 2.980474, Params tensor([  5.2312, -16.5321]), Grad tensor([-0.0233,  0.1319])\n",
      "Epoch 612, Loss 2.979938, Params tensor([  5.2319, -16.5361]), Grad tensor([-0.0232,  0.1313])\n",
      "Epoch 613, Loss 2.979404, Params tensor([  5.2326, -16.5400]), Grad tensor([-0.0231,  0.1306])\n",
      "Epoch 614, Loss 2.978879, Params tensor([  5.2333, -16.5439]), Grad tensor([-0.0229,  0.1299])\n",
      "Epoch 615, Loss 2.978357, Params tensor([  5.2340, -16.5478]), Grad tensor([-0.0229,  0.1293])\n",
      "Epoch 616, Loss 2.977841, Params tensor([  5.2347, -16.5516]), Grad tensor([-0.0227,  0.1286])\n",
      "Epoch 617, Loss 2.977332, Params tensor([  5.2353, -16.5555]), Grad tensor([-0.0226,  0.1279])\n",
      "Epoch 618, Loss 2.976826, Params tensor([  5.2360, -16.5593]), Grad tensor([-0.0225,  0.1273])\n",
      "Epoch 619, Loss 2.976326, Params tensor([  5.2367, -16.5631]), Grad tensor([-0.0224,  0.1266])\n",
      "Epoch 620, Loss 2.975831, Params tensor([  5.2374, -16.5669]), Grad tensor([-0.0222,  0.1260])\n",
      "Epoch 621, Loss 2.975342, Params tensor([  5.2380, -16.5706]), Grad tensor([-0.0222,  0.1254])\n",
      "Epoch 622, Loss 2.974858, Params tensor([  5.2387, -16.5744]), Grad tensor([-0.0220,  0.1247])\n",
      "Epoch 623, Loss 2.974376, Params tensor([  5.2393, -16.5781]), Grad tensor([-0.0219,  0.1241])\n",
      "Epoch 624, Loss 2.973902, Params tensor([  5.2400, -16.5818]), Grad tensor([-0.0218,  0.1235])\n",
      "Epoch 625, Loss 2.973431, Params tensor([  5.2406, -16.5855]), Grad tensor([-0.0217,  0.1228])\n",
      "Epoch 626, Loss 2.972965, Params tensor([  5.2413, -16.5891]), Grad tensor([-0.0216,  0.1222])\n",
      "Epoch 627, Loss 2.972505, Params tensor([  5.2419, -16.5928]), Grad tensor([-0.0215,  0.1216])\n",
      "Epoch 628, Loss 2.972048, Params tensor([  5.2426, -16.5964]), Grad tensor([-0.0214,  0.1210])\n",
      "Epoch 629, Loss 2.971598, Params tensor([  5.2432, -16.6000]), Grad tensor([-0.0213,  0.1203])\n",
      "Epoch 630, Loss 2.971152, Params tensor([  5.2439, -16.6036]), Grad tensor([-0.0212,  0.1197])\n",
      "Epoch 631, Loss 2.970709, Params tensor([  5.2445, -16.6072]), Grad tensor([-0.0210,  0.1191])\n",
      "Epoch 632, Loss 2.970271, Params tensor([  5.2451, -16.6107]), Grad tensor([-0.0209,  0.1185])\n",
      "Epoch 633, Loss 2.969837, Params tensor([  5.2457, -16.6143]), Grad tensor([-0.0208,  0.1179])\n",
      "Epoch 634, Loss 2.969409, Params tensor([  5.2464, -16.6178]), Grad tensor([-0.0207,  0.1173])\n",
      "Epoch 635, Loss 2.968984, Params tensor([  5.2470, -16.6213]), Grad tensor([-0.0206,  0.1167])\n",
      "Epoch 636, Loss 2.968564, Params tensor([  5.2476, -16.6248]), Grad tensor([-0.0205,  0.1161])\n",
      "Epoch 637, Loss 2.968148, Params tensor([  5.2482, -16.6282]), Grad tensor([-0.0204,  0.1155])\n",
      "Epoch 638, Loss 2.967736, Params tensor([  5.2488, -16.6317]), Grad tensor([-0.0203,  0.1149])\n",
      "Epoch 639, Loss 2.967330, Params tensor([  5.2494, -16.6351]), Grad tensor([-0.0202,  0.1143])\n",
      "Epoch 640, Loss 2.966926, Params tensor([  5.2500, -16.6385]), Grad tensor([-0.0201,  0.1138])\n",
      "Epoch 641, Loss 2.966525, Params tensor([  5.2506, -16.6419]), Grad tensor([-0.0200,  0.1132])\n",
      "Epoch 642, Loss 2.966131, Params tensor([  5.2512, -16.6453]), Grad tensor([-0.0199,  0.1126])\n",
      "Epoch 643, Loss 2.965740, Params tensor([  5.2518, -16.6487]), Grad tensor([-0.0198,  0.1120])\n",
      "Epoch 644, Loss 2.965351, Params tensor([  5.2524, -16.6520]), Grad tensor([-0.0197,  0.1115])\n",
      "Epoch 645, Loss 2.964970, Params tensor([  5.2530, -16.6553]), Grad tensor([-0.0196,  0.1109])\n",
      "Epoch 646, Loss 2.964590, Params tensor([  5.2536, -16.6586]), Grad tensor([-0.0195,  0.1103])\n",
      "Epoch 647, Loss 2.964214, Params tensor([  5.2542, -16.6619]), Grad tensor([-0.0194,  0.1098])\n",
      "Epoch 648, Loss 2.963842, Params tensor([  5.2547, -16.6652]), Grad tensor([-0.0193,  0.1092])\n",
      "Epoch 649, Loss 2.963474, Params tensor([  5.2553, -16.6685]), Grad tensor([-0.0192,  0.1086])\n",
      "Epoch 650, Loss 2.963109, Params tensor([  5.2559, -16.6717]), Grad tensor([-0.0191,  0.1081])\n",
      "Epoch 651, Loss 2.962749, Params tensor([  5.2565, -16.6749]), Grad tensor([-0.0190,  0.1075])\n",
      "Epoch 652, Loss 2.962392, Params tensor([  5.2570, -16.6782]), Grad tensor([-0.0189,  0.1070])\n",
      "Epoch 653, Loss 2.962039, Params tensor([  5.2576, -16.6813]), Grad tensor([-0.0188,  0.1064])\n",
      "Epoch 654, Loss 2.961690, Params tensor([  5.2581, -16.6845]), Grad tensor([-0.0187,  0.1059])\n",
      "Epoch 655, Loss 2.961344, Params tensor([  5.2587, -16.6877]), Grad tensor([-0.0186,  0.1054])\n",
      "Epoch 656, Loss 2.960999, Params tensor([  5.2593, -16.6908]), Grad tensor([-0.0185,  0.1048])\n",
      "Epoch 657, Loss 2.960661, Params tensor([  5.2598, -16.6940]), Grad tensor([-0.0184,  0.1043])\n",
      "Epoch 658, Loss 2.960326, Params tensor([  5.2604, -16.6971]), Grad tensor([-0.0183,  0.1038])\n",
      "Epoch 659, Loss 2.959993, Params tensor([  5.2609, -16.7002]), Grad tensor([-0.0182,  0.1032])\n",
      "Epoch 660, Loss 2.959664, Params tensor([  5.2615, -16.7033]), Grad tensor([-0.0181,  0.1027])\n",
      "Epoch 661, Loss 2.959340, Params tensor([  5.2620, -16.7063]), Grad tensor([-0.0180,  0.1022])\n",
      "Epoch 662, Loss 2.959017, Params tensor([  5.2625, -16.7094]), Grad tensor([-0.0180,  0.1017])\n",
      "Epoch 663, Loss 2.958698, Params tensor([  5.2631, -16.7124]), Grad tensor([-0.0179,  0.1011])\n",
      "Epoch 664, Loss 2.958383, Params tensor([  5.2636, -16.7154]), Grad tensor([-0.0178,  0.1006])\n",
      "Epoch 665, Loss 2.958070, Params tensor([  5.2641, -16.7184]), Grad tensor([-0.0177,  0.1001])\n",
      "Epoch 666, Loss 2.957761, Params tensor([  5.2647, -16.7214]), Grad tensor([-0.0176,  0.0996])\n",
      "Epoch 667, Loss 2.957454, Params tensor([  5.2652, -16.7244]), Grad tensor([-0.0175,  0.0991])\n",
      "Epoch 668, Loss 2.957152, Params tensor([  5.2657, -16.7273]), Grad tensor([-0.0174,  0.0986])\n",
      "Epoch 669, Loss 2.956851, Params tensor([  5.2662, -16.7303]), Grad tensor([-0.0173,  0.0981])\n",
      "Epoch 670, Loss 2.956554, Params tensor([  5.2667, -16.7332]), Grad tensor([-0.0172,  0.0976])\n",
      "Epoch 671, Loss 2.956260, Params tensor([  5.2673, -16.7361]), Grad tensor([-0.0172,  0.0971])\n",
      "Epoch 672, Loss 2.955969, Params tensor([  5.2678, -16.7390]), Grad tensor([-0.0171,  0.0966])\n",
      "Epoch 673, Loss 2.955681, Params tensor([  5.2683, -16.7419]), Grad tensor([-0.0170,  0.0961])\n",
      "Epoch 674, Loss 2.955397, Params tensor([  5.2688, -16.7448]), Grad tensor([-0.0169,  0.0956])\n",
      "Epoch 675, Loss 2.955115, Params tensor([  5.2693, -16.7476]), Grad tensor([-0.0168,  0.0951])\n",
      "Epoch 676, Loss 2.954836, Params tensor([  5.2698, -16.7505]), Grad tensor([-0.0167,  0.0946])\n",
      "Epoch 677, Loss 2.954559, Params tensor([  5.2703, -16.7533]), Grad tensor([-0.0167,  0.0942])\n",
      "Epoch 678, Loss 2.954285, Params tensor([  5.2708, -16.7561]), Grad tensor([-0.0165,  0.0937])\n",
      "Epoch 679, Loss 2.954015, Params tensor([  5.2713, -16.7589]), Grad tensor([-0.0165,  0.0932])\n",
      "Epoch 680, Loss 2.953746, Params tensor([  5.2718, -16.7617]), Grad tensor([-0.0163,  0.0927])\n",
      "Epoch 681, Loss 2.953480, Params tensor([  5.2723, -16.7645]), Grad tensor([-0.0163,  0.0923])\n",
      "Epoch 682, Loss 2.953219, Params tensor([  5.2728, -16.7672]), Grad tensor([-0.0162,  0.0918])\n",
      "Epoch 683, Loss 2.952958, Params tensor([  5.2732, -16.7699]), Grad tensor([-0.0161,  0.0913])\n",
      "Epoch 684, Loss 2.952701, Params tensor([  5.2737, -16.7727]), Grad tensor([-0.0161,  0.0909])\n",
      "Epoch 685, Loss 2.952447, Params tensor([  5.2742, -16.7754]), Grad tensor([-0.0160,  0.0904])\n",
      "Epoch 686, Loss 2.952193, Params tensor([  5.2747, -16.7781]), Grad tensor([-0.0159,  0.0899])\n",
      "Epoch 687, Loss 2.951945, Params tensor([  5.2751, -16.7808]), Grad tensor([-0.0158,  0.0895])\n",
      "Epoch 688, Loss 2.951697, Params tensor([  5.2756, -16.7834]), Grad tensor([-0.0157,  0.0890])\n",
      "Epoch 689, Loss 2.951454, Params tensor([  5.2761, -16.7861]), Grad tensor([-0.0157,  0.0886])\n",
      "Epoch 690, Loss 2.951212, Params tensor([  5.2766, -16.7887]), Grad tensor([-0.0156,  0.0881])\n",
      "Epoch 691, Loss 2.950972, Params tensor([  5.2770, -16.7914]), Grad tensor([-0.0155,  0.0877])\n",
      "Epoch 692, Loss 2.950734, Params tensor([  5.2775, -16.7940]), Grad tensor([-0.0154,  0.0872])\n",
      "Epoch 693, Loss 2.950500, Params tensor([  5.2779, -16.7966]), Grad tensor([-0.0153,  0.0868])\n",
      "Epoch 694, Loss 2.950267, Params tensor([  5.2784, -16.7992]), Grad tensor([-0.0153,  0.0863])\n",
      "Epoch 695, Loss 2.950037, Params tensor([  5.2789, -16.8018]), Grad tensor([-0.0152,  0.0859])\n",
      "Epoch 696, Loss 2.949809, Params tensor([  5.2793, -16.8043]), Grad tensor([-0.0151,  0.0855])\n",
      "Epoch 697, Loss 2.949585, Params tensor([  5.2798, -16.8069]), Grad tensor([-0.0150,  0.0850])\n",
      "Epoch 698, Loss 2.949360, Params tensor([  5.2802, -16.8094]), Grad tensor([-0.0149,  0.0846])\n",
      "Epoch 699, Loss 2.949140, Params tensor([  5.2807, -16.8119]), Grad tensor([-0.0149,  0.0842])\n",
      "Epoch 700, Loss 2.948921, Params tensor([  5.2811, -16.8144]), Grad tensor([-0.0148,  0.0837])\n",
      "Epoch 701, Loss 2.948705, Params tensor([  5.2815, -16.8169]), Grad tensor([-0.0147,  0.0833])\n",
      "Epoch 702, Loss 2.948493, Params tensor([  5.2820, -16.8194]), Grad tensor([-0.0146,  0.0829])\n",
      "Epoch 703, Loss 2.948279, Params tensor([  5.2824, -16.8219]), Grad tensor([-0.0146,  0.0824])\n",
      "Epoch 704, Loss 2.948070, Params tensor([  5.2829, -16.8244]), Grad tensor([-0.0145,  0.0820])\n",
      "Epoch 705, Loss 2.947861, Params tensor([  5.2833, -16.8268]), Grad tensor([-0.0144,  0.0816])\n",
      "Epoch 706, Loss 2.947658, Params tensor([  5.2837, -16.8292]), Grad tensor([-0.0144,  0.0812])\n",
      "Epoch 707, Loss 2.947452, Params tensor([  5.2841, -16.8317]), Grad tensor([-0.0143,  0.0808])\n",
      "Epoch 708, Loss 2.947252, Params tensor([  5.2846, -16.8341]), Grad tensor([-0.0142,  0.0804])\n",
      "Epoch 709, Loss 2.947051, Params tensor([  5.2850, -16.8365]), Grad tensor([-0.0141,  0.0800])\n",
      "Epoch 710, Loss 2.946856, Params tensor([  5.2854, -16.8389]), Grad tensor([-0.0141,  0.0795])\n",
      "Epoch 711, Loss 2.946661, Params tensor([  5.2858, -16.8412]), Grad tensor([-0.0140,  0.0792])\n",
      "Epoch 712, Loss 2.946467, Params tensor([  5.2863, -16.8436]), Grad tensor([-0.0139,  0.0787])\n",
      "Epoch 713, Loss 2.946275, Params tensor([  5.2867, -16.8460]), Grad tensor([-0.0138,  0.0783])\n",
      "Epoch 714, Loss 2.946085, Params tensor([  5.2871, -16.8483]), Grad tensor([-0.0138,  0.0779])\n",
      "Epoch 715, Loss 2.945899, Params tensor([  5.2875, -16.8506]), Grad tensor([-0.0137,  0.0775])\n",
      "Epoch 716, Loss 2.945712, Params tensor([  5.2879, -16.8529]), Grad tensor([-0.0136,  0.0772])\n",
      "Epoch 717, Loss 2.945528, Params tensor([  5.2883, -16.8552]), Grad tensor([-0.0136,  0.0768])\n",
      "Epoch 718, Loss 2.945348, Params tensor([  5.2887, -16.8575]), Grad tensor([-0.0135,  0.0764])\n",
      "Epoch 719, Loss 2.945168, Params tensor([  5.2891, -16.8598]), Grad tensor([-0.0135,  0.0760])\n",
      "Epoch 720, Loss 2.944990, Params tensor([  5.2895, -16.8621]), Grad tensor([-0.0133,  0.0756])\n",
      "Epoch 721, Loss 2.944813, Params tensor([  5.2899, -16.8643]), Grad tensor([-0.0133,  0.0752])\n",
      "Epoch 722, Loss 2.944639, Params tensor([  5.2903, -16.8666]), Grad tensor([-0.0132,  0.0748])\n",
      "Epoch 723, Loss 2.944465, Params tensor([  5.2907, -16.8688]), Grad tensor([-0.0132,  0.0744])\n",
      "Epoch 724, Loss 2.944293, Params tensor([  5.2911, -16.8710]), Grad tensor([-0.0131,  0.0741])\n",
      "Epoch 725, Loss 2.944125, Params tensor([  5.2915, -16.8732]), Grad tensor([-0.0130,  0.0737])\n",
      "Epoch 726, Loss 2.943958, Params tensor([  5.2919, -16.8754]), Grad tensor([-0.0129,  0.0733])\n",
      "Epoch 727, Loss 2.943791, Params tensor([  5.2923, -16.8776]), Grad tensor([-0.0129,  0.0729])\n",
      "Epoch 728, Loss 2.943627, Params tensor([  5.2926, -16.8798]), Grad tensor([-0.0128,  0.0726])\n",
      "Epoch 729, Loss 2.943466, Params tensor([  5.2930, -16.8820]), Grad tensor([-0.0127,  0.0722])\n",
      "Epoch 730, Loss 2.943304, Params tensor([  5.2934, -16.8841]), Grad tensor([-0.0127,  0.0718])\n",
      "Epoch 731, Loss 2.943146, Params tensor([  5.2938, -16.8863]), Grad tensor([-0.0126,  0.0715])\n",
      "Epoch 732, Loss 2.942988, Params tensor([  5.2942, -16.8884]), Grad tensor([-0.0126,  0.0711])\n",
      "Epoch 733, Loss 2.942831, Params tensor([  5.2945, -16.8905]), Grad tensor([-0.0125,  0.0707])\n",
      "Epoch 734, Loss 2.942676, Params tensor([  5.2949, -16.8926]), Grad tensor([-0.0124,  0.0704])\n",
      "Epoch 735, Loss 2.942525, Params tensor([  5.2953, -16.8947]), Grad tensor([-0.0124,  0.0700])\n",
      "Epoch 736, Loss 2.942372, Params tensor([  5.2957, -16.8968]), Grad tensor([-0.0123,  0.0697])\n",
      "Epoch 737, Loss 2.942224, Params tensor([  5.2960, -16.8989]), Grad tensor([-0.0123,  0.0693])\n",
      "Epoch 738, Loss 2.942074, Params tensor([  5.2964, -16.9010]), Grad tensor([-0.0122,  0.0690])\n",
      "Epoch 739, Loss 2.941928, Params tensor([  5.2967, -16.9030]), Grad tensor([-0.0121,  0.0686])\n",
      "Epoch 740, Loss 2.941784, Params tensor([  5.2971, -16.9051]), Grad tensor([-0.0120,  0.0683])\n",
      "Epoch 741, Loss 2.941640, Params tensor([  5.2975, -16.9071]), Grad tensor([-0.0120,  0.0679])\n",
      "Epoch 742, Loss 2.941497, Params tensor([  5.2978, -16.9091]), Grad tensor([-0.0119,  0.0676])\n",
      "Epoch 743, Loss 2.941356, Params tensor([  5.2982, -16.9112]), Grad tensor([-0.0119,  0.0672])\n",
      "Epoch 744, Loss 2.941216, Params tensor([  5.2985, -16.9132]), Grad tensor([-0.0118,  0.0669])\n",
      "Epoch 745, Loss 2.941080, Params tensor([  5.2989, -16.9152]), Grad tensor([-0.0118,  0.0665])\n",
      "Epoch 746, Loss 2.940941, Params tensor([  5.2992, -16.9172]), Grad tensor([-0.0117,  0.0662])\n",
      "Epoch 747, Loss 2.940807, Params tensor([  5.2996, -16.9191]), Grad tensor([-0.0116,  0.0658])\n",
      "Epoch 748, Loss 2.940675, Params tensor([  5.2999, -16.9211]), Grad tensor([-0.0116,  0.0655])\n",
      "Epoch 749, Loss 2.940540, Params tensor([  5.3003, -16.9230]), Grad tensor([-0.0115,  0.0652])\n",
      "Epoch 750, Loss 2.940408, Params tensor([  5.3006, -16.9250]), Grad tensor([-0.0115,  0.0648])\n",
      "Epoch 751, Loss 2.940279, Params tensor([  5.3010, -16.9269]), Grad tensor([-0.0114,  0.0645])\n",
      "Epoch 752, Loss 2.940153, Params tensor([  5.3013, -16.9289]), Grad tensor([-0.0113,  0.0642])\n",
      "Epoch 753, Loss 2.940025, Params tensor([  5.3016, -16.9308]), Grad tensor([-0.0113,  0.0639])\n",
      "Epoch 754, Loss 2.939898, Params tensor([  5.3020, -16.9327]), Grad tensor([-0.0112,  0.0635])\n",
      "Epoch 755, Loss 2.939774, Params tensor([  5.3023, -16.9346]), Grad tensor([-0.0112,  0.0632])\n",
      "Epoch 756, Loss 2.939651, Params tensor([  5.3027, -16.9365]), Grad tensor([-0.0111,  0.0629])\n",
      "Epoch 757, Loss 2.939530, Params tensor([  5.3030, -16.9383]), Grad tensor([-0.0111,  0.0626])\n",
      "Epoch 758, Loss 2.939408, Params tensor([  5.3033, -16.9402]), Grad tensor([-0.0110,  0.0623])\n",
      "Epoch 759, Loss 2.939288, Params tensor([  5.3036, -16.9421]), Grad tensor([-0.0110,  0.0619])\n",
      "Epoch 760, Loss 2.939171, Params tensor([  5.3040, -16.9439]), Grad tensor([-0.0109,  0.0616])\n",
      "Epoch 761, Loss 2.939054, Params tensor([  5.3043, -16.9457]), Grad tensor([-0.0108,  0.0613])\n",
      "Epoch 762, Loss 2.938938, Params tensor([  5.3046, -16.9476]), Grad tensor([-0.0108,  0.0610])\n",
      "Epoch 763, Loss 2.938823, Params tensor([  5.3049, -16.9494]), Grad tensor([-0.0107,  0.0607])\n",
      "Epoch 764, Loss 2.938708, Params tensor([  5.3053, -16.9512]), Grad tensor([-0.0107,  0.0604])\n",
      "Epoch 765, Loss 2.938596, Params tensor([  5.3056, -16.9530]), Grad tensor([-0.0106,  0.0601])\n",
      "Epoch 766, Loss 2.938484, Params tensor([  5.3059, -16.9548]), Grad tensor([-0.0105,  0.0598])\n",
      "Epoch 767, Loss 2.938374, Params tensor([  5.3062, -16.9566]), Grad tensor([-0.0105,  0.0595])\n",
      "Epoch 768, Loss 2.938265, Params tensor([  5.3065, -16.9584]), Grad tensor([-0.0104,  0.0592])\n",
      "Epoch 769, Loss 2.938156, Params tensor([  5.3068, -16.9601]), Grad tensor([-0.0104,  0.0588])\n",
      "Epoch 770, Loss 2.938051, Params tensor([  5.3071, -16.9619]), Grad tensor([-0.0103,  0.0586])\n",
      "Epoch 771, Loss 2.937943, Params tensor([  5.3075, -16.9636]), Grad tensor([-0.0103,  0.0582])\n",
      "Epoch 772, Loss 2.937840, Params tensor([  5.3078, -16.9654]), Grad tensor([-0.0102,  0.0580])\n",
      "Epoch 773, Loss 2.937738, Params tensor([  5.3081, -16.9671]), Grad tensor([-0.0102,  0.0577])\n",
      "Epoch 774, Loss 2.937634, Params tensor([  5.3084, -16.9688]), Grad tensor([-0.0101,  0.0574])\n",
      "Epoch 775, Loss 2.937532, Params tensor([  5.3087, -16.9705]), Grad tensor([-0.0101,  0.0571])\n",
      "Epoch 776, Loss 2.937431, Params tensor([  5.3090, -16.9722]), Grad tensor([-0.0100,  0.0568])\n",
      "Epoch 777, Loss 2.937331, Params tensor([  5.3093, -16.9739]), Grad tensor([-0.0100,  0.0565])\n",
      "Epoch 778, Loss 2.937234, Params tensor([  5.3096, -16.9756]), Grad tensor([-0.0099,  0.0562])\n",
      "Epoch 779, Loss 2.937136, Params tensor([  5.3099, -16.9773]), Grad tensor([-0.0099,  0.0559])\n",
      "Epoch 780, Loss 2.937039, Params tensor([  5.3102, -16.9790]), Grad tensor([-0.0098,  0.0556])\n",
      "Epoch 781, Loss 2.936945, Params tensor([  5.3105, -16.9806]), Grad tensor([-0.0098,  0.0553])\n",
      "Epoch 782, Loss 2.936849, Params tensor([  5.3107, -16.9823]), Grad tensor([-0.0097,  0.0551])\n",
      "Epoch 783, Loss 2.936757, Params tensor([  5.3110, -16.9839]), Grad tensor([-0.0097,  0.0548])\n",
      "Epoch 784, Loss 2.936665, Params tensor([  5.3113, -16.9856]), Grad tensor([-0.0096,  0.0545])\n",
      "Epoch 785, Loss 2.936572, Params tensor([  5.3116, -16.9872]), Grad tensor([-0.0096,  0.0542])\n",
      "Epoch 786, Loss 2.936481, Params tensor([  5.3119, -16.9888]), Grad tensor([-0.0096,  0.0539])\n",
      "Epoch 787, Loss 2.936393, Params tensor([  5.3122, -16.9904]), Grad tensor([-0.0095,  0.0537])\n",
      "Epoch 788, Loss 2.936302, Params tensor([  5.3125, -16.9920]), Grad tensor([-0.0094,  0.0534])\n",
      "Epoch 789, Loss 2.936214, Params tensor([  5.3127, -16.9936]), Grad tensor([-0.0094,  0.0531])\n",
      "Epoch 790, Loss 2.936126, Params tensor([  5.3130, -16.9952]), Grad tensor([-0.0093,  0.0529])\n",
      "Epoch 791, Loss 2.936043, Params tensor([  5.3133, -16.9968]), Grad tensor([-0.0093,  0.0526])\n",
      "Epoch 792, Loss 2.935955, Params tensor([  5.3136, -16.9983]), Grad tensor([-0.0092,  0.0523])\n",
      "Epoch 793, Loss 2.935871, Params tensor([  5.3139, -16.9999]), Grad tensor([-0.0092,  0.0521])\n",
      "Epoch 794, Loss 2.935787, Params tensor([  5.3141, -17.0015]), Grad tensor([-0.0092,  0.0518])\n",
      "Epoch 795, Loss 2.935704, Params tensor([  5.3144, -17.0030]), Grad tensor([-0.0091,  0.0515])\n",
      "Epoch 796, Loss 2.935624, Params tensor([  5.3147, -17.0045]), Grad tensor([-0.0090,  0.0513])\n",
      "Epoch 797, Loss 2.935542, Params tensor([  5.3149, -17.0061]), Grad tensor([-0.0090,  0.0510])\n",
      "Epoch 798, Loss 2.935461, Params tensor([  5.3152, -17.0076]), Grad tensor([-0.0090,  0.0507])\n",
      "Epoch 799, Loss 2.935381, Params tensor([  5.3155, -17.0091]), Grad tensor([-0.0089,  0.0505])\n",
      "Epoch 800, Loss 2.935304, Params tensor([  5.3158, -17.0106]), Grad tensor([-0.0089,  0.0502])\n",
      "Epoch 801, Loss 2.935225, Params tensor([  5.3160, -17.0121]), Grad tensor([-0.0088,  0.0500])\n",
      "Epoch 802, Loss 2.935148, Params tensor([  5.3163, -17.0136]), Grad tensor([-0.0088,  0.0497])\n",
      "Epoch 803, Loss 2.935074, Params tensor([  5.3165, -17.0151]), Grad tensor([-0.0087,  0.0495])\n",
      "Epoch 804, Loss 2.934996, Params tensor([  5.3168, -17.0166]), Grad tensor([-0.0087,  0.0492])\n",
      "Epoch 805, Loss 2.934922, Params tensor([  5.3171, -17.0180]), Grad tensor([-0.0086,  0.0490])\n",
      "Epoch 806, Loss 2.934849, Params tensor([  5.3173, -17.0195]), Grad tensor([-0.0086,  0.0487])\n",
      "Epoch 807, Loss 2.934774, Params tensor([  5.3176, -17.0209]), Grad tensor([-0.0085,  0.0485])\n",
      "Epoch 808, Loss 2.934702, Params tensor([  5.3178, -17.0224]), Grad tensor([-0.0086,  0.0482])\n",
      "Epoch 809, Loss 2.934631, Params tensor([  5.3181, -17.0238]), Grad tensor([-0.0084,  0.0480])\n",
      "Epoch 810, Loss 2.934560, Params tensor([  5.3183, -17.0253]), Grad tensor([-0.0085,  0.0477])\n",
      "Epoch 811, Loss 2.934489, Params tensor([  5.3186, -17.0267]), Grad tensor([-0.0084,  0.0475])\n",
      "Epoch 812, Loss 2.934421, Params tensor([  5.3188, -17.0281]), Grad tensor([-0.0084,  0.0472])\n",
      "Epoch 813, Loss 2.934351, Params tensor([  5.3191, -17.0295]), Grad tensor([-0.0083,  0.0470])\n",
      "Epoch 814, Loss 2.934283, Params tensor([  5.3193, -17.0309]), Grad tensor([-0.0083,  0.0468])\n",
      "Epoch 815, Loss 2.934216, Params tensor([  5.3196, -17.0323]), Grad tensor([-0.0082,  0.0465])\n",
      "Epoch 816, Loss 2.934150, Params tensor([  5.3198, -17.0337]), Grad tensor([-0.0082,  0.0463])\n",
      "Epoch 817, Loss 2.934082, Params tensor([  5.3201, -17.0351]), Grad tensor([-0.0081,  0.0461])\n",
      "Epoch 818, Loss 2.934017, Params tensor([  5.3203, -17.0365]), Grad tensor([-0.0081,  0.0458])\n",
      "Epoch 819, Loss 2.933952, Params tensor([  5.3206, -17.0378]), Grad tensor([-0.0080,  0.0456])\n",
      "Epoch 820, Loss 2.933888, Params tensor([  5.3208, -17.0392]), Grad tensor([-0.0080,  0.0453])\n",
      "Epoch 821, Loss 2.933823, Params tensor([  5.3210, -17.0405]), Grad tensor([-0.0079,  0.0451])\n",
      "Epoch 822, Loss 2.933761, Params tensor([  5.3213, -17.0419]), Grad tensor([-0.0080,  0.0449])\n",
      "Epoch 823, Loss 2.933698, Params tensor([  5.3215, -17.0432]), Grad tensor([-0.0079,  0.0447])\n",
      "Epoch 824, Loss 2.933638, Params tensor([  5.3218, -17.0446]), Grad tensor([-0.0079,  0.0444])\n",
      "Epoch 825, Loss 2.933576, Params tensor([  5.3220, -17.0459]), Grad tensor([-0.0078,  0.0442])\n",
      "Epoch 826, Loss 2.933517, Params tensor([  5.3222, -17.0472]), Grad tensor([-0.0078,  0.0440])\n",
      "Epoch 827, Loss 2.933458, Params tensor([  5.3224, -17.0485]), Grad tensor([-0.0077,  0.0438])\n",
      "Epoch 828, Loss 2.933398, Params tensor([  5.3227, -17.0498]), Grad tensor([-0.0077,  0.0435])\n",
      "Epoch 829, Loss 2.933341, Params tensor([  5.3229, -17.0511]), Grad tensor([-0.0077,  0.0433])\n",
      "Epoch 830, Loss 2.933282, Params tensor([  5.3231, -17.0524]), Grad tensor([-0.0076,  0.0431])\n",
      "Epoch 831, Loss 2.933225, Params tensor([  5.3234, -17.0537]), Grad tensor([-0.0076,  0.0429])\n",
      "Epoch 832, Loss 2.933168, Params tensor([  5.3236, -17.0550]), Grad tensor([-0.0075,  0.0427])\n",
      "Epoch 833, Loss 2.933113, Params tensor([  5.3238, -17.0563]), Grad tensor([-0.0075,  0.0424])\n",
      "Epoch 834, Loss 2.933056, Params tensor([  5.3240, -17.0575]), Grad tensor([-0.0075,  0.0422])\n",
      "Epoch 835, Loss 2.933002, Params tensor([  5.3243, -17.0588]), Grad tensor([-0.0074,  0.0420])\n",
      "Epoch 836, Loss 2.932946, Params tensor([  5.3245, -17.0600]), Grad tensor([-0.0074,  0.0418])\n",
      "Epoch 837, Loss 2.932893, Params tensor([  5.3247, -17.0613]), Grad tensor([-0.0073,  0.0416])\n",
      "Epoch 838, Loss 2.932839, Params tensor([  5.3249, -17.0625]), Grad tensor([-0.0073,  0.0414])\n",
      "Epoch 839, Loss 2.932786, Params tensor([  5.3251, -17.0638]), Grad tensor([-0.0073,  0.0412])\n",
      "Epoch 840, Loss 2.932734, Params tensor([  5.3254, -17.0650]), Grad tensor([-0.0072,  0.0409])\n",
      "Epoch 841, Loss 2.932683, Params tensor([  5.3256, -17.0662]), Grad tensor([-0.0072,  0.0407])\n",
      "Epoch 842, Loss 2.932632, Params tensor([  5.3258, -17.0674]), Grad tensor([-0.0072,  0.0405])\n",
      "Epoch 843, Loss 2.932581, Params tensor([  5.3260, -17.0686]), Grad tensor([-0.0071,  0.0403])\n",
      "Epoch 844, Loss 2.932530, Params tensor([  5.3262, -17.0698]), Grad tensor([-0.0071,  0.0401])\n",
      "Epoch 845, Loss 2.932482, Params tensor([  5.3264, -17.0710]), Grad tensor([-0.0070,  0.0399])\n",
      "Epoch 846, Loss 2.932432, Params tensor([  5.3266, -17.0722]), Grad tensor([-0.0070,  0.0397])\n",
      "Epoch 847, Loss 2.932384, Params tensor([  5.3268, -17.0734]), Grad tensor([-0.0070,  0.0395])\n",
      "Epoch 848, Loss 2.932336, Params tensor([  5.3271, -17.0746]), Grad tensor([-0.0069,  0.0393])\n",
      "Epoch 849, Loss 2.932286, Params tensor([  5.3273, -17.0758]), Grad tensor([-0.0069,  0.0391])\n",
      "Epoch 850, Loss 2.932241, Params tensor([  5.3275, -17.0769]), Grad tensor([-0.0069,  0.0389])\n",
      "Epoch 851, Loss 2.932194, Params tensor([  5.3277, -17.0781]), Grad tensor([-0.0069,  0.0387])\n",
      "Epoch 852, Loss 2.932147, Params tensor([  5.3279, -17.0792]), Grad tensor([-0.0068,  0.0385])\n",
      "Epoch 853, Loss 2.932101, Params tensor([  5.3281, -17.0804]), Grad tensor([-0.0068,  0.0383])\n",
      "Epoch 854, Loss 2.932056, Params tensor([  5.3283, -17.0815]), Grad tensor([-0.0067,  0.0381])\n",
      "Epoch 855, Loss 2.932010, Params tensor([  5.3285, -17.0827]), Grad tensor([-0.0067,  0.0379])\n",
      "Epoch 856, Loss 2.931968, Params tensor([  5.3287, -17.0838]), Grad tensor([-0.0067,  0.0377])\n",
      "Epoch 857, Loss 2.931923, Params tensor([  5.3289, -17.0849]), Grad tensor([-0.0066,  0.0375])\n",
      "Epoch 858, Loss 2.931879, Params tensor([  5.3291, -17.0861]), Grad tensor([-0.0066,  0.0373])\n",
      "Epoch 859, Loss 2.931837, Params tensor([  5.3293, -17.0872]), Grad tensor([-0.0065,  0.0372])\n",
      "Epoch 860, Loss 2.931793, Params tensor([  5.3295, -17.0883]), Grad tensor([-0.0065,  0.0370])\n",
      "Epoch 861, Loss 2.931751, Params tensor([  5.3297, -17.0894]), Grad tensor([-0.0065,  0.0368])\n",
      "Epoch 862, Loss 2.931711, Params tensor([  5.3299, -17.0905]), Grad tensor([-0.0065,  0.0366])\n",
      "Epoch 863, Loss 2.931669, Params tensor([  5.3301, -17.0916]), Grad tensor([-0.0064,  0.0364])\n",
      "Epoch 864, Loss 2.931628, Params tensor([  5.3302, -17.0927]), Grad tensor([-0.0064,  0.0362])\n",
      "Epoch 865, Loss 2.931587, Params tensor([  5.3304, -17.0937]), Grad tensor([-0.0064,  0.0360])\n",
      "Epoch 866, Loss 2.931547, Params tensor([  5.3306, -17.0948]), Grad tensor([-0.0063,  0.0359])\n",
      "Epoch 867, Loss 2.931507, Params tensor([  5.3308, -17.0959]), Grad tensor([-0.0063,  0.0357])\n",
      "Epoch 868, Loss 2.931469, Params tensor([  5.3310, -17.0970]), Grad tensor([-0.0062,  0.0355])\n",
      "Epoch 869, Loss 2.931429, Params tensor([  5.3312, -17.0980]), Grad tensor([-0.0063,  0.0353])\n",
      "Epoch 870, Loss 2.931392, Params tensor([  5.3314, -17.0991]), Grad tensor([-0.0062,  0.0351])\n",
      "Epoch 871, Loss 2.931353, Params tensor([  5.3316, -17.1001]), Grad tensor([-0.0062,  0.0349])\n",
      "Epoch 872, Loss 2.931313, Params tensor([  5.3317, -17.1012]), Grad tensor([-0.0061,  0.0348])\n",
      "Epoch 873, Loss 2.931277, Params tensor([  5.3319, -17.1022]), Grad tensor([-0.0061,  0.0346])\n",
      "Epoch 874, Loss 2.931240, Params tensor([  5.3321, -17.1032]), Grad tensor([-0.0061,  0.0344])\n",
      "Epoch 875, Loss 2.931204, Params tensor([  5.3323, -17.1043]), Grad tensor([-0.0060,  0.0342])\n",
      "Epoch 876, Loss 2.931169, Params tensor([  5.3325, -17.1053]), Grad tensor([-0.0060,  0.0341])\n",
      "Epoch 877, Loss 2.931134, Params tensor([  5.3327, -17.1063]), Grad tensor([-0.0060,  0.0339])\n",
      "Epoch 878, Loss 2.931098, Params tensor([  5.3328, -17.1073]), Grad tensor([-0.0060,  0.0337])\n",
      "Epoch 879, Loss 2.931063, Params tensor([  5.3330, -17.1083]), Grad tensor([-0.0059,  0.0335])\n",
      "Epoch 880, Loss 2.931027, Params tensor([  5.3332, -17.1093]), Grad tensor([-0.0059,  0.0334])\n",
      "Epoch 881, Loss 2.930992, Params tensor([  5.3334, -17.1103]), Grad tensor([-0.0059,  0.0332])\n",
      "Epoch 882, Loss 2.930958, Params tensor([  5.3335, -17.1113]), Grad tensor([-0.0058,  0.0330])\n",
      "Epoch 883, Loss 2.930924, Params tensor([  5.3337, -17.1123]), Grad tensor([-0.0058,  0.0329])\n",
      "Epoch 884, Loss 2.930893, Params tensor([  5.3339, -17.1133]), Grad tensor([-0.0058,  0.0327])\n",
      "Epoch 885, Loss 2.930858, Params tensor([  5.3341, -17.1142]), Grad tensor([-0.0057,  0.0325])\n",
      "Epoch 886, Loss 2.930826, Params tensor([  5.3342, -17.1152]), Grad tensor([-0.0057,  0.0324])\n",
      "Epoch 887, Loss 2.930793, Params tensor([  5.3344, -17.1162]), Grad tensor([-0.0057,  0.0322])\n",
      "Epoch 888, Loss 2.930762, Params tensor([  5.3346, -17.1171]), Grad tensor([-0.0057,  0.0320])\n",
      "Epoch 889, Loss 2.930730, Params tensor([  5.3347, -17.1181]), Grad tensor([-0.0056,  0.0319])\n",
      "Epoch 890, Loss 2.930698, Params tensor([  5.3349, -17.1191]), Grad tensor([-0.0056,  0.0317])\n",
      "Epoch 891, Loss 2.930668, Params tensor([  5.3351, -17.1200]), Grad tensor([-0.0056,  0.0316])\n",
      "Epoch 892, Loss 2.930637, Params tensor([  5.3352, -17.1209]), Grad tensor([-0.0055,  0.0314])\n",
      "Epoch 893, Loss 2.930607, Params tensor([  5.3354, -17.1219]), Grad tensor([-0.0055,  0.0312])\n",
      "Epoch 894, Loss 2.930576, Params tensor([  5.3356, -17.1228]), Grad tensor([-0.0055,  0.0311])\n",
      "Epoch 895, Loss 2.930547, Params tensor([  5.3357, -17.1237]), Grad tensor([-0.0055,  0.0309])\n",
      "Epoch 896, Loss 2.930516, Params tensor([  5.3359, -17.1247]), Grad tensor([-0.0054,  0.0308])\n",
      "Epoch 897, Loss 2.930488, Params tensor([  5.3361, -17.1256]), Grad tensor([-0.0054,  0.0306])\n",
      "Epoch 898, Loss 2.930458, Params tensor([  5.3362, -17.1265]), Grad tensor([-0.0054,  0.0304])\n",
      "Epoch 899, Loss 2.930430, Params tensor([  5.3364, -17.1274]), Grad tensor([-0.0054,  0.0303])\n",
      "Epoch 900, Loss 2.930403, Params tensor([  5.3365, -17.1283]), Grad tensor([-0.0053,  0.0301])\n",
      "Epoch 901, Loss 2.930374, Params tensor([  5.3367, -17.1292]), Grad tensor([-0.0053,  0.0300])\n",
      "Epoch 902, Loss 2.930346, Params tensor([  5.3369, -17.1301]), Grad tensor([-0.0052,  0.0298])\n",
      "Epoch 903, Loss 2.930318, Params tensor([  5.3370, -17.1310]), Grad tensor([-0.0053,  0.0297])\n",
      "Epoch 904, Loss 2.930292, Params tensor([  5.3372, -17.1319]), Grad tensor([-0.0052,  0.0295])\n",
      "Epoch 905, Loss 2.930266, Params tensor([  5.3373, -17.1328]), Grad tensor([-0.0052,  0.0294])\n",
      "Epoch 906, Loss 2.930238, Params tensor([  5.3375, -17.1336]), Grad tensor([-0.0051,  0.0292])\n",
      "Epoch 907, Loss 2.930212, Params tensor([  5.3376, -17.1345]), Grad tensor([-0.0051,  0.0291])\n",
      "Epoch 908, Loss 2.930185, Params tensor([  5.3378, -17.1354]), Grad tensor([-0.0051,  0.0289])\n",
      "Epoch 909, Loss 2.930161, Params tensor([  5.3379, -17.1362]), Grad tensor([-0.0051,  0.0288])\n",
      "Epoch 910, Loss 2.930134, Params tensor([  5.3381, -17.1371]), Grad tensor([-0.0051,  0.0286])\n",
      "Epoch 911, Loss 2.930109, Params tensor([  5.3382, -17.1379]), Grad tensor([-0.0050,  0.0285])\n",
      "Epoch 912, Loss 2.930084, Params tensor([  5.3384, -17.1388]), Grad tensor([-0.0050,  0.0283])\n",
      "Epoch 913, Loss 2.930059, Params tensor([  5.3385, -17.1396]), Grad tensor([-0.0050,  0.0282])\n",
      "Epoch 914, Loss 2.930036, Params tensor([  5.3387, -17.1405]), Grad tensor([-0.0049,  0.0281])\n",
      "Epoch 915, Loss 2.930012, Params tensor([  5.3388, -17.1413]), Grad tensor([-0.0049,  0.0279])\n",
      "Epoch 916, Loss 2.929987, Params tensor([  5.3390, -17.1422]), Grad tensor([-0.0049,  0.0278])\n",
      "Epoch 917, Loss 2.929962, Params tensor([  5.3391, -17.1430]), Grad tensor([-0.0049,  0.0276])\n",
      "Epoch 918, Loss 2.929940, Params tensor([  5.3393, -17.1438]), Grad tensor([-0.0048,  0.0275])\n",
      "Epoch 919, Loss 2.929916, Params tensor([  5.3394, -17.1446]), Grad tensor([-0.0048,  0.0273])\n",
      "Epoch 920, Loss 2.929893, Params tensor([  5.3396, -17.1454]), Grad tensor([-0.0048,  0.0272])\n",
      "Epoch 921, Loss 2.929870, Params tensor([  5.3397, -17.1463]), Grad tensor([-0.0048,  0.0271])\n",
      "Epoch 922, Loss 2.929848, Params tensor([  5.3399, -17.1471]), Grad tensor([-0.0048,  0.0269])\n",
      "Epoch 923, Loss 2.929825, Params tensor([  5.3400, -17.1479]), Grad tensor([-0.0047,  0.0268])\n",
      "Epoch 924, Loss 2.929802, Params tensor([  5.3401, -17.1487]), Grad tensor([-0.0047,  0.0267])\n",
      "Epoch 925, Loss 2.929780, Params tensor([  5.3403, -17.1495]), Grad tensor([-0.0047,  0.0265])\n",
      "Epoch 926, Loss 2.929758, Params tensor([  5.3404, -17.1503]), Grad tensor([-0.0046,  0.0264])\n",
      "Epoch 927, Loss 2.929738, Params tensor([  5.3406, -17.1510]), Grad tensor([-0.0046,  0.0263])\n",
      "Epoch 928, Loss 2.929715, Params tensor([  5.3407, -17.1518]), Grad tensor([-0.0046,  0.0261])\n",
      "Epoch 929, Loss 2.929695, Params tensor([  5.3408, -17.1526]), Grad tensor([-0.0046,  0.0260])\n",
      "Epoch 930, Loss 2.929675, Params tensor([  5.3410, -17.1534]), Grad tensor([-0.0046,  0.0258])\n",
      "Epoch 931, Loss 2.929653, Params tensor([  5.3411, -17.1542]), Grad tensor([-0.0045,  0.0257])\n",
      "Epoch 932, Loss 2.929634, Params tensor([  5.3412, -17.1549]), Grad tensor([-0.0045,  0.0256])\n",
      "Epoch 933, Loss 2.929613, Params tensor([  5.3414, -17.1557]), Grad tensor([-0.0045,  0.0255])\n",
      "Epoch 934, Loss 2.929593, Params tensor([  5.3415, -17.1564]), Grad tensor([-0.0045,  0.0253])\n",
      "Epoch 935, Loss 2.929575, Params tensor([  5.3416, -17.1572]), Grad tensor([-0.0044,  0.0252])\n",
      "Epoch 936, Loss 2.929553, Params tensor([  5.3418, -17.1579]), Grad tensor([-0.0044,  0.0251])\n",
      "Epoch 937, Loss 2.929535, Params tensor([  5.3419, -17.1587]), Grad tensor([-0.0044,  0.0249])\n",
      "Epoch 938, Loss 2.929516, Params tensor([  5.3420, -17.1594]), Grad tensor([-0.0044,  0.0248])\n",
      "Epoch 939, Loss 2.929497, Params tensor([  5.3422, -17.1602]), Grad tensor([-0.0043,  0.0247])\n",
      "Epoch 940, Loss 2.929478, Params tensor([  5.3423, -17.1609]), Grad tensor([-0.0044,  0.0246])\n",
      "Epoch 941, Loss 2.929459, Params tensor([  5.3424, -17.1617]), Grad tensor([-0.0043,  0.0244])\n",
      "Epoch 942, Loss 2.929441, Params tensor([  5.3426, -17.1624]), Grad tensor([-0.0044,  0.0243])\n",
      "Epoch 943, Loss 2.929421, Params tensor([  5.3427, -17.1631]), Grad tensor([-0.0042,  0.0242])\n",
      "Epoch 944, Loss 2.929405, Params tensor([  5.3428, -17.1638]), Grad tensor([-0.0043,  0.0241])\n",
      "Epoch 945, Loss 2.929387, Params tensor([  5.3429, -17.1645]), Grad tensor([-0.0042,  0.0240])\n",
      "Epoch 946, Loss 2.929368, Params tensor([  5.3431, -17.1653]), Grad tensor([-0.0043,  0.0238])\n",
      "Epoch 947, Loss 2.929352, Params tensor([  5.3432, -17.1660]), Grad tensor([-0.0042,  0.0237])\n",
      "Epoch 948, Loss 2.929335, Params tensor([  5.3433, -17.1667]), Grad tensor([-0.0042,  0.0236])\n",
      "Epoch 949, Loss 2.929315, Params tensor([  5.3434, -17.1674]), Grad tensor([-0.0041,  0.0235])\n",
      "Epoch 950, Loss 2.929300, Params tensor([  5.3436, -17.1681]), Grad tensor([-0.0041,  0.0233])\n",
      "Epoch 951, Loss 2.929281, Params tensor([  5.3437, -17.1688]), Grad tensor([-0.0041,  0.0232])\n",
      "Epoch 952, Loss 2.929266, Params tensor([  5.3438, -17.1695]), Grad tensor([-0.0041,  0.0231])\n",
      "Epoch 953, Loss 2.929250, Params tensor([  5.3439, -17.1702]), Grad tensor([-0.0040,  0.0230])\n",
      "Epoch 954, Loss 2.929235, Params tensor([  5.3441, -17.1709]), Grad tensor([-0.0041,  0.0229])\n",
      "Epoch 955, Loss 2.929217, Params tensor([  5.3442, -17.1715]), Grad tensor([-0.0040,  0.0228])\n",
      "Epoch 956, Loss 2.929201, Params tensor([  5.3443, -17.1722]), Grad tensor([-0.0040,  0.0226])\n",
      "Epoch 957, Loss 2.929184, Params tensor([  5.3444, -17.1729]), Grad tensor([-0.0040,  0.0225])\n",
      "Epoch 958, Loss 2.929169, Params tensor([  5.3445, -17.1736]), Grad tensor([-0.0040,  0.0224])\n",
      "Epoch 959, Loss 2.929153, Params tensor([  5.3447, -17.1742]), Grad tensor([-0.0039,  0.0223])\n",
      "Epoch 960, Loss 2.929138, Params tensor([  5.3448, -17.1749]), Grad tensor([-0.0039,  0.0222])\n",
      "Epoch 961, Loss 2.929123, Params tensor([  5.3449, -17.1756]), Grad tensor([-0.0039,  0.0221])\n",
      "Epoch 962, Loss 2.929108, Params tensor([  5.3450, -17.1762]), Grad tensor([-0.0039,  0.0220])\n",
      "Epoch 963, Loss 2.929094, Params tensor([  5.3451, -17.1769]), Grad tensor([-0.0039,  0.0218])\n",
      "Epoch 964, Loss 2.929080, Params tensor([  5.3452, -17.1775]), Grad tensor([-0.0038,  0.0217])\n",
      "Epoch 965, Loss 2.929064, Params tensor([  5.3454, -17.1782]), Grad tensor([-0.0038,  0.0216])\n",
      "Epoch 966, Loss 2.929050, Params tensor([  5.3455, -17.1788]), Grad tensor([-0.0038,  0.0215])\n",
      "Epoch 967, Loss 2.929036, Params tensor([  5.3456, -17.1795]), Grad tensor([-0.0038,  0.0214])\n",
      "Epoch 968, Loss 2.929022, Params tensor([  5.3457, -17.1801]), Grad tensor([-0.0038,  0.0213])\n",
      "Epoch 969, Loss 2.929007, Params tensor([  5.3458, -17.1807]), Grad tensor([-0.0038,  0.0212])\n",
      "Epoch 970, Loss 2.928994, Params tensor([  5.3459, -17.1814]), Grad tensor([-0.0037,  0.0211])\n",
      "Epoch 971, Loss 2.928982, Params tensor([  5.3460, -17.1820]), Grad tensor([-0.0037,  0.0210])\n",
      "Epoch 972, Loss 2.928967, Params tensor([  5.3461, -17.1826]), Grad tensor([-0.0037,  0.0209])\n",
      "Epoch 973, Loss 2.928952, Params tensor([  5.3462, -17.1832]), Grad tensor([-0.0037,  0.0208])\n",
      "Epoch 974, Loss 2.928941, Params tensor([  5.3464, -17.1839]), Grad tensor([-0.0037,  0.0206])\n",
      "Epoch 975, Loss 2.928926, Params tensor([  5.3465, -17.1845]), Grad tensor([-0.0036,  0.0205])\n",
      "Epoch 976, Loss 2.928913, Params tensor([  5.3466, -17.1851]), Grad tensor([-0.0036,  0.0204])\n",
      "Epoch 977, Loss 2.928902, Params tensor([  5.3467, -17.1857]), Grad tensor([-0.0036,  0.0203])\n",
      "Epoch 978, Loss 2.928887, Params tensor([  5.3468, -17.1863]), Grad tensor([-0.0036,  0.0202])\n",
      "Epoch 979, Loss 2.928876, Params tensor([  5.3469, -17.1869]), Grad tensor([-0.0035,  0.0201])\n",
      "Epoch 980, Loss 2.928864, Params tensor([  5.3470, -17.1875]), Grad tensor([-0.0035,  0.0200])\n",
      "Epoch 981, Loss 2.928851, Params tensor([  5.3471, -17.1881]), Grad tensor([-0.0035,  0.0199])\n",
      "Epoch 982, Loss 2.928840, Params tensor([  5.3472, -17.1887]), Grad tensor([-0.0035,  0.0198])\n",
      "Epoch 983, Loss 2.928826, Params tensor([  5.3473, -17.1893]), Grad tensor([-0.0035,  0.0197])\n",
      "Epoch 984, Loss 2.928815, Params tensor([  5.3474, -17.1899]), Grad tensor([-0.0034,  0.0196])\n",
      "Epoch 985, Loss 2.928802, Params tensor([  5.3475, -17.1905]), Grad tensor([-0.0035,  0.0195])\n",
      "Epoch 986, Loss 2.928791, Params tensor([  5.3476, -17.1911]), Grad tensor([-0.0034,  0.0194])\n",
      "Epoch 987, Loss 2.928779, Params tensor([  5.3477, -17.1916]), Grad tensor([-0.0034,  0.0193])\n",
      "Epoch 988, Loss 2.928768, Params tensor([  5.3478, -17.1922]), Grad tensor([-0.0034,  0.0192])\n",
      "Epoch 989, Loss 2.928755, Params tensor([  5.3479, -17.1928]), Grad tensor([-0.0034,  0.0191])\n",
      "Epoch 990, Loss 2.928745, Params tensor([  5.3480, -17.1934]), Grad tensor([-0.0034,  0.0190])\n",
      "Epoch 991, Loss 2.928735, Params tensor([  5.3481, -17.1939]), Grad tensor([-0.0033,  0.0189])\n",
      "Epoch 992, Loss 2.928723, Params tensor([  5.3482, -17.1945]), Grad tensor([-0.0033,  0.0188])\n",
      "Epoch 993, Loss 2.928712, Params tensor([  5.3483, -17.1950]), Grad tensor([-0.0033,  0.0187])\n",
      "Epoch 994, Loss 2.928703, Params tensor([  5.3484, -17.1956]), Grad tensor([-0.0033,  0.0186])\n",
      "Epoch 995, Loss 2.928690, Params tensor([  5.3485, -17.1962]), Grad tensor([-0.0033,  0.0185])\n",
      "Epoch 996, Loss 2.928681, Params tensor([  5.3486, -17.1967]), Grad tensor([-0.0032,  0.0185])\n",
      "Epoch 997, Loss 2.928668, Params tensor([  5.3487, -17.1973]), Grad tensor([-0.0033,  0.0184])\n",
      "Epoch 998, Loss 2.928658, Params tensor([  5.3488, -17.1978]), Grad tensor([-0.0032,  0.0183])\n",
      "Epoch 999, Loss 2.928648, Params tensor([  5.3489, -17.1984]), Grad tensor([-0.0032,  0.0182])\n",
      "Epoch 1000, Loss 2.928639, Params tensor([  5.3490, -17.1989]), Grad tensor([-0.0032,  0.0181])\n",
      "Epoch 1001, Loss 2.928628, Params tensor([  5.3491, -17.1994]), Grad tensor([-0.0032,  0.0180])\n",
      "Epoch 1002, Loss 2.928619, Params tensor([  5.3492, -17.2000]), Grad tensor([-0.0032,  0.0179])\n",
      "Epoch 1003, Loss 2.928607, Params tensor([  5.3493, -17.2005]), Grad tensor([-0.0031,  0.0178])\n",
      "Epoch 1004, Loss 2.928598, Params tensor([  5.3494, -17.2010]), Grad tensor([-0.0031,  0.0177])\n",
      "Epoch 1005, Loss 2.928589, Params tensor([  5.3495, -17.2016]), Grad tensor([-0.0031,  0.0176])\n",
      "Epoch 1006, Loss 2.928578, Params tensor([  5.3496, -17.2021]), Grad tensor([-0.0031,  0.0175])\n",
      "Epoch 1007, Loss 2.928569, Params tensor([  5.3497, -17.2026]), Grad tensor([-0.0031,  0.0174])\n",
      "Epoch 1008, Loss 2.928560, Params tensor([  5.3498, -17.2031]), Grad tensor([-0.0030,  0.0174])\n",
      "Epoch 1009, Loss 2.928549, Params tensor([  5.3499, -17.2037]), Grad tensor([-0.0031,  0.0173])\n",
      "Epoch 1010, Loss 2.928541, Params tensor([  5.3499, -17.2042]), Grad tensor([-0.0030,  0.0172])\n",
      "Epoch 1011, Loss 2.928534, Params tensor([  5.3500, -17.2047]), Grad tensor([-0.0031,  0.0171])\n",
      "Epoch 1012, Loss 2.928524, Params tensor([  5.3501, -17.2052]), Grad tensor([-0.0030,  0.0170])\n",
      "Epoch 1013, Loss 2.928515, Params tensor([  5.3502, -17.2057]), Grad tensor([-0.0030,  0.0169])\n",
      "Epoch 1014, Loss 2.928505, Params tensor([  5.3503, -17.2062]), Grad tensor([-0.0030,  0.0168])\n",
      "Epoch 1015, Loss 2.928497, Params tensor([  5.3504, -17.2067]), Grad tensor([-0.0030,  0.0167])\n",
      "Epoch 1016, Loss 2.928489, Params tensor([  5.3505, -17.2072]), Grad tensor([-0.0029,  0.0167])\n",
      "Epoch 1017, Loss 2.928480, Params tensor([  5.3506, -17.2077]), Grad tensor([-0.0030,  0.0166])\n",
      "Epoch 1018, Loss 2.928471, Params tensor([  5.3507, -17.2082]), Grad tensor([-0.0029,  0.0165])\n",
      "Epoch 1019, Loss 2.928463, Params tensor([  5.3507, -17.2087]), Grad tensor([-0.0029,  0.0164])\n",
      "Epoch 1020, Loss 2.928455, Params tensor([  5.3508, -17.2092]), Grad tensor([-0.0029,  0.0163])\n",
      "Epoch 1021, Loss 2.928446, Params tensor([  5.3509, -17.2097]), Grad tensor([-0.0029,  0.0162])\n",
      "Epoch 1022, Loss 2.928437, Params tensor([  5.3510, -17.2102]), Grad tensor([-0.0029,  0.0162])\n",
      "Epoch 1023, Loss 2.928429, Params tensor([  5.3511, -17.2106]), Grad tensor([-0.0028,  0.0161])\n",
      "Epoch 1024, Loss 2.928422, Params tensor([  5.3512, -17.2111]), Grad tensor([-0.0028,  0.0160])\n",
      "Epoch 1025, Loss 2.928414, Params tensor([  5.3513, -17.2116]), Grad tensor([-0.0028,  0.0159])\n",
      "Epoch 1026, Loss 2.928407, Params tensor([  5.3513, -17.2121]), Grad tensor([-0.0028,  0.0158])\n",
      "Epoch 1027, Loss 2.928398, Params tensor([  5.3514, -17.2125]), Grad tensor([-0.0028,  0.0158])\n",
      "Epoch 1028, Loss 2.928392, Params tensor([  5.3515, -17.2130]), Grad tensor([-0.0028,  0.0157])\n",
      "Epoch 1029, Loss 2.928385, Params tensor([  5.3516, -17.2135]), Grad tensor([-0.0027,  0.0156])\n",
      "Epoch 1030, Loss 2.928377, Params tensor([  5.3517, -17.2139]), Grad tensor([-0.0028,  0.0155])\n",
      "Epoch 1031, Loss 2.928368, Params tensor([  5.3518, -17.2144]), Grad tensor([-0.0027,  0.0154])\n",
      "Epoch 1032, Loss 2.928361, Params tensor([  5.3518, -17.2149]), Grad tensor([-0.0027,  0.0154])\n",
      "Epoch 1033, Loss 2.928355, Params tensor([  5.3519, -17.2153]), Grad tensor([-0.0027,  0.0153])\n",
      "Epoch 1034, Loss 2.928346, Params tensor([  5.3520, -17.2158]), Grad tensor([-0.0027,  0.0152])\n",
      "Epoch 1035, Loss 2.928340, Params tensor([  5.3521, -17.2162]), Grad tensor([-0.0027,  0.0151])\n",
      "Epoch 1036, Loss 2.928331, Params tensor([  5.3522, -17.2167]), Grad tensor([-0.0026,  0.0150])\n",
      "Epoch 1037, Loss 2.928325, Params tensor([  5.3522, -17.2171]), Grad tensor([-0.0027,  0.0150])\n",
      "Epoch 1038, Loss 2.928319, Params tensor([  5.3523, -17.2176]), Grad tensor([-0.0026,  0.0149])\n",
      "Epoch 1039, Loss 2.928313, Params tensor([  5.3524, -17.2180]), Grad tensor([-0.0027,  0.0148])\n",
      "Epoch 1040, Loss 2.928305, Params tensor([  5.3525, -17.2185]), Grad tensor([-0.0026,  0.0147])\n",
      "Epoch 1041, Loss 2.928299, Params tensor([  5.3525, -17.2189]), Grad tensor([-0.0026,  0.0147])\n",
      "Epoch 1042, Loss 2.928293, Params tensor([  5.3526, -17.2194]), Grad tensor([-0.0026,  0.0146])\n",
      "Epoch 1043, Loss 2.928284, Params tensor([  5.3527, -17.2198]), Grad tensor([-0.0026,  0.0145])\n",
      "Epoch 1044, Loss 2.928279, Params tensor([  5.3528, -17.2202]), Grad tensor([-0.0025,  0.0144])\n",
      "Epoch 1045, Loss 2.928271, Params tensor([  5.3529, -17.2206]), Grad tensor([-0.0025,  0.0144])\n",
      "Epoch 1046, Loss 2.928267, Params tensor([  5.3529, -17.2211]), Grad tensor([-0.0025,  0.0143])\n",
      "Epoch 1047, Loss 2.928260, Params tensor([  5.3530, -17.2215]), Grad tensor([-0.0025,  0.0142])\n",
      "Epoch 1048, Loss 2.928253, Params tensor([  5.3531, -17.2219]), Grad tensor([-0.0025,  0.0141])\n",
      "Epoch 1049, Loss 2.928246, Params tensor([  5.3532, -17.2224]), Grad tensor([-0.0025,  0.0141])\n",
      "Epoch 1050, Loss 2.928241, Params tensor([  5.3532, -17.2228]), Grad tensor([-0.0025,  0.0140])\n",
      "Epoch 1051, Loss 2.928236, Params tensor([  5.3533, -17.2232]), Grad tensor([-0.0024,  0.0139])\n",
      "Epoch 1052, Loss 2.928228, Params tensor([  5.3534, -17.2236]), Grad tensor([-0.0025,  0.0139])\n",
      "Epoch 1053, Loss 2.928224, Params tensor([  5.3535, -17.2240]), Grad tensor([-0.0024,  0.0138])\n",
      "Epoch 1054, Loss 2.928218, Params tensor([  5.3535, -17.2244]), Grad tensor([-0.0024,  0.0137])\n",
      "Epoch 1055, Loss 2.928212, Params tensor([  5.3536, -17.2248]), Grad tensor([-0.0024,  0.0137])\n",
      "Epoch 1056, Loss 2.928206, Params tensor([  5.3537, -17.2252]), Grad tensor([-0.0024,  0.0136])\n",
      "Epoch 1057, Loss 2.928200, Params tensor([  5.3537, -17.2257]), Grad tensor([-0.0024,  0.0135])\n",
      "Epoch 1058, Loss 2.928196, Params tensor([  5.3538, -17.2261]), Grad tensor([-0.0024,  0.0134])\n",
      "Epoch 1059, Loss 2.928190, Params tensor([  5.3539, -17.2265]), Grad tensor([-0.0023,  0.0134])\n",
      "Epoch 1060, Loss 2.928184, Params tensor([  5.3540, -17.2269]), Grad tensor([-0.0024,  0.0133])\n",
      "Epoch 1061, Loss 2.928179, Params tensor([  5.3540, -17.2273]), Grad tensor([-0.0023,  0.0132])\n",
      "Epoch 1062, Loss 2.928173, Params tensor([  5.3541, -17.2276]), Grad tensor([-0.0023,  0.0132])\n",
      "Epoch 1063, Loss 2.928166, Params tensor([  5.3542, -17.2280]), Grad tensor([-0.0023,  0.0131])\n",
      "Epoch 1064, Loss 2.928162, Params tensor([  5.3542, -17.2284]), Grad tensor([-0.0023,  0.0130])\n",
      "Epoch 1065, Loss 2.928156, Params tensor([  5.3543, -17.2288]), Grad tensor([-0.0023,  0.0130])\n",
      "Epoch 1066, Loss 2.928153, Params tensor([  5.3544, -17.2292]), Grad tensor([-0.0023,  0.0129])\n",
      "Epoch 1067, Loss 2.928146, Params tensor([  5.3544, -17.2296]), Grad tensor([-0.0023,  0.0128])\n",
      "Epoch 1068, Loss 2.928140, Params tensor([  5.3545, -17.2300]), Grad tensor([-0.0023,  0.0128])\n",
      "Epoch 1069, Loss 2.928138, Params tensor([  5.3546, -17.2304]), Grad tensor([-0.0023,  0.0127])\n",
      "Epoch 1070, Loss 2.928131, Params tensor([  5.3546, -17.2307]), Grad tensor([-0.0022,  0.0126])\n",
      "Epoch 1071, Loss 2.928127, Params tensor([  5.3547, -17.2311]), Grad tensor([-0.0022,  0.0126])\n",
      "Epoch 1072, Loss 2.928122, Params tensor([  5.3548, -17.2315]), Grad tensor([-0.0022,  0.0125])\n",
      "Epoch 1073, Loss 2.928115, Params tensor([  5.3548, -17.2319]), Grad tensor([-0.0022,  0.0124])\n",
      "Epoch 1074, Loss 2.928111, Params tensor([  5.3549, -17.2322]), Grad tensor([-0.0022,  0.0124])\n",
      "Epoch 1075, Loss 2.928108, Params tensor([  5.3550, -17.2326]), Grad tensor([-0.0022,  0.0123])\n",
      "Epoch 1076, Loss 2.928103, Params tensor([  5.3550, -17.2330]), Grad tensor([-0.0022,  0.0123])\n",
      "Epoch 1077, Loss 2.928097, Params tensor([  5.3551, -17.2333]), Grad tensor([-0.0022,  0.0122])\n",
      "Epoch 1078, Loss 2.928094, Params tensor([  5.3552, -17.2337]), Grad tensor([-0.0021,  0.0121])\n",
      "Epoch 1079, Loss 2.928088, Params tensor([  5.3552, -17.2341]), Grad tensor([-0.0021,  0.0121])\n",
      "Epoch 1080, Loss 2.928084, Params tensor([  5.3553, -17.2344]), Grad tensor([-0.0021,  0.0120])\n",
      "Epoch 1081, Loss 2.928079, Params tensor([  5.3554, -17.2348]), Grad tensor([-0.0021,  0.0120])\n",
      "Epoch 1082, Loss 2.928075, Params tensor([  5.3554, -17.2351]), Grad tensor([-0.0021,  0.0119])\n",
      "Epoch 1083, Loss 2.928071, Params tensor([  5.3555, -17.2355]), Grad tensor([-0.0021,  0.0118])\n",
      "Epoch 1084, Loss 2.928066, Params tensor([  5.3555, -17.2359]), Grad tensor([-0.0021,  0.0118])\n",
      "Epoch 1085, Loss 2.928062, Params tensor([  5.3556, -17.2362]), Grad tensor([-0.0021,  0.0117])\n",
      "Epoch 1086, Loss 2.928058, Params tensor([  5.3557, -17.2366]), Grad tensor([-0.0020,  0.0117])\n",
      "Epoch 1087, Loss 2.928054, Params tensor([  5.3557, -17.2369]), Grad tensor([-0.0020,  0.0116])\n",
      "Epoch 1088, Loss 2.928050, Params tensor([  5.3558, -17.2372]), Grad tensor([-0.0020,  0.0115])\n",
      "Epoch 1089, Loss 2.928047, Params tensor([  5.3558, -17.2376]), Grad tensor([-0.0020,  0.0115])\n",
      "Epoch 1090, Loss 2.928042, Params tensor([  5.3559, -17.2379]), Grad tensor([-0.0020,  0.0114])\n",
      "Epoch 1091, Loss 2.928037, Params tensor([  5.3560, -17.2383]), Grad tensor([-0.0020,  0.0114])\n",
      "Epoch 1092, Loss 2.928035, Params tensor([  5.3560, -17.2386]), Grad tensor([-0.0020,  0.0113])\n",
      "Epoch 1093, Loss 2.928031, Params tensor([  5.3561, -17.2390]), Grad tensor([-0.0020,  0.0112])\n",
      "Epoch 1094, Loss 2.928026, Params tensor([  5.3561, -17.2393]), Grad tensor([-0.0020,  0.0112])\n",
      "Epoch 1095, Loss 2.928022, Params tensor([  5.3562, -17.2396]), Grad tensor([-0.0020,  0.0111])\n",
      "Epoch 1096, Loss 2.928018, Params tensor([  5.3563, -17.2400]), Grad tensor([-0.0020,  0.0111])\n",
      "Epoch 1097, Loss 2.928014, Params tensor([  5.3563, -17.2403]), Grad tensor([-0.0019,  0.0110])\n",
      "Epoch 1098, Loss 2.928010, Params tensor([  5.3564, -17.2406]), Grad tensor([-0.0019,  0.0110])\n",
      "Epoch 1099, Loss 2.928006, Params tensor([  5.3564, -17.2409]), Grad tensor([-0.0019,  0.0109])\n",
      "Epoch 1100, Loss 2.928003, Params tensor([  5.3565, -17.2413]), Grad tensor([-0.0019,  0.0108])\n",
      "Epoch 1101, Loss 2.927998, Params tensor([  5.3566, -17.2416]), Grad tensor([-0.0019,  0.0108])\n",
      "Epoch 1102, Loss 2.927997, Params tensor([  5.3566, -17.2419]), Grad tensor([-0.0019,  0.0107])\n",
      "Epoch 1103, Loss 2.927993, Params tensor([  5.3567, -17.2422]), Grad tensor([-0.0019,  0.0107])\n",
      "Epoch 1104, Loss 2.927989, Params tensor([  5.3567, -17.2425]), Grad tensor([-0.0019,  0.0106])\n",
      "Epoch 1105, Loss 2.927985, Params tensor([  5.3568, -17.2429]), Grad tensor([-0.0019,  0.0106])\n",
      "Epoch 1106, Loss 2.927981, Params tensor([  5.3568, -17.2432]), Grad tensor([-0.0019,  0.0105])\n",
      "Epoch 1107, Loss 2.927978, Params tensor([  5.3569, -17.2435]), Grad tensor([-0.0019,  0.0105])\n",
      "Epoch 1108, Loss 2.927976, Params tensor([  5.3569, -17.2438]), Grad tensor([-0.0018,  0.0104])\n",
      "Epoch 1109, Loss 2.927972, Params tensor([  5.3570, -17.2441]), Grad tensor([-0.0018,  0.0104])\n",
      "Epoch 1110, Loss 2.927968, Params tensor([  5.3571, -17.2444]), Grad tensor([-0.0018,  0.0103])\n",
      "Epoch 1111, Loss 2.927964, Params tensor([  5.3571, -17.2447]), Grad tensor([-0.0018,  0.0103])\n",
      "Epoch 1112, Loss 2.927962, Params tensor([  5.3572, -17.2450]), Grad tensor([-0.0018,  0.0102])\n",
      "Epoch 1113, Loss 2.927958, Params tensor([  5.3572, -17.2453]), Grad tensor([-0.0018,  0.0102])\n",
      "Epoch 1114, Loss 2.927955, Params tensor([  5.3573, -17.2456]), Grad tensor([-0.0018,  0.0101])\n",
      "Epoch 1115, Loss 2.927952, Params tensor([  5.3573, -17.2459]), Grad tensor([-0.0018,  0.0100])\n",
      "Epoch 1116, Loss 2.927949, Params tensor([  5.3574, -17.2462]), Grad tensor([-0.0018,  0.0100])\n",
      "Epoch 1117, Loss 2.927946, Params tensor([  5.3574, -17.2465]), Grad tensor([-0.0018,  0.0099])\n",
      "Epoch 1118, Loss 2.927943, Params tensor([  5.3575, -17.2468]), Grad tensor([-0.0017,  0.0099])\n",
      "Epoch 1119, Loss 2.927941, Params tensor([  5.3575, -17.2471]), Grad tensor([-0.0018,  0.0098])\n",
      "Epoch 1120, Loss 2.927938, Params tensor([  5.3576, -17.2474]), Grad tensor([-0.0017,  0.0098])\n",
      "Epoch 1121, Loss 2.927934, Params tensor([  5.3576, -17.2477]), Grad tensor([-0.0017,  0.0097])\n",
      "Epoch 1122, Loss 2.927931, Params tensor([  5.3577, -17.2480]), Grad tensor([-0.0017,  0.0097])\n",
      "Epoch 1123, Loss 2.927928, Params tensor([  5.3577, -17.2483]), Grad tensor([-0.0017,  0.0096])\n",
      "Epoch 1124, Loss 2.927926, Params tensor([  5.3578, -17.2486]), Grad tensor([-0.0017,  0.0096])\n",
      "Epoch 1125, Loss 2.927923, Params tensor([  5.3578, -17.2489]), Grad tensor([-0.0017,  0.0095])\n",
      "Epoch 1126, Loss 2.927921, Params tensor([  5.3579, -17.2492]), Grad tensor([-0.0017,  0.0095])\n",
      "Epoch 1127, Loss 2.927917, Params tensor([  5.3579, -17.2494]), Grad tensor([-0.0017,  0.0094])\n",
      "Epoch 1128, Loss 2.927914, Params tensor([  5.3580, -17.2497]), Grad tensor([-0.0017,  0.0094])\n",
      "Epoch 1129, Loss 2.927912, Params tensor([  5.3580, -17.2500]), Grad tensor([-0.0016,  0.0094])\n",
      "Epoch 1130, Loss 2.927908, Params tensor([  5.3581, -17.2503]), Grad tensor([-0.0017,  0.0093])\n",
      "Epoch 1131, Loss 2.927907, Params tensor([  5.3581, -17.2506]), Grad tensor([-0.0016,  0.0093])\n",
      "Epoch 1132, Loss 2.927903, Params tensor([  5.3582, -17.2508]), Grad tensor([-0.0016,  0.0092])\n",
      "Epoch 1133, Loss 2.927900, Params tensor([  5.3582, -17.2511]), Grad tensor([-0.0016,  0.0092])\n",
      "Epoch 1134, Loss 2.927900, Params tensor([  5.3583, -17.2514]), Grad tensor([-0.0016,  0.0091])\n",
      "Epoch 1135, Loss 2.927896, Params tensor([  5.3583, -17.2517]), Grad tensor([-0.0016,  0.0091])\n",
      "Epoch 1136, Loss 2.927892, Params tensor([  5.3584, -17.2519]), Grad tensor([-0.0016,  0.0090])\n",
      "Epoch 1137, Loss 2.927891, Params tensor([  5.3584, -17.2522]), Grad tensor([-0.0016,  0.0090])\n",
      "Epoch 1138, Loss 2.927888, Params tensor([  5.3585, -17.2525]), Grad tensor([-0.0016,  0.0089])\n",
      "Epoch 1139, Loss 2.927887, Params tensor([  5.3585, -17.2527]), Grad tensor([-0.0015,  0.0089])\n",
      "Epoch 1140, Loss 2.927884, Params tensor([  5.3586, -17.2530]), Grad tensor([-0.0016,  0.0088])\n",
      "Epoch 1141, Loss 2.927880, Params tensor([  5.3586, -17.2533]), Grad tensor([-0.0015,  0.0088])\n",
      "Epoch 1142, Loss 2.927879, Params tensor([  5.3587, -17.2535]), Grad tensor([-0.0015,  0.0088])\n",
      "Epoch 1143, Loss 2.927876, Params tensor([  5.3587, -17.2538]), Grad tensor([-0.0015,  0.0087])\n",
      "Epoch 1144, Loss 2.927873, Params tensor([  5.3588, -17.2541]), Grad tensor([-0.0015,  0.0087])\n",
      "Epoch 1145, Loss 2.927872, Params tensor([  5.3588, -17.2543]), Grad tensor([-0.0015,  0.0086])\n",
      "Epoch 1146, Loss 2.927869, Params tensor([  5.3588, -17.2546]), Grad tensor([-0.0015,  0.0086])\n",
      "Epoch 1147, Loss 2.927866, Params tensor([  5.3589, -17.2548]), Grad tensor([-0.0015,  0.0085])\n",
      "Epoch 1148, Loss 2.927865, Params tensor([  5.3589, -17.2551]), Grad tensor([-0.0015,  0.0085])\n",
      "Epoch 1149, Loss 2.927862, Params tensor([  5.3590, -17.2553]), Grad tensor([-0.0015,  0.0084])\n",
      "Epoch 1150, Loss 2.927860, Params tensor([  5.3590, -17.2556]), Grad tensor([-0.0015,  0.0084])\n",
      "Epoch 1151, Loss 2.927858, Params tensor([  5.3591, -17.2558]), Grad tensor([-0.0015,  0.0084])\n",
      "Epoch 1152, Loss 2.927857, Params tensor([  5.3591, -17.2561]), Grad tensor([-0.0015,  0.0083])\n",
      "Epoch 1153, Loss 2.927854, Params tensor([  5.3592, -17.2563]), Grad tensor([-0.0015,  0.0083])\n",
      "Epoch 1154, Loss 2.927853, Params tensor([  5.3592, -17.2566]), Grad tensor([-0.0015,  0.0082])\n",
      "Epoch 1155, Loss 2.927850, Params tensor([  5.3592, -17.2568]), Grad tensor([-0.0014,  0.0082])\n",
      "Epoch 1156, Loss 2.927846, Params tensor([  5.3593, -17.2571]), Grad tensor([-0.0015,  0.0081])\n",
      "Epoch 1157, Loss 2.927845, Params tensor([  5.3593, -17.2573]), Grad tensor([-0.0014,  0.0081])\n",
      "Epoch 1158, Loss 2.927843, Params tensor([  5.3594, -17.2576]), Grad tensor([-0.0015,  0.0081])\n",
      "Epoch 1159, Loss 2.927841, Params tensor([  5.3594, -17.2578]), Grad tensor([-0.0014,  0.0080])\n",
      "Epoch 1160, Loss 2.927839, Params tensor([  5.3595, -17.2580]), Grad tensor([-0.0014,  0.0080])\n",
      "Epoch 1161, Loss 2.927837, Params tensor([  5.3595, -17.2583]), Grad tensor([-0.0014,  0.0079])\n",
      "Epoch 1162, Loss 2.927837, Params tensor([  5.3595, -17.2585]), Grad tensor([-0.0014,  0.0079])\n",
      "Epoch 1163, Loss 2.927834, Params tensor([  5.3596, -17.2587]), Grad tensor([-0.0013,  0.0079])\n",
      "Epoch 1164, Loss 2.927831, Params tensor([  5.3596, -17.2590]), Grad tensor([-0.0014,  0.0078])\n",
      "Epoch 1165, Loss 2.927830, Params tensor([  5.3597, -17.2592]), Grad tensor([-0.0013,  0.0078])\n",
      "Epoch 1166, Loss 2.927828, Params tensor([  5.3597, -17.2594]), Grad tensor([-0.0014,  0.0077])\n",
      "Epoch 1167, Loss 2.927826, Params tensor([  5.3598, -17.2597]), Grad tensor([-0.0014,  0.0077])\n",
      "Epoch 1168, Loss 2.927824, Params tensor([  5.3598, -17.2599]), Grad tensor([-0.0014,  0.0077])\n",
      "Epoch 1169, Loss 2.927823, Params tensor([  5.3598, -17.2601]), Grad tensor([-0.0014,  0.0076])\n",
      "Epoch 1170, Loss 2.927821, Params tensor([  5.3599, -17.2604]), Grad tensor([-0.0013,  0.0076])\n",
      "Epoch 1171, Loss 2.927819, Params tensor([  5.3599, -17.2606]), Grad tensor([-0.0013,  0.0075])\n",
      "Epoch 1172, Loss 2.927816, Params tensor([  5.3600, -17.2608]), Grad tensor([-0.0013,  0.0075])\n",
      "Epoch 1173, Loss 2.927816, Params tensor([  5.3600, -17.2610]), Grad tensor([-0.0013,  0.0075])\n",
      "Epoch 1174, Loss 2.927814, Params tensor([  5.3600, -17.2613]), Grad tensor([-0.0013,  0.0074])\n",
      "Epoch 1175, Loss 2.927811, Params tensor([  5.3601, -17.2615]), Grad tensor([-0.0013,  0.0074])\n",
      "Epoch 1176, Loss 2.927809, Params tensor([  5.3601, -17.2617]), Grad tensor([-0.0013,  0.0074])\n",
      "Epoch 1177, Loss 2.927807, Params tensor([  5.3601, -17.2619]), Grad tensor([-0.0013,  0.0073])\n",
      "Epoch 1178, Loss 2.927806, Params tensor([  5.3602, -17.2621]), Grad tensor([-0.0013,  0.0073])\n",
      "Epoch 1179, Loss 2.927805, Params tensor([  5.3602, -17.2624]), Grad tensor([-0.0013,  0.0072])\n",
      "Epoch 1180, Loss 2.927803, Params tensor([  5.3603, -17.2626]), Grad tensor([-0.0013,  0.0072])\n",
      "Epoch 1181, Loss 2.927802, Params tensor([  5.3603, -17.2628]), Grad tensor([-0.0012,  0.0072])\n",
      "Epoch 1182, Loss 2.927800, Params tensor([  5.3603, -17.2630]), Grad tensor([-0.0013,  0.0071])\n",
      "Epoch 1183, Loss 2.927799, Params tensor([  5.3604, -17.2632]), Grad tensor([-0.0012,  0.0071])\n",
      "Epoch 1184, Loss 2.927798, Params tensor([  5.3604, -17.2634]), Grad tensor([-0.0013,  0.0071])\n",
      "Epoch 1185, Loss 2.927795, Params tensor([  5.3605, -17.2636]), Grad tensor([-0.0012,  0.0070])\n",
      "Epoch 1186, Loss 2.927794, Params tensor([  5.3605, -17.2638]), Grad tensor([-0.0012,  0.0070])\n",
      "Epoch 1187, Loss 2.927793, Params tensor([  5.3605, -17.2641]), Grad tensor([-0.0012,  0.0070])\n",
      "Epoch 1188, Loss 2.927792, Params tensor([  5.3606, -17.2643]), Grad tensor([-0.0012,  0.0069])\n",
      "Epoch 1189, Loss 2.927791, Params tensor([  5.3606, -17.2645]), Grad tensor([-0.0012,  0.0069])\n",
      "Epoch 1190, Loss 2.927788, Params tensor([  5.3606, -17.2647]), Grad tensor([-0.0012,  0.0069])\n",
      "Epoch 1191, Loss 2.927787, Params tensor([  5.3607, -17.2649]), Grad tensor([-0.0012,  0.0068])\n",
      "Epoch 1192, Loss 2.927784, Params tensor([  5.3607, -17.2651]), Grad tensor([-0.0012,  0.0068])\n",
      "Epoch 1193, Loss 2.927784, Params tensor([  5.3607, -17.2653]), Grad tensor([-0.0012,  0.0067])\n",
      "Epoch 1194, Loss 2.927783, Params tensor([  5.3608, -17.2655]), Grad tensor([-0.0012,  0.0067])\n",
      "Epoch 1195, Loss 2.927781, Params tensor([  5.3608, -17.2657]), Grad tensor([-0.0012,  0.0067])\n",
      "Epoch 1196, Loss 2.927781, Params tensor([  5.3608, -17.2659]), Grad tensor([-0.0011,  0.0066])\n",
      "Epoch 1197, Loss 2.927779, Params tensor([  5.3609, -17.2661]), Grad tensor([-0.0012,  0.0066])\n",
      "Epoch 1198, Loss 2.927778, Params tensor([  5.3609, -17.2663]), Grad tensor([-0.0011,  0.0066])\n",
      "Epoch 1199, Loss 2.927775, Params tensor([  5.3610, -17.2665]), Grad tensor([-0.0012,  0.0065])\n",
      "Epoch 1200, Loss 2.927775, Params tensor([  5.3610, -17.2667]), Grad tensor([-0.0011,  0.0065])\n",
      "Epoch 1201, Loss 2.927773, Params tensor([  5.3610, -17.2669]), Grad tensor([-0.0012,  0.0065])\n",
      "Epoch 1202, Loss 2.927773, Params tensor([  5.3611, -17.2671]), Grad tensor([-0.0011,  0.0064])\n",
      "Epoch 1203, Loss 2.927770, Params tensor([  5.3611, -17.2673]), Grad tensor([-0.0011,  0.0064])\n",
      "Epoch 1204, Loss 2.927770, Params tensor([  5.3611, -17.2674]), Grad tensor([-0.0011,  0.0064])\n",
      "Epoch 1205, Loss 2.927768, Params tensor([  5.3612, -17.2676]), Grad tensor([-0.0011,  0.0063])\n",
      "Epoch 1206, Loss 2.927766, Params tensor([  5.3612, -17.2678]), Grad tensor([-0.0011,  0.0063])\n",
      "Epoch 1207, Loss 2.927765, Params tensor([  5.3612, -17.2680]), Grad tensor([-0.0011,  0.0063])\n",
      "Epoch 1208, Loss 2.927764, Params tensor([  5.3613, -17.2682]), Grad tensor([-0.0011,  0.0062])\n",
      "Epoch 1209, Loss 2.927763, Params tensor([  5.3613, -17.2684]), Grad tensor([-0.0011,  0.0062])\n",
      "Epoch 1210, Loss 2.927763, Params tensor([  5.3613, -17.2686]), Grad tensor([-0.0011,  0.0062])\n",
      "Epoch 1211, Loss 2.927761, Params tensor([  5.3614, -17.2688]), Grad tensor([-0.0011,  0.0062])\n",
      "Epoch 1212, Loss 2.927758, Params tensor([  5.3614, -17.2689]), Grad tensor([-0.0011,  0.0061])\n",
      "Epoch 1213, Loss 2.927760, Params tensor([  5.3614, -17.2691]), Grad tensor([-0.0011,  0.0061])\n",
      "Epoch 1214, Loss 2.927758, Params tensor([  5.3615, -17.2693]), Grad tensor([-0.0011,  0.0061])\n",
      "Epoch 1215, Loss 2.927757, Params tensor([  5.3615, -17.2695]), Grad tensor([-0.0010,  0.0060])\n",
      "Epoch 1216, Loss 2.927754, Params tensor([  5.3615, -17.2697]), Grad tensor([-0.0011,  0.0060])\n",
      "Epoch 1217, Loss 2.927753, Params tensor([  5.3615, -17.2698]), Grad tensor([-0.0010,  0.0060])\n",
      "Epoch 1218, Loss 2.927754, Params tensor([  5.3616, -17.2700]), Grad tensor([-0.0011,  0.0059])\n",
      "Epoch 1219, Loss 2.927751, Params tensor([  5.3616, -17.2702]), Grad tensor([-0.0010,  0.0059])\n",
      "Epoch 1220, Loss 2.927752, Params tensor([  5.3616, -17.2704]), Grad tensor([-0.0011,  0.0059])\n",
      "Epoch 1221, Loss 2.927749, Params tensor([  5.3617, -17.2706]), Grad tensor([-0.0010,  0.0059])\n",
      "Epoch 1222, Loss 2.927750, Params tensor([  5.3617, -17.2707]), Grad tensor([-0.0011,  0.0058])\n",
      "Epoch 1223, Loss 2.927748, Params tensor([  5.3617, -17.2709]), Grad tensor([-0.0010,  0.0058])\n",
      "Epoch 1224, Loss 2.927747, Params tensor([  5.3618, -17.2711]), Grad tensor([-0.0011,  0.0057])\n",
      "Epoch 1225, Loss 2.927747, Params tensor([  5.3618, -17.2712]), Grad tensor([-0.0010,  0.0057])\n",
      "Epoch 1226, Loss 2.927743, Params tensor([  5.3618, -17.2714]), Grad tensor([-0.0011,  0.0057])\n",
      "Epoch 1227, Loss 2.927743, Params tensor([  5.3619, -17.2716]), Grad tensor([-0.0010,  0.0057])\n",
      "Epoch 1228, Loss 2.927742, Params tensor([  5.3619, -17.2718]), Grad tensor([-0.0010,  0.0056])\n",
      "Epoch 1229, Loss 2.927742, Params tensor([  5.3619, -17.2719]), Grad tensor([-0.0010,  0.0056])\n",
      "Epoch 1230, Loss 2.927742, Params tensor([  5.3619, -17.2721]), Grad tensor([-0.0010,  0.0056])\n",
      "Epoch 1231, Loss 2.927739, Params tensor([  5.3620, -17.2723]), Grad tensor([-0.0010,  0.0056])\n",
      "Epoch 1232, Loss 2.927738, Params tensor([  5.3620, -17.2724]), Grad tensor([-0.0010,  0.0055])\n",
      "Epoch 1233, Loss 2.927738, Params tensor([  5.3620, -17.2726]), Grad tensor([-0.0010,  0.0055])\n",
      "Epoch 1234, Loss 2.927737, Params tensor([  5.3621, -17.2728]), Grad tensor([-0.0010,  0.0055])\n",
      "Epoch 1235, Loss 2.927737, Params tensor([  5.3621, -17.2729]), Grad tensor([-0.0009,  0.0054])\n",
      "Epoch 1236, Loss 2.927735, Params tensor([  5.3621, -17.2731]), Grad tensor([-0.0010,  0.0054])\n",
      "Epoch 1237, Loss 2.927733, Params tensor([  5.3621, -17.2732]), Grad tensor([-0.0009,  0.0054])\n",
      "Epoch 1238, Loss 2.927734, Params tensor([  5.3622, -17.2734]), Grad tensor([-0.0010,  0.0054])\n",
      "Epoch 1239, Loss 2.927732, Params tensor([  5.3622, -17.2736]), Grad tensor([-0.0009,  0.0053])\n",
      "Epoch 1240, Loss 2.927730, Params tensor([  5.3622, -17.2737]), Grad tensor([-0.0010,  0.0053])\n",
      "Epoch 1241, Loss 2.927731, Params tensor([  5.3623, -17.2739]), Grad tensor([-0.0009,  0.0053])\n",
      "Epoch 1242, Loss 2.927728, Params tensor([  5.3623, -17.2740]), Grad tensor([-0.0010,  0.0052])\n",
      "Epoch 1243, Loss 2.927730, Params tensor([  5.3623, -17.2742]), Grad tensor([-0.0009,  0.0052])\n",
      "Epoch 1244, Loss 2.927729, Params tensor([  5.3623, -17.2743]), Grad tensor([-0.0009,  0.0052])\n",
      "Epoch 1245, Loss 2.927727, Params tensor([  5.3624, -17.2745]), Grad tensor([-0.0009,  0.0052])\n",
      "Epoch 1246, Loss 2.927726, Params tensor([  5.3624, -17.2747]), Grad tensor([-0.0009,  0.0051])\n",
      "Epoch 1247, Loss 2.927725, Params tensor([  5.3624, -17.2748]), Grad tensor([-0.0009,  0.0051])\n",
      "Epoch 1248, Loss 2.927724, Params tensor([  5.3625, -17.2750]), Grad tensor([-0.0009,  0.0051])\n",
      "Epoch 1249, Loss 2.927724, Params tensor([  5.3625, -17.2751]), Grad tensor([-0.0009,  0.0051])\n",
      "Epoch 1250, Loss 2.927724, Params tensor([  5.3625, -17.2753]), Grad tensor([-0.0009,  0.0050])\n",
      "Epoch 1251, Loss 2.927723, Params tensor([  5.3625, -17.2754]), Grad tensor([-0.0009,  0.0050])\n",
      "Epoch 1252, Loss 2.927721, Params tensor([  5.3626, -17.2756]), Grad tensor([-0.0009,  0.0050])\n",
      "Epoch 1253, Loss 2.927722, Params tensor([  5.3626, -17.2757]), Grad tensor([-0.0008,  0.0050])\n",
      "Epoch 1254, Loss 2.927721, Params tensor([  5.3626, -17.2759]), Grad tensor([-0.0009,  0.0049])\n",
      "Epoch 1255, Loss 2.927719, Params tensor([  5.3626, -17.2760]), Grad tensor([-0.0008,  0.0049])\n",
      "Epoch 1256, Loss 2.927720, Params tensor([  5.3627, -17.2762]), Grad tensor([-0.0009,  0.0049])\n",
      "Epoch 1257, Loss 2.927717, Params tensor([  5.3627, -17.2763]), Grad tensor([-0.0008,  0.0049])\n",
      "Epoch 1258, Loss 2.927716, Params tensor([  5.3627, -17.2764]), Grad tensor([-0.0009,  0.0048])\n",
      "Epoch 1259, Loss 2.927716, Params tensor([  5.3627, -17.2766]), Grad tensor([-0.0009,  0.0048])\n",
      "Epoch 1260, Loss 2.927716, Params tensor([  5.3628, -17.2767]), Grad tensor([-0.0008,  0.0048])\n",
      "Epoch 1261, Loss 2.927716, Params tensor([  5.3628, -17.2769]), Grad tensor([-0.0008,  0.0048])\n",
      "Epoch 1262, Loss 2.927714, Params tensor([  5.3628, -17.2770]), Grad tensor([-0.0008,  0.0047])\n",
      "Epoch 1263, Loss 2.927714, Params tensor([  5.3628, -17.2772]), Grad tensor([-0.0009,  0.0047])\n",
      "Epoch 1264, Loss 2.927713, Params tensor([  5.3629, -17.2773]), Grad tensor([-0.0008,  0.0047])\n",
      "Epoch 1265, Loss 2.927712, Params tensor([  5.3629, -17.2774]), Grad tensor([-0.0008,  0.0047])\n",
      "Epoch 1266, Loss 2.927711, Params tensor([  5.3629, -17.2776]), Grad tensor([-0.0008,  0.0046])\n",
      "Epoch 1267, Loss 2.927711, Params tensor([  5.3629, -17.2777]), Grad tensor([-0.0008,  0.0046])\n",
      "Epoch 1268, Loss 2.927709, Params tensor([  5.3630, -17.2779]), Grad tensor([-0.0008,  0.0046])\n",
      "Epoch 1269, Loss 2.927709, Params tensor([  5.3630, -17.2780]), Grad tensor([-0.0008,  0.0046])\n",
      "Epoch 1270, Loss 2.927708, Params tensor([  5.3630, -17.2781]), Grad tensor([-0.0008,  0.0046])\n",
      "Epoch 1271, Loss 2.927709, Params tensor([  5.3630, -17.2783]), Grad tensor([-0.0008,  0.0045])\n",
      "Epoch 1272, Loss 2.927707, Params tensor([  5.3631, -17.2784]), Grad tensor([-0.0008,  0.0045])\n",
      "Epoch 1273, Loss 2.927706, Params tensor([  5.3631, -17.2785]), Grad tensor([-0.0008,  0.0045])\n",
      "Epoch 1274, Loss 2.927707, Params tensor([  5.3631, -17.2787]), Grad tensor([-0.0008,  0.0045])\n",
      "Epoch 1275, Loss 2.927705, Params tensor([  5.3631, -17.2788]), Grad tensor([-0.0008,  0.0044])\n",
      "Epoch 1276, Loss 2.927705, Params tensor([  5.3632, -17.2789]), Grad tensor([-0.0007,  0.0044])\n",
      "Epoch 1277, Loss 2.927705, Params tensor([  5.3632, -17.2791]), Grad tensor([-0.0008,  0.0044])\n",
      "Epoch 1278, Loss 2.927704, Params tensor([  5.3632, -17.2792]), Grad tensor([-0.0007,  0.0044])\n",
      "Epoch 1279, Loss 2.927702, Params tensor([  5.3632, -17.2793]), Grad tensor([-0.0008,  0.0043])\n",
      "Epoch 1280, Loss 2.927703, Params tensor([  5.3632, -17.2795]), Grad tensor([-0.0007,  0.0043])\n",
      "Epoch 1281, Loss 2.927702, Params tensor([  5.3633, -17.2796]), Grad tensor([-0.0008,  0.0043])\n",
      "Epoch 1282, Loss 2.927701, Params tensor([  5.3633, -17.2797]), Grad tensor([-0.0007,  0.0043])\n",
      "Epoch 1283, Loss 2.927702, Params tensor([  5.3633, -17.2798]), Grad tensor([-0.0008,  0.0043])\n",
      "Epoch 1284, Loss 2.927701, Params tensor([  5.3633, -17.2800]), Grad tensor([-0.0007,  0.0042])\n",
      "Epoch 1285, Loss 2.927702, Params tensor([  5.3634, -17.2801]), Grad tensor([-0.0008,  0.0042])\n",
      "Epoch 1286, Loss 2.927701, Params tensor([  5.3634, -17.2802]), Grad tensor([-0.0007,  0.0042])\n",
      "Epoch 1287, Loss 2.927699, Params tensor([  5.3634, -17.2804]), Grad tensor([-0.0007,  0.0042])\n",
      "Epoch 1288, Loss 2.927696, Params tensor([  5.3634, -17.2805]), Grad tensor([-0.0008,  0.0041])\n",
      "Epoch 1289, Loss 2.927697, Params tensor([  5.3634, -17.2806]), Grad tensor([-0.0007,  0.0041])\n",
      "Epoch 1290, Loss 2.927697, Params tensor([  5.3635, -17.2807]), Grad tensor([-0.0007,  0.0041])\n",
      "Epoch 1291, Loss 2.927698, Params tensor([  5.3635, -17.2808]), Grad tensor([-0.0007,  0.0041])\n",
      "Epoch 1292, Loss 2.927696, Params tensor([  5.3635, -17.2810]), Grad tensor([-0.0007,  0.0041])\n",
      "Epoch 1293, Loss 2.927695, Params tensor([  5.3635, -17.2811]), Grad tensor([-0.0007,  0.0040])\n",
      "Epoch 1294, Loss 2.927696, Params tensor([  5.3636, -17.2812]), Grad tensor([-0.0007,  0.0040])\n",
      "Epoch 1295, Loss 2.927694, Params tensor([  5.3636, -17.2813]), Grad tensor([-0.0007,  0.0040])\n",
      "Epoch 1296, Loss 2.927695, Params tensor([  5.3636, -17.2815]), Grad tensor([-0.0007,  0.0040])\n",
      "Epoch 1297, Loss 2.927696, Params tensor([  5.3636, -17.2816]), Grad tensor([-0.0007,  0.0040])\n",
      "Epoch 1298, Loss 2.927693, Params tensor([  5.3636, -17.2817]), Grad tensor([-0.0007,  0.0039])\n",
      "Epoch 1299, Loss 2.927693, Params tensor([  5.3637, -17.2818]), Grad tensor([-0.0007,  0.0039])\n",
      "Epoch 1300, Loss 2.927692, Params tensor([  5.3637, -17.2819]), Grad tensor([-0.0007,  0.0039])\n",
      "Epoch 1301, Loss 2.927692, Params tensor([  5.3637, -17.2820]), Grad tensor([-0.0007,  0.0039])\n",
      "Epoch 1302, Loss 2.927692, Params tensor([  5.3637, -17.2822]), Grad tensor([-0.0007,  0.0039])\n",
      "Epoch 1303, Loss 2.927692, Params tensor([  5.3637, -17.2823]), Grad tensor([-0.0007,  0.0038])\n",
      "Epoch 1304, Loss 2.927690, Params tensor([  5.3638, -17.2824]), Grad tensor([-0.0007,  0.0038])\n",
      "Epoch 1305, Loss 2.927688, Params tensor([  5.3638, -17.2825]), Grad tensor([-0.0006,  0.0038])\n",
      "Epoch 1306, Loss 2.927689, Params tensor([  5.3638, -17.2826]), Grad tensor([-0.0007,  0.0038])\n",
      "Epoch 1307, Loss 2.927689, Params tensor([  5.3638, -17.2827]), Grad tensor([-0.0006,  0.0038])\n",
      "Epoch 1308, Loss 2.927688, Params tensor([  5.3638, -17.2828]), Grad tensor([-0.0007,  0.0037])\n",
      "Epoch 1309, Loss 2.927688, Params tensor([  5.3639, -17.2829]), Grad tensor([-0.0007,  0.0037])\n",
      "Epoch 1310, Loss 2.927689, Params tensor([  5.3639, -17.2831]), Grad tensor([-0.0007,  0.0037])\n",
      "Epoch 1311, Loss 2.927687, Params tensor([  5.3639, -17.2832]), Grad tensor([-0.0007,  0.0037])\n",
      "Epoch 1312, Loss 2.927686, Params tensor([  5.3639, -17.2833]), Grad tensor([-0.0006,  0.0037])\n",
      "Epoch 1313, Loss 2.927686, Params tensor([  5.3639, -17.2834]), Grad tensor([-0.0006,  0.0037])\n",
      "Epoch 1314, Loss 2.927686, Params tensor([  5.3640, -17.2835]), Grad tensor([-0.0006,  0.0036])\n",
      "Epoch 1315, Loss 2.927686, Params tensor([  5.3640, -17.2836]), Grad tensor([-0.0007,  0.0036])\n",
      "Epoch 1316, Loss 2.927686, Params tensor([  5.3640, -17.2837]), Grad tensor([-0.0006,  0.0036])\n",
      "Epoch 1317, Loss 2.927685, Params tensor([  5.3640, -17.2838]), Grad tensor([-0.0006,  0.0036])\n",
      "Epoch 1318, Loss 2.927685, Params tensor([  5.3640, -17.2839]), Grad tensor([-0.0006,  0.0036])\n",
      "Epoch 1319, Loss 2.927685, Params tensor([  5.3641, -17.2840]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1320, Loss 2.927684, Params tensor([  5.3641, -17.2841]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1321, Loss 2.927683, Params tensor([  5.3641, -17.2842]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1322, Loss 2.927682, Params tensor([  5.3641, -17.2844]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1323, Loss 2.927681, Params tensor([  5.3641, -17.2845]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1324, Loss 2.927682, Params tensor([  5.3641, -17.2846]), Grad tensor([-0.0006,  0.0035])\n",
      "Epoch 1325, Loss 2.927682, Params tensor([  5.3642, -17.2847]), Grad tensor([-0.0006,  0.0034])\n",
      "Epoch 1326, Loss 2.927680, Params tensor([  5.3642, -17.2848]), Grad tensor([-0.0006,  0.0034])\n",
      "Epoch 1327, Loss 2.927681, Params tensor([  5.3642, -17.2849]), Grad tensor([-0.0006,  0.0034])\n",
      "Epoch 1328, Loss 2.927682, Params tensor([  5.3642, -17.2850]), Grad tensor([-0.0006,  0.0034])\n",
      "Epoch 1329, Loss 2.927681, Params tensor([  5.3642, -17.2851]), Grad tensor([-0.0006,  0.0034])\n",
      "Epoch 1330, Loss 2.927679, Params tensor([  5.3643, -17.2852]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1331, Loss 2.927680, Params tensor([  5.3643, -17.2853]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1332, Loss 2.927679, Params tensor([  5.3643, -17.2854]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1333, Loss 2.927679, Params tensor([  5.3643, -17.2855]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1334, Loss 2.927678, Params tensor([  5.3643, -17.2856]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1335, Loss 2.927678, Params tensor([  5.3643, -17.2857]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1336, Loss 2.927679, Params tensor([  5.3644, -17.2858]), Grad tensor([-0.0006,  0.0033])\n",
      "Epoch 1337, Loss 2.927677, Params tensor([  5.3644, -17.2859]), Grad tensor([-0.0006,  0.0032])\n",
      "Epoch 1338, Loss 2.927677, Params tensor([  5.3644, -17.2860]), Grad tensor([-0.0006,  0.0032])\n",
      "Epoch 1339, Loss 2.927678, Params tensor([  5.3644, -17.2861]), Grad tensor([-0.0006,  0.0032])\n",
      "Epoch 1340, Loss 2.927676, Params tensor([  5.3644, -17.2861]), Grad tensor([-0.0006,  0.0032])\n",
      "Epoch 1341, Loss 2.927675, Params tensor([  5.3644, -17.2862]), Grad tensor([-0.0006,  0.0032])\n",
      "Epoch 1342, Loss 2.927676, Params tensor([  5.3645, -17.2863]), Grad tensor([-0.0006,  0.0031])\n",
      "Epoch 1343, Loss 2.927676, Params tensor([  5.3645, -17.2864]), Grad tensor([-0.0006,  0.0031])\n",
      "Epoch 1344, Loss 2.927675, Params tensor([  5.3645, -17.2865]), Grad tensor([-0.0005,  0.0031])\n",
      "Epoch 1345, Loss 2.927676, Params tensor([  5.3645, -17.2866]), Grad tensor([-0.0005,  0.0031])\n",
      "Epoch 1346, Loss 2.927676, Params tensor([  5.3645, -17.2867]), Grad tensor([-0.0006,  0.0031])\n",
      "Epoch 1347, Loss 2.927674, Params tensor([  5.3645, -17.2868]), Grad tensor([-0.0005,  0.0031])\n",
      "Epoch 1348, Loss 2.927676, Params tensor([  5.3646, -17.2869]), Grad tensor([-0.0005,  0.0031])\n",
      "Epoch 1349, Loss 2.927674, Params tensor([  5.3646, -17.2870]), Grad tensor([-0.0005,  0.0030])\n",
      "Epoch 1350, Loss 2.927674, Params tensor([  5.3646, -17.2871]), Grad tensor([-0.0005,  0.0030])\n",
      "Epoch 1351, Loss 2.927674, Params tensor([  5.3646, -17.2872]), Grad tensor([-0.0006,  0.0030])\n",
      "Epoch 1352, Loss 2.927672, Params tensor([  5.3646, -17.2873]), Grad tensor([-0.0005,  0.0030])\n",
      "Epoch 1353, Loss 2.927674, Params tensor([  5.3646, -17.2873]), Grad tensor([-0.0005,  0.0030])\n",
      "Epoch 1354, Loss 2.927673, Params tensor([  5.3647, -17.2874]), Grad tensor([-0.0005,  0.0030])\n",
      "Epoch 1355, Loss 2.927672, Params tensor([  5.3647, -17.2875]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1356, Loss 2.927672, Params tensor([  5.3647, -17.2876]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1357, Loss 2.927671, Params tensor([  5.3647, -17.2877]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1358, Loss 2.927671, Params tensor([  5.3647, -17.2878]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1359, Loss 2.927670, Params tensor([  5.3647, -17.2879]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1360, Loss 2.927670, Params tensor([  5.3647, -17.2880]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1361, Loss 2.927671, Params tensor([  5.3648, -17.2880]), Grad tensor([-0.0005,  0.0029])\n",
      "Epoch 1362, Loss 2.927670, Params tensor([  5.3648, -17.2881]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1363, Loss 2.927671, Params tensor([  5.3648, -17.2882]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1364, Loss 2.927670, Params tensor([  5.3648, -17.2883]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1365, Loss 2.927670, Params tensor([  5.3648, -17.2884]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1366, Loss 2.927669, Params tensor([  5.3648, -17.2885]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1367, Loss 2.927670, Params tensor([  5.3649, -17.2886]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1368, Loss 2.927668, Params tensor([  5.3649, -17.2886]), Grad tensor([-0.0005,  0.0028])\n",
      "Epoch 1369, Loss 2.927668, Params tensor([  5.3649, -17.2887]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1370, Loss 2.927667, Params tensor([  5.3649, -17.2888]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1371, Loss 2.927669, Params tensor([  5.3649, -17.2889]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1372, Loss 2.927668, Params tensor([  5.3649, -17.2890]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1373, Loss 2.927670, Params tensor([  5.3649, -17.2890]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1374, Loss 2.927668, Params tensor([  5.3650, -17.2891]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1375, Loss 2.927668, Params tensor([  5.3650, -17.2892]), Grad tensor([-0.0005,  0.0027])\n",
      "Epoch 1376, Loss 2.927667, Params tensor([  5.3650, -17.2893]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1377, Loss 2.927669, Params tensor([  5.3650, -17.2894]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1378, Loss 2.927668, Params tensor([  5.3650, -17.2894]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1379, Loss 2.927666, Params tensor([  5.3650, -17.2895]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1380, Loss 2.927665, Params tensor([  5.3650, -17.2896]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1381, Loss 2.927665, Params tensor([  5.3650, -17.2897]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1382, Loss 2.927665, Params tensor([  5.3651, -17.2897]), Grad tensor([-0.0004,  0.0026])\n",
      "Epoch 1383, Loss 2.927665, Params tensor([  5.3651, -17.2898]), Grad tensor([-0.0005,  0.0026])\n",
      "Epoch 1384, Loss 2.927665, Params tensor([  5.3651, -17.2899]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1385, Loss 2.927665, Params tensor([  5.3651, -17.2900]), Grad tensor([-0.0005,  0.0025])\n",
      "Epoch 1386, Loss 2.927666, Params tensor([  5.3651, -17.2901]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1387, Loss 2.927665, Params tensor([  5.3651, -17.2901]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1388, Loss 2.927664, Params tensor([  5.3651, -17.2902]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1389, Loss 2.927665, Params tensor([  5.3652, -17.2903]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1390, Loss 2.927664, Params tensor([  5.3652, -17.2904]), Grad tensor([-0.0004,  0.0025])\n",
      "Epoch 1391, Loss 2.927664, Params tensor([  5.3652, -17.2904]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1392, Loss 2.927664, Params tensor([  5.3652, -17.2905]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1393, Loss 2.927664, Params tensor([  5.3652, -17.2906]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1394, Loss 2.927664, Params tensor([  5.3652, -17.2906]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1395, Loss 2.927664, Params tensor([  5.3652, -17.2907]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1396, Loss 2.927664, Params tensor([  5.3652, -17.2908]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1397, Loss 2.927664, Params tensor([  5.3653, -17.2909]), Grad tensor([-0.0005,  0.0024])\n",
      "Epoch 1398, Loss 2.927664, Params tensor([  5.3653, -17.2909]), Grad tensor([-0.0004,  0.0024])\n",
      "Epoch 1399, Loss 2.927663, Params tensor([  5.3653, -17.2910]), Grad tensor([-0.0005,  0.0023])\n",
      "Epoch 1400, Loss 2.927662, Params tensor([  5.3653, -17.2911]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1401, Loss 2.927664, Params tensor([  5.3653, -17.2911]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1402, Loss 2.927664, Params tensor([  5.3653, -17.2912]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1403, Loss 2.927663, Params tensor([  5.3653, -17.2913]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1404, Loss 2.927660, Params tensor([  5.3653, -17.2913]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1405, Loss 2.927663, Params tensor([  5.3654, -17.2914]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1406, Loss 2.927661, Params tensor([  5.3654, -17.2915]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1407, Loss 2.927661, Params tensor([  5.3654, -17.2916]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1408, Loss 2.927662, Params tensor([  5.3654, -17.2916]), Grad tensor([-0.0004,  0.0023])\n",
      "Epoch 1409, Loss 2.927661, Params tensor([  5.3654, -17.2917]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1410, Loss 2.927661, Params tensor([  5.3654, -17.2918]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1411, Loss 2.927661, Params tensor([  5.3654, -17.2918]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1412, Loss 2.927661, Params tensor([  5.3654, -17.2919]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1413, Loss 2.927661, Params tensor([  5.3655, -17.2920]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1414, Loss 2.927660, Params tensor([  5.3655, -17.2920]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1415, Loss 2.927660, Params tensor([  5.3655, -17.2921]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1416, Loss 2.927660, Params tensor([  5.3655, -17.2921]), Grad tensor([-0.0004,  0.0022])\n",
      "Epoch 1417, Loss 2.927660, Params tensor([  5.3655, -17.2922]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1418, Loss 2.927660, Params tensor([  5.3655, -17.2923]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1419, Loss 2.927658, Params tensor([  5.3655, -17.2923]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1420, Loss 2.927660, Params tensor([  5.3655, -17.2924]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1421, Loss 2.927660, Params tensor([  5.3655, -17.2925]), Grad tensor([-0.0003,  0.0021])\n",
      "Epoch 1422, Loss 2.927659, Params tensor([  5.3656, -17.2925]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1423, Loss 2.927659, Params tensor([  5.3656, -17.2926]), Grad tensor([-0.0003,  0.0021])\n",
      "Epoch 1424, Loss 2.927659, Params tensor([  5.3656, -17.2927]), Grad tensor([-0.0004,  0.0021])\n",
      "Epoch 1425, Loss 2.927659, Params tensor([  5.3656, -17.2927]), Grad tensor([-0.0003,  0.0021])\n",
      "Epoch 1426, Loss 2.927660, Params tensor([  5.3656, -17.2928]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1427, Loss 2.927658, Params tensor([  5.3656, -17.2928]), Grad tensor([-0.0003,  0.0020])\n",
      "Epoch 1428, Loss 2.927657, Params tensor([  5.3656, -17.2929]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1429, Loss 2.927659, Params tensor([  5.3656, -17.2930]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1430, Loss 2.927657, Params tensor([  5.3656, -17.2930]), Grad tensor([-0.0003,  0.0020])\n",
      "Epoch 1431, Loss 2.927659, Params tensor([  5.3657, -17.2931]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1432, Loss 2.927658, Params tensor([  5.3657, -17.2931]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1433, Loss 2.927658, Params tensor([  5.3657, -17.2932]), Grad tensor([-0.0003,  0.0020])\n",
      "Epoch 1434, Loss 2.927659, Params tensor([  5.3657, -17.2933]), Grad tensor([-0.0003,  0.0020])\n",
      "Epoch 1435, Loss 2.927658, Params tensor([  5.3657, -17.2933]), Grad tensor([-0.0004,  0.0020])\n",
      "Epoch 1436, Loss 2.927657, Params tensor([  5.3657, -17.2934]), Grad tensor([-0.0003,  0.0020])\n",
      "Epoch 1437, Loss 2.927658, Params tensor([  5.3657, -17.2934]), Grad tensor([-0.0004,  0.0019])\n",
      "Epoch 1438, Loss 2.927657, Params tensor([  5.3657, -17.2935]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1439, Loss 2.927658, Params tensor([  5.3657, -17.2935]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1440, Loss 2.927657, Params tensor([  5.3657, -17.2936]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1441, Loss 2.927658, Params tensor([  5.3658, -17.2937]), Grad tensor([-0.0004,  0.0019])\n",
      "Epoch 1442, Loss 2.927657, Params tensor([  5.3658, -17.2937]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1443, Loss 2.927657, Params tensor([  5.3658, -17.2938]), Grad tensor([-0.0004,  0.0019])\n",
      "Epoch 1444, Loss 2.927657, Params tensor([  5.3658, -17.2938]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1445, Loss 2.927657, Params tensor([  5.3658, -17.2939]), Grad tensor([-0.0004,  0.0019])\n",
      "Epoch 1446, Loss 2.927656, Params tensor([  5.3658, -17.2939]), Grad tensor([-0.0003,  0.0019])\n",
      "Epoch 1447, Loss 2.927656, Params tensor([  5.3658, -17.2940]), Grad tensor([-0.0004,  0.0018])\n",
      "Epoch 1448, Loss 2.927656, Params tensor([  5.3658, -17.2941]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1449, Loss 2.927657, Params tensor([  5.3658, -17.2941]), Grad tensor([-0.0004,  0.0018])\n",
      "Epoch 1450, Loss 2.927655, Params tensor([  5.3658, -17.2942]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1451, Loss 2.927656, Params tensor([  5.3659, -17.2942]), Grad tensor([-0.0004,  0.0018])\n",
      "Epoch 1452, Loss 2.927656, Params tensor([  5.3659, -17.2943]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1453, Loss 2.927656, Params tensor([  5.3659, -17.2943]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1454, Loss 2.927656, Params tensor([  5.3659, -17.2944]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1455, Loss 2.927655, Params tensor([  5.3659, -17.2944]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1456, Loss 2.927654, Params tensor([  5.3659, -17.2945]), Grad tensor([-0.0003,  0.0018])\n",
      "Epoch 1457, Loss 2.927655, Params tensor([  5.3659, -17.2945]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1458, Loss 2.927656, Params tensor([  5.3659, -17.2946]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1459, Loss 2.927656, Params tensor([  5.3659, -17.2946]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1460, Loss 2.927655, Params tensor([  5.3659, -17.2947]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1461, Loss 2.927655, Params tensor([  5.3659, -17.2947]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1462, Loss 2.927655, Params tensor([  5.3660, -17.2948]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1463, Loss 2.927654, Params tensor([  5.3660, -17.2948]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1464, Loss 2.927655, Params tensor([  5.3660, -17.2949]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1465, Loss 2.927654, Params tensor([  5.3660, -17.2949]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1466, Loss 2.927656, Params tensor([  5.3660, -17.2950]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1467, Loss 2.927653, Params tensor([  5.3660, -17.2950]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1468, Loss 2.927654, Params tensor([  5.3660, -17.2951]), Grad tensor([-0.0003,  0.0017])\n",
      "Epoch 1469, Loss 2.927654, Params tensor([  5.3660, -17.2951]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1470, Loss 2.927655, Params tensor([  5.3660, -17.2952]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1471, Loss 2.927653, Params tensor([  5.3660, -17.2952]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1472, Loss 2.927654, Params tensor([  5.3660, -17.2953]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1473, Loss 2.927654, Params tensor([  5.3661, -17.2953]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1474, Loss 2.927654, Params tensor([  5.3661, -17.2954]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1475, Loss 2.927654, Params tensor([  5.3661, -17.2954]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1476, Loss 2.927654, Params tensor([  5.3661, -17.2955]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1477, Loss 2.927654, Params tensor([  5.3661, -17.2955]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1478, Loss 2.927654, Params tensor([  5.3661, -17.2956]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1479, Loss 2.927653, Params tensor([  5.3661, -17.2956]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1480, Loss 2.927654, Params tensor([  5.3661, -17.2957]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1481, Loss 2.927654, Params tensor([  5.3661, -17.2957]), Grad tensor([-0.0003,  0.0016])\n",
      "Epoch 1482, Loss 2.927653, Params tensor([  5.3661, -17.2958]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1483, Loss 2.927655, Params tensor([  5.3661, -17.2958]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1484, Loss 2.927652, Params tensor([  5.3661, -17.2959]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1485, Loss 2.927653, Params tensor([  5.3661, -17.2959]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1486, Loss 2.927653, Params tensor([  5.3662, -17.2959]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1487, Loss 2.927654, Params tensor([  5.3662, -17.2960]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1488, Loss 2.927653, Params tensor([  5.3662, -17.2960]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1489, Loss 2.927654, Params tensor([  5.3662, -17.2961]), Grad tensor([-0.0002,  0.0015])\n",
      "Epoch 1490, Loss 2.927653, Params tensor([  5.3662, -17.2961]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1491, Loss 2.927653, Params tensor([  5.3662, -17.2962]), Grad tensor([-0.0002,  0.0015])\n",
      "Epoch 1492, Loss 2.927654, Params tensor([  5.3662, -17.2962]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1493, Loss 2.927653, Params tensor([  5.3662, -17.2963]), Grad tensor([-0.0003,  0.0015])\n",
      "Epoch 1494, Loss 2.927651, Params tensor([  5.3662, -17.2963]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1495, Loss 2.927653, Params tensor([  5.3662, -17.2963]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1496, Loss 2.927651, Params tensor([  5.3662, -17.2964]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1497, Loss 2.927653, Params tensor([  5.3662, -17.2964]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1498, Loss 2.927651, Params tensor([  5.3663, -17.2965]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1499, Loss 2.927652, Params tensor([  5.3663, -17.2965]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1500, Loss 2.927653, Params tensor([  5.3663, -17.2966]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1501, Loss 2.927652, Params tensor([  5.3663, -17.2966]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1502, Loss 2.927653, Params tensor([  5.3663, -17.2966]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1503, Loss 2.927651, Params tensor([  5.3663, -17.2967]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1504, Loss 2.927650, Params tensor([  5.3663, -17.2967]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1505, Loss 2.927651, Params tensor([  5.3663, -17.2968]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1506, Loss 2.927653, Params tensor([  5.3663, -17.2968]), Grad tensor([-0.0003,  0.0014])\n",
      "Epoch 1507, Loss 2.927653, Params tensor([  5.3663, -17.2968]), Grad tensor([-0.0002,  0.0014])\n",
      "Epoch 1508, Loss 2.927653, Params tensor([  5.3663, -17.2969]), Grad tensor([-0.0003,  0.0013])\n",
      "Epoch 1509, Loss 2.927653, Params tensor([  5.3663, -17.2969]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1510, Loss 2.927651, Params tensor([  5.3663, -17.2970]), Grad tensor([-0.0003,  0.0013])\n",
      "Epoch 1511, Loss 2.927652, Params tensor([  5.3663, -17.2970]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1512, Loss 2.927651, Params tensor([  5.3664, -17.2970]), Grad tensor([-0.0003,  0.0013])\n",
      "Epoch 1513, Loss 2.927650, Params tensor([  5.3664, -17.2971]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1514, Loss 2.927651, Params tensor([  5.3664, -17.2971]), Grad tensor([-0.0003,  0.0013])\n",
      "Epoch 1515, Loss 2.927651, Params tensor([  5.3664, -17.2972]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1516, Loss 2.927650, Params tensor([  5.3664, -17.2972]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1517, Loss 2.927650, Params tensor([  5.3664, -17.2972]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1518, Loss 2.927651, Params tensor([  5.3664, -17.2973]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1519, Loss 2.927650, Params tensor([  5.3664, -17.2973]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1520, Loss 2.927650, Params tensor([  5.3664, -17.2974]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1521, Loss 2.927650, Params tensor([  5.3664, -17.2974]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1522, Loss 2.927650, Params tensor([  5.3664, -17.2974]), Grad tensor([-0.0002,  0.0013])\n",
      "Epoch 1523, Loss 2.927650, Params tensor([  5.3664, -17.2975]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1524, Loss 2.927652, Params tensor([  5.3664, -17.2975]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1525, Loss 2.927649, Params tensor([  5.3664, -17.2975]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1526, Loss 2.927651, Params tensor([  5.3664, -17.2976]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1527, Loss 2.927650, Params tensor([  5.3665, -17.2976]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1528, Loss 2.927649, Params tensor([  5.3665, -17.2977]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1529, Loss 2.927650, Params tensor([  5.3665, -17.2977]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1530, Loss 2.927651, Params tensor([  5.3665, -17.2977]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1531, Loss 2.927650, Params tensor([  5.3665, -17.2978]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1532, Loss 2.927650, Params tensor([  5.3665, -17.2978]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1533, Loss 2.927650, Params tensor([  5.3665, -17.2978]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1534, Loss 2.927651, Params tensor([  5.3665, -17.2979]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1535, Loss 2.927649, Params tensor([  5.3665, -17.2979]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1536, Loss 2.927650, Params tensor([  5.3665, -17.2979]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1537, Loss 2.927651, Params tensor([  5.3665, -17.2980]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1538, Loss 2.927649, Params tensor([  5.3665, -17.2980]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1539, Loss 2.927650, Params tensor([  5.3665, -17.2980]), Grad tensor([-0.0002,  0.0012])\n",
      "Epoch 1540, Loss 2.927650, Params tensor([  5.3665, -17.2981]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1541, Loss 2.927650, Params tensor([  5.3665, -17.2981]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1542, Loss 2.927650, Params tensor([  5.3665, -17.2981]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1543, Loss 2.927650, Params tensor([  5.3666, -17.2982]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1544, Loss 2.927649, Params tensor([  5.3666, -17.2982]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1545, Loss 2.927649, Params tensor([  5.3666, -17.2982]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1546, Loss 2.927649, Params tensor([  5.3666, -17.2983]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1547, Loss 2.927650, Params tensor([  5.3666, -17.2983]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1548, Loss 2.927649, Params tensor([  5.3666, -17.2983]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1549, Loss 2.927650, Params tensor([  5.3666, -17.2984]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1550, Loss 2.927649, Params tensor([  5.3666, -17.2984]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1551, Loss 2.927649, Params tensor([  5.3666, -17.2984]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1552, Loss 2.927649, Params tensor([  5.3666, -17.2985]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1553, Loss 2.927649, Params tensor([  5.3666, -17.2985]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1554, Loss 2.927651, Params tensor([  5.3666, -17.2985]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1555, Loss 2.927649, Params tensor([  5.3666, -17.2986]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1556, Loss 2.927650, Params tensor([  5.3666, -17.2986]), Grad tensor([-0.0002,  0.0011])\n",
      "Epoch 1557, Loss 2.927650, Params tensor([  5.3666, -17.2986]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1558, Loss 2.927649, Params tensor([  5.3666, -17.2987]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1559, Loss 2.927650, Params tensor([  5.3666, -17.2987]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1560, Loss 2.927649, Params tensor([  5.3666, -17.2987]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1561, Loss 2.927651, Params tensor([  5.3667, -17.2988]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1562, Loss 2.927650, Params tensor([  5.3667, -17.2988]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1563, Loss 2.927650, Params tensor([  5.3667, -17.2988]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1564, Loss 2.927649, Params tensor([  5.3667, -17.2988]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1565, Loss 2.927649, Params tensor([  5.3667, -17.2989]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1566, Loss 2.927650, Params tensor([  5.3667, -17.2989]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1567, Loss 2.927648, Params tensor([  5.3667, -17.2989]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1568, Loss 2.927649, Params tensor([  5.3667, -17.2990]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1569, Loss 2.927648, Params tensor([  5.3667, -17.2990]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1570, Loss 2.927650, Params tensor([  5.3667, -17.2990]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1571, Loss 2.927649, Params tensor([  5.3667, -17.2991]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1572, Loss 2.927649, Params tensor([  5.3667, -17.2991]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1573, Loss 2.927650, Params tensor([  5.3667, -17.2991]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1574, Loss 2.927648, Params tensor([  5.3667, -17.2991]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1575, Loss 2.927650, Params tensor([  5.3667, -17.2992]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1576, Loss 2.927649, Params tensor([  5.3667, -17.2992]), Grad tensor([-0.0002,  0.0010])\n",
      "Epoch 1577, Loss 2.927649, Params tensor([  5.3667, -17.2992]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1578, Loss 2.927649, Params tensor([  5.3667, -17.2993]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1579, Loss 2.927648, Params tensor([  5.3667, -17.2993]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1580, Loss 2.927648, Params tensor([  5.3668, -17.2993]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1581, Loss 2.927648, Params tensor([  5.3668, -17.2993]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1582, Loss 2.927648, Params tensor([  5.3668, -17.2994]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1583, Loss 2.927649, Params tensor([  5.3668, -17.2994]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1584, Loss 2.927647, Params tensor([  5.3668, -17.2994]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1585, Loss 2.927649, Params tensor([  5.3668, -17.2995]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1586, Loss 2.927648, Params tensor([  5.3668, -17.2995]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1587, Loss 2.927648, Params tensor([  5.3668, -17.2995]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1588, Loss 2.927648, Params tensor([  5.3668, -17.2995]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1589, Loss 2.927648, Params tensor([  5.3668, -17.2996]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1590, Loss 2.927649, Params tensor([  5.3668, -17.2996]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1591, Loss 2.927649, Params tensor([  5.3668, -17.2996]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1592, Loss 2.927649, Params tensor([  5.3668, -17.2996]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1593, Loss 2.927649, Params tensor([  5.3668, -17.2997]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1594, Loss 2.927648, Params tensor([  5.3668, -17.2997]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1595, Loss 2.927648, Params tensor([  5.3668, -17.2997]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1596, Loss 2.927648, Params tensor([  5.3668, -17.2997]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1597, Loss 2.927649, Params tensor([  5.3668, -17.2998]), Grad tensor([-0.0002,  0.0009])\n",
      "Epoch 1598, Loss 2.927646, Params tensor([  5.3668, -17.2998]), Grad tensor([-0.0001,  0.0009])\n",
      "Epoch 1599, Loss 2.927648, Params tensor([  5.3668, -17.2998]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1600, Loss 2.927648, Params tensor([  5.3668, -17.2998]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1601, Loss 2.927648, Params tensor([  5.3669, -17.2999]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1602, Loss 2.927648, Params tensor([  5.3669, -17.2999]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1603, Loss 2.927647, Params tensor([  5.3669, -17.2999]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1604, Loss 2.927648, Params tensor([  5.3669, -17.2999]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1605, Loss 2.927648, Params tensor([  5.3669, -17.3000]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1606, Loss 2.927649, Params tensor([  5.3669, -17.3000]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1607, Loss 2.927647, Params tensor([  5.3669, -17.3000]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1608, Loss 2.927647, Params tensor([  5.3669, -17.3000]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1609, Loss 2.927648, Params tensor([  5.3669, -17.3001]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1610, Loss 2.927647, Params tensor([  5.3669, -17.3001]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1611, Loss 2.927648, Params tensor([  5.3669, -17.3001]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1612, Loss 2.927648, Params tensor([  5.3669, -17.3001]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1613, Loss 2.927648, Params tensor([  5.3669, -17.3002]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1614, Loss 2.927649, Params tensor([  5.3669, -17.3002]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1615, Loss 2.927648, Params tensor([  5.3669, -17.3002]), Grad tensor([-0.0002,  0.0008])\n",
      "Epoch 1616, Loss 2.927649, Params tensor([  5.3669, -17.3002]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1617, Loss 2.927648, Params tensor([  5.3669, -17.3003]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1618, Loss 2.927649, Params tensor([  5.3669, -17.3003]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1619, Loss 2.927649, Params tensor([  5.3669, -17.3003]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1620, Loss 2.927648, Params tensor([  5.3669, -17.3003]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1621, Loss 2.927648, Params tensor([  5.3669, -17.3003]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1622, Loss 2.927648, Params tensor([  5.3669, -17.3004]), Grad tensor([-0.0001,  0.0008])\n",
      "Epoch 1623, Loss 2.927647, Params tensor([  5.3669, -17.3004]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1624, Loss 2.927647, Params tensor([  5.3669, -17.3004]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1625, Loss 2.927648, Params tensor([  5.3670, -17.3004]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1626, Loss 2.927648, Params tensor([  5.3670, -17.3005]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1627, Loss 2.927647, Params tensor([  5.3670, -17.3005]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1628, Loss 2.927647, Params tensor([  5.3670, -17.3005]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1629, Loss 2.927649, Params tensor([  5.3670, -17.3005]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1630, Loss 2.927648, Params tensor([  5.3670, -17.3005]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1631, Loss 2.927648, Params tensor([  5.3670, -17.3006]), Grad tensor([-9.6560e-05,  7.2512e-04])\n",
      "Epoch 1632, Loss 2.927647, Params tensor([  5.3670, -17.3006]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1633, Loss 2.927646, Params tensor([  5.3670, -17.3006]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1634, Loss 2.927647, Params tensor([  5.3670, -17.3006]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1635, Loss 2.927648, Params tensor([  5.3670, -17.3007]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1636, Loss 2.927649, Params tensor([  5.3670, -17.3007]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1637, Loss 2.927648, Params tensor([  5.3670, -17.3007]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1638, Loss 2.927647, Params tensor([  5.3670, -17.3007]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1639, Loss 2.927647, Params tensor([  5.3670, -17.3007]), Grad tensor([-0.0002,  0.0007])\n",
      "Epoch 1640, Loss 2.927648, Params tensor([  5.3670, -17.3008]), Grad tensor([-9.0122e-05,  6.9305e-04])\n",
      "Epoch 1641, Loss 2.927648, Params tensor([  5.3670, -17.3008]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1642, Loss 2.927648, Params tensor([  5.3670, -17.3008]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1643, Loss 2.927646, Params tensor([  5.3670, -17.3008]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1644, Loss 2.927647, Params tensor([  5.3670, -17.3008]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1645, Loss 2.927646, Params tensor([  5.3670, -17.3009]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1646, Loss 2.927649, Params tensor([  5.3670, -17.3009]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1647, Loss 2.927648, Params tensor([  5.3670, -17.3009]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1648, Loss 2.927647, Params tensor([  5.3670, -17.3009]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1649, Loss 2.927647, Params tensor([  5.3670, -17.3009]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1650, Loss 2.927647, Params tensor([  5.3670, -17.3010]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1651, Loss 2.927647, Params tensor([  5.3670, -17.3010]), Grad tensor([-0.0001,  0.0007])\n",
      "Epoch 1652, Loss 2.927647, Params tensor([  5.3670, -17.3010]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1653, Loss 2.927646, Params tensor([  5.3671, -17.3010]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1654, Loss 2.927647, Params tensor([  5.3671, -17.3010]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1655, Loss 2.927646, Params tensor([  5.3671, -17.3011]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1656, Loss 2.927647, Params tensor([  5.3671, -17.3011]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1657, Loss 2.927648, Params tensor([  5.3671, -17.3011]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1658, Loss 2.927648, Params tensor([  5.3671, -17.3011]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1659, Loss 2.927647, Params tensor([  5.3671, -17.3011]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1660, Loss 2.927647, Params tensor([  5.3671, -17.3011]), Grad tensor([-9.0718e-05,  6.2406e-04])\n",
      "Epoch 1661, Loss 2.927647, Params tensor([  5.3671, -17.3012]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1662, Loss 2.927646, Params tensor([  5.3671, -17.3012]), Grad tensor([-9.3639e-05,  6.1688e-04])\n",
      "Epoch 1663, Loss 2.927647, Params tensor([  5.3671, -17.3012]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1664, Loss 2.927647, Params tensor([  5.3671, -17.3012]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1665, Loss 2.927647, Params tensor([  5.3671, -17.3012]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1666, Loss 2.927646, Params tensor([  5.3671, -17.3013]), Grad tensor([-7.7665e-05,  6.0654e-04])\n",
      "Epoch 1667, Loss 2.927648, Params tensor([  5.3671, -17.3013]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1668, Loss 2.927646, Params tensor([  5.3671, -17.3013]), Grad tensor([-9.0778e-05,  5.9810e-04])\n",
      "Epoch 1669, Loss 2.927647, Params tensor([  5.3671, -17.3013]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1670, Loss 2.927647, Params tensor([  5.3671, -17.3013]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1671, Loss 2.927647, Params tensor([  5.3671, -17.3013]), Grad tensor([-9.3758e-05,  5.8880e-04])\n",
      "Epoch 1672, Loss 2.927648, Params tensor([  5.3671, -17.3014]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1673, Loss 2.927646, Params tensor([  5.3671, -17.3014]), Grad tensor([-8.2970e-05,  5.8427e-04])\n",
      "Epoch 1674, Loss 2.927647, Params tensor([  5.3671, -17.3014]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1675, Loss 2.927647, Params tensor([  5.3671, -17.3014]), Grad tensor([-7.5400e-05,  5.7989e-04])\n",
      "Epoch 1676, Loss 2.927647, Params tensor([  5.3671, -17.3014]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1677, Loss 2.927647, Params tensor([  5.3671, -17.3015]), Grad tensor([-9.8050e-05,  5.6991e-04])\n",
      "Epoch 1678, Loss 2.927647, Params tensor([  5.3671, -17.3015]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1679, Loss 2.927647, Params tensor([  5.3671, -17.3015]), Grad tensor([-8.4698e-05,  5.6615e-04])\n",
      "Epoch 1680, Loss 2.927646, Params tensor([  5.3671, -17.3015]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1681, Loss 2.927646, Params tensor([  5.3671, -17.3015]), Grad tensor([-7.5340e-05,  5.6165e-04])\n",
      "Epoch 1682, Loss 2.927647, Params tensor([  5.3671, -17.3015]), Grad tensor([-0.0001,  0.0006])\n",
      "Epoch 1683, Loss 2.927646, Params tensor([  5.3671, -17.3016]), Grad tensor([-9.7752e-05,  5.5179e-04])\n",
      "Epoch 1684, Loss 2.927647, Params tensor([  5.3672, -17.3016]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1685, Loss 2.927647, Params tensor([  5.3672, -17.3016]), Grad tensor([-9.1195e-05,  5.4696e-04])\n",
      "Epoch 1686, Loss 2.927648, Params tensor([  5.3672, -17.3016]), Grad tensor([-9.5248e-05,  5.4315e-04])\n",
      "Epoch 1687, Loss 2.927647, Params tensor([  5.3672, -17.3016]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1688, Loss 2.927648, Params tensor([  5.3672, -17.3016]), Grad tensor([-6.8545e-05,  5.4225e-04])\n",
      "Epoch 1689, Loss 2.927647, Params tensor([  5.3672, -17.3017]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1690, Loss 2.927648, Params tensor([  5.3672, -17.3017]), Grad tensor([-6.4433e-05,  5.3719e-04])\n",
      "Epoch 1691, Loss 2.927647, Params tensor([  5.3672, -17.3017]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1692, Loss 2.927647, Params tensor([  5.3672, -17.3017]), Grad tensor([-7.4446e-05,  5.3030e-04])\n",
      "Epoch 1693, Loss 2.927647, Params tensor([  5.3672, -17.3017]), Grad tensor([-9.0420e-05,  5.2461e-04])\n",
      "Epoch 1694, Loss 2.927646, Params tensor([  5.3672, -17.3017]), Grad tensor([-8.2910e-05,  5.2324e-04])\n",
      "Epoch 1695, Loss 2.927646, Params tensor([  5.3672, -17.3017]), Grad tensor([-9.9123e-05,  5.1752e-04])\n",
      "Epoch 1696, Loss 2.927648, Params tensor([  5.3672, -17.3018]), Grad tensor([-8.7559e-05,  5.1707e-04])\n",
      "Epoch 1697, Loss 2.927647, Params tensor([  5.3672, -17.3018]), Grad tensor([-8.0287e-05,  5.1576e-04])\n",
      "Epoch 1698, Loss 2.927647, Params tensor([  5.3672, -17.3018]), Grad tensor([-9.7454e-05,  5.1013e-04])\n",
      "Epoch 1699, Loss 2.927647, Params tensor([  5.3672, -17.3018]), Grad tensor([-9.4175e-05,  5.0825e-04])\n",
      "Epoch 1700, Loss 2.927648, Params tensor([  5.3672, -17.3018]), Grad tensor([-8.2672e-05,  5.0744e-04])\n",
      "Epoch 1701, Loss 2.927647, Params tensor([  5.3672, -17.3018]), Grad tensor([-9.3102e-05,  5.0277e-04])\n",
      "Epoch 1702, Loss 2.927647, Params tensor([  5.3672, -17.3019]), Grad tensor([-8.4221e-05,  5.0169e-04])\n",
      "Epoch 1703, Loss 2.927647, Params tensor([  5.3672, -17.3019]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1704, Loss 2.927646, Params tensor([  5.3672, -17.3019]), Grad tensor([-6.6161e-05,  4.9946e-04])\n",
      "Epoch 1705, Loss 2.927646, Params tensor([  5.3672, -17.3019]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1706, Loss 2.927647, Params tensor([  5.3672, -17.3019]), Grad tensor([-7.0274e-05,  4.9350e-04])\n",
      "Epoch 1707, Loss 2.927647, Params tensor([  5.3672, -17.3019]), Grad tensor([-0.0001,  0.0005])\n",
      "Epoch 1708, Loss 2.927647, Params tensor([  5.3672, -17.3019]), Grad tensor([-7.6175e-05,  4.8685e-04])\n",
      "Epoch 1709, Loss 2.927648, Params tensor([  5.3672, -17.3020]), Grad tensor([-9.6083e-05,  4.8101e-04])\n",
      "Epoch 1710, Loss 2.927646, Params tensor([  5.3672, -17.3020]), Grad tensor([-8.9705e-05,  4.7925e-04])\n",
      "Epoch 1711, Loss 2.927647, Params tensor([  5.3672, -17.3020]), Grad tensor([-8.1480e-05,  4.7803e-04])\n",
      "Epoch 1712, Loss 2.927647, Params tensor([  5.3672, -17.3020]), Grad tensor([-9.1493e-05,  4.7365e-04])\n",
      "Epoch 1713, Loss 2.927647, Params tensor([  5.3672, -17.3020]), Grad tensor([-6.7830e-05,  4.7562e-04])\n",
      "Epoch 1714, Loss 2.927648, Params tensor([  5.3672, -17.3020]), Grad tensor([-9.4712e-05,  4.6861e-04])\n",
      "Epoch 1715, Loss 2.927648, Params tensor([  5.3672, -17.3020]), Grad tensor([-6.6519e-05,  4.7123e-04])\n",
      "Epoch 1716, Loss 2.927647, Params tensor([  5.3672, -17.3021]), Grad tensor([-8.1003e-05,  4.6596e-04])\n",
      "Epoch 1717, Loss 2.927646, Params tensor([  5.3672, -17.3021]), Grad tensor([-8.6367e-05,  4.6274e-04])\n",
      "Epoch 1718, Loss 2.927647, Params tensor([  5.3672, -17.3021]), Grad tensor([-8.6367e-05,  4.6068e-04])\n",
      "Epoch 1719, Loss 2.927646, Params tensor([  5.3672, -17.3021]), Grad tensor([-8.0049e-05,  4.5907e-04])\n",
      "Epoch 1720, Loss 2.927646, Params tensor([  5.3672, -17.3021]), Grad tensor([-7.8261e-05,  4.5738e-04])\n",
      "Epoch 1721, Loss 2.927648, Params tensor([  5.3672, -17.3021]), Grad tensor([-7.8142e-05,  4.5508e-04])\n",
      "Epoch 1722, Loss 2.927647, Params tensor([  5.3673, -17.3021]), Grad tensor([-8.2195e-05,  4.5204e-04])\n",
      "Epoch 1723, Loss 2.927647, Params tensor([  5.3673, -17.3021]), Grad tensor([-7.5519e-05,  4.5088e-04])\n",
      "Epoch 1724, Loss 2.927647, Params tensor([  5.3673, -17.3022]), Grad tensor([-7.4923e-05,  4.4879e-04])\n",
      "Epoch 1725, Loss 2.927647, Params tensor([  5.3673, -17.3022]), Grad tensor([-6.8963e-05,  4.4733e-04])\n",
      "Epoch 1726, Loss 2.927647, Params tensor([  5.3673, -17.3022]), Grad tensor([-9.2447e-05,  4.4072e-04])\n",
      "Epoch 1727, Loss 2.927646, Params tensor([  5.3673, -17.3022]), Grad tensor([-6.6578e-05,  4.4295e-04])\n",
      "Epoch 1728, Loss 2.927646, Params tensor([  5.3673, -17.3022]), Grad tensor([-9.3937e-05,  4.3592e-04])\n",
      "Epoch 1729, Loss 2.927648, Params tensor([  5.3673, -17.3022]), Grad tensor([-6.2644e-05,  4.3911e-04])\n",
      "Epoch 1730, Loss 2.927647, Params tensor([  5.3673, -17.3022]), Grad tensor([-9.1434e-05,  4.3154e-04])\n",
      "Epoch 1731, Loss 2.927647, Params tensor([  5.3673, -17.3023]), Grad tensor([-6.0022e-05,  4.3491e-04])\n",
      "Epoch 1732, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-9.1076e-05,  4.2722e-04])\n",
      "Epoch 1733, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-4.9055e-05,  4.3195e-04])\n",
      "Epoch 1734, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-0.0001,  0.0004])\n",
      "Epoch 1735, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-5.1677e-05,  4.2668e-04])\n",
      "Epoch 1736, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-0.0001,  0.0004])\n",
      "Epoch 1737, Loss 2.927646, Params tensor([  5.3673, -17.3023]), Grad tensor([-7.6830e-05,  4.1765e-04])\n",
      "Epoch 1738, Loss 2.927647, Params tensor([  5.3673, -17.3023]), Grad tensor([-7.6294e-05,  4.1562e-04])\n",
      "Epoch 1739, Loss 2.927648, Params tensor([  5.3673, -17.3024]), Grad tensor([-7.6115e-05,  4.1333e-04])\n",
      "Epoch 1740, Loss 2.927647, Params tensor([  5.3673, -17.3024]), Grad tensor([-7.0691e-05,  4.1190e-04])\n",
      "Epoch 1741, Loss 2.927648, Params tensor([  5.3673, -17.3024]), Grad tensor([-7.7009e-05,  4.0883e-04])\n",
      "Epoch 1742, Loss 2.927647, Params tensor([  5.3673, -17.3024]), Grad tensor([-5.4181e-05,  4.1077e-04])\n",
      "Epoch 1743, Loss 2.927646, Params tensor([  5.3673, -17.3024]), Grad tensor([-8.6188e-05,  4.0314e-04])\n",
      "Epoch 1744, Loss 2.927645, Params tensor([  5.3673, -17.3024]), Grad tensor([-6.9380e-05,  4.0400e-04])\n",
      "Epoch 1745, Loss 2.927646, Params tensor([  5.3673, -17.3024]), Grad tensor([-7.3075e-05,  4.0138e-04])\n",
      "Epoch 1746, Loss 2.927645, Params tensor([  5.3673, -17.3024]), Grad tensor([-5.1200e-05,  4.0329e-04])\n",
      "Epoch 1747, Loss 2.927647, Params tensor([  5.3673, -17.3025]), Grad tensor([-9.0480e-05,  3.9449e-04])\n",
      "Epoch 1748, Loss 2.927646, Params tensor([  5.3673, -17.3025]), Grad tensor([-3.6061e-05,  4.0203e-04])\n",
      "Epoch 1749, Loss 2.927646, Params tensor([  5.3673, -17.3025]), Grad tensor([-0.0001,  0.0004])\n",
      "Epoch 1750, Loss 2.927648, Params tensor([  5.3673, -17.3025]), Grad tensor([-5.5015e-05,  3.9479e-04])\n",
      "Epoch 1751, Loss 2.927646, Params tensor([  5.3673, -17.3025]), Grad tensor([-8.7261e-05,  3.8701e-04])\n",
      "Epoch 1752, Loss 2.927645, Params tensor([  5.3673, -17.3025]), Grad tensor([-6.2168e-05,  3.8925e-04])\n",
      "Epoch 1753, Loss 2.927646, Params tensor([  5.3673, -17.3025]), Grad tensor([-6.4731e-05,  3.8660e-04])\n",
      "Epoch 1754, Loss 2.927646, Params tensor([  5.3673, -17.3025]), Grad tensor([-7.3016e-05,  3.8314e-04])\n",
      "Epoch 1755, Loss 2.927647, Params tensor([  5.3673, -17.3025]), Grad tensor([-5.2214e-05,  3.8517e-04])\n",
      "Epoch 1756, Loss 2.927647, Params tensor([  5.3673, -17.3026]), Grad tensor([-8.7857e-05,  3.7673e-04])\n",
      "Epoch 1757, Loss 2.927647, Params tensor([  5.3673, -17.3026]), Grad tensor([-3.8505e-05,  3.8338e-04])\n",
      "Epoch 1758, Loss 2.927646, Params tensor([  5.3673, -17.3026]), Grad tensor([-0.0001,  0.0004])\n",
      "Epoch 1759, Loss 2.927648, Params tensor([  5.3673, -17.3026]), Grad tensor([-4.7624e-05,  3.7792e-04])\n",
      "Epoch 1760, Loss 2.927647, Params tensor([  5.3673, -17.3026]), Grad tensor([-8.8751e-05,  3.6880e-04])\n",
      "Epoch 1761, Loss 2.927646, Params tensor([  5.3673, -17.3026]), Grad tensor([-3.6418e-05,  3.7584e-04])\n",
      "Epoch 1762, Loss 2.927646, Params tensor([  5.3673, -17.3026]), Grad tensor([-9.6560e-05,  3.6320e-04])\n",
      "Epoch 1763, Loss 2.927645, Params tensor([  5.3673, -17.3026]), Grad tensor([-5.4002e-05,  3.6871e-04])\n",
      "Epoch 1764, Loss 2.927647, Params tensor([  5.3673, -17.3026]), Grad tensor([-7.7367e-05,  3.6249e-04])\n",
      "Epoch 1765, Loss 2.927647, Params tensor([  5.3673, -17.3027]), Grad tensor([-5.9307e-05,  3.6356e-04])\n",
      "Epoch 1766, Loss 2.927646, Params tensor([  5.3673, -17.3027]), Grad tensor([-6.4075e-05,  3.6091e-04])\n",
      "Epoch 1767, Loss 2.927648, Params tensor([  5.3673, -17.3027]), Grad tensor([-7.0333e-05,  3.5781e-04])\n",
      "Epoch 1768, Loss 2.927648, Params tensor([  5.3673, -17.3027]), Grad tensor([-8.2195e-05,  3.5375e-04])\n",
      "Epoch 1769, Loss 2.927647, Params tensor([  5.3674, -17.3027]), Grad tensor([-5.7995e-05,  3.5611e-04])\n",
      "Epoch 1770, Loss 2.927646, Params tensor([  5.3674, -17.3027]), Grad tensor([-7.0035e-05,  3.5197e-04])\n",
      "Epoch 1771, Loss 2.927646, Params tensor([  5.3674, -17.3027]), Grad tensor([-7.4744e-05,  3.4922e-04])\n",
      "Epoch 1772, Loss 2.927645, Params tensor([  5.3674, -17.3027]), Grad tensor([-2.7955e-05,  3.5545e-04])\n",
      "Epoch 1773, Loss 2.927647, Params tensor([  5.3674, -17.3027]), Grad tensor([-8.7202e-05,  3.4299e-04])\n",
      "Epoch 1774, Loss 2.927646, Params tensor([  5.3674, -17.3028]), Grad tensor([-5.1320e-05,  3.4764e-04])\n",
      "Epoch 1775, Loss 2.927647, Params tensor([  5.3674, -17.3028]), Grad tensor([-6.7711e-05,  3.4320e-04])\n",
      "Epoch 1776, Loss 2.927646, Params tensor([  5.3674, -17.3028]), Grad tensor([-4.9293e-05,  3.4490e-04])\n",
      "Epoch 1777, Loss 2.927648, Params tensor([  5.3674, -17.3028]), Grad tensor([-6.9916e-05,  3.3957e-04])\n",
      "Epoch 1778, Loss 2.927646, Params tensor([  5.3674, -17.3028]), Grad tensor([-5.4419e-05,  3.4073e-04])\n",
      "Epoch 1779, Loss 2.927647, Params tensor([  5.3674, -17.3028]), Grad tensor([-7.1645e-05,  3.3602e-04])\n",
      "Epoch 1780, Loss 2.927646, Params tensor([  5.3674, -17.3028]), Grad tensor([-2.8193e-05,  3.4171e-04])\n",
      "Epoch 1781, Loss 2.927648, Params tensor([  5.3674, -17.3028]), Grad tensor([-6.8426e-05,  3.3319e-04])\n",
      "Epoch 1782, Loss 2.927648, Params tensor([  5.3674, -17.3028]), Grad tensor([-5.6267e-05,  3.3361e-04])\n",
      "Epoch 1783, Loss 2.927647, Params tensor([  5.3674, -17.3028]), Grad tensor([-4.1604e-05,  3.3426e-04])\n",
      "Epoch 1784, Loss 2.927646, Params tensor([  5.3674, -17.3029]), Grad tensor([-5.3704e-05,  3.3048e-04])\n",
      "Epoch 1785, Loss 2.927646, Params tensor([  5.3674, -17.3029]), Grad tensor([-7.3433e-05,  3.2544e-04])\n",
      "Epoch 1786, Loss 2.927647, Params tensor([  5.3674, -17.3029]), Grad tensor([-2.9027e-05,  3.3158e-04])\n",
      "Epoch 1787, Loss 2.927646, Params tensor([  5.3674, -17.3029]), Grad tensor([-7.7009e-05,  3.2175e-04])\n",
      "Epoch 1788, Loss 2.927645, Params tensor([  5.3674, -17.3029]), Grad tensor([-3.1710e-05,  3.2774e-04])\n",
      "Epoch 1789, Loss 2.927647, Params tensor([  5.3674, -17.3029]), Grad tensor([-7.2777e-05,  3.1897e-04])\n",
      "Epoch 1790, Loss 2.927645, Params tensor([  5.3674, -17.3029]), Grad tensor([-3.3259e-05,  3.2428e-04])\n",
      "Epoch 1791, Loss 2.927646, Params tensor([  5.3674, -17.3029]), Grad tensor([-7.7605e-05,  3.1495e-04])\n",
      "Epoch 1792, Loss 2.927647, Params tensor([  5.3674, -17.3029]), Grad tensor([-3.1292e-05,  3.2133e-04])\n",
      "Epoch 1793, Loss 2.927647, Params tensor([  5.3674, -17.3029]), Grad tensor([-8.0884e-05,  3.1087e-04])\n",
      "Epoch 1794, Loss 2.927647, Params tensor([  5.3674, -17.3029]), Grad tensor([-3.6597e-05,  3.1719e-04])\n",
      "Epoch 1795, Loss 2.927648, Params tensor([  5.3674, -17.3030]), Grad tensor([-7.9095e-05,  3.0807e-04])\n",
      "Epoch 1796, Loss 2.927645, Params tensor([  5.3674, -17.3030]), Grad tensor([-3.4094e-05,  3.1412e-04])\n",
      "Epoch 1797, Loss 2.927647, Params tensor([  5.3674, -17.3030]), Grad tensor([-7.9393e-05,  3.0422e-04])\n",
      "Epoch 1798, Loss 2.927646, Params tensor([  5.3674, -17.3030]), Grad tensor([-3.3557e-05,  3.1075e-04])\n",
      "Epoch 1799, Loss 2.927646, Params tensor([  5.3674, -17.3030]), Grad tensor([-8.1718e-05,  3.0074e-04])\n",
      "Epoch 1800, Loss 2.927645, Params tensor([  5.3674, -17.3030]), Grad tensor([-3.4928e-05,  3.0729e-04])\n",
      "Epoch 1801, Loss 2.927645, Params tensor([  5.3674, -17.3030]), Grad tensor([-8.5831e-05,  2.9653e-04])\n",
      "Epoch 1802, Loss 2.927646, Params tensor([  5.3674, -17.3030]), Grad tensor([-4.5717e-05,  3.0217e-04])\n",
      "Epoch 1803, Loss 2.927645, Params tensor([  5.3674, -17.3030]), Grad tensor([-6.0976e-05,  2.9770e-04])\n",
      "Epoch 1804, Loss 2.927646, Params tensor([  5.3674, -17.3030]), Grad tensor([-4.8161e-05,  2.9847e-04])\n",
      "Epoch 1805, Loss 2.927647, Params tensor([  5.3674, -17.3031]), Grad tensor([-5.5075e-05,  2.9549e-04])\n",
      "Epoch 1806, Loss 2.927644, Params tensor([  5.3674, -17.3031]), Grad tensor([-7.3075e-05,  2.9060e-04])\n",
      "Epoch 1807, Loss 2.927645, Params tensor([  5.3674, -17.3031]), Grad tensor([-2.7776e-05,  2.9677e-04])\n",
      "Epoch 1808, Loss 2.927644, Params tensor([  5.3674, -17.3031]), Grad tensor([-6.8069e-05,  2.8804e-04])\n",
      "Epoch 1809, Loss 2.927646, Params tensor([  5.3674, -17.3031]), Grad tensor([-5.7340e-05,  2.8822e-04])\n",
      "Epoch 1810, Loss 2.927646, Params tensor([  5.3674, -17.3031]), Grad tensor([-4.6134e-05,  2.8864e-04])\n",
      "Epoch 1811, Loss 2.927646, Params tensor([  5.3674, -17.3031]), Grad tensor([-5.8472e-05,  2.8449e-04])\n",
      "Epoch 1812, Loss 2.927647, Params tensor([  5.3674, -17.3031]), Grad tensor([-3.0041e-05,  2.8852e-04])\n",
      "Epoch 1813, Loss 2.927646, Params tensor([  5.3674, -17.3031]), Grad tensor([-7.1287e-05,  2.7943e-04])\n",
      "Epoch 1814, Loss 2.927645, Params tensor([  5.3674, -17.3031]), Grad tensor([-3.6776e-05,  2.8399e-04])\n",
      "Epoch 1815, Loss 2.927647, Params tensor([  5.3674, -17.3031]), Grad tensor([-5.8174e-05,  2.7916e-04])\n",
      "Epoch 1816, Loss 2.927647, Params tensor([  5.3674, -17.3031]), Grad tensor([-2.7120e-05,  2.8315e-04])\n",
      "Epoch 1817, Loss 2.927647, Params tensor([  5.3674, -17.3032]), Grad tensor([-5.5373e-05,  2.7719e-04])\n",
      "Epoch 1818, Loss 2.927645, Params tensor([  5.3674, -17.3032]), Grad tensor([-4.8816e-05,  2.7680e-04])\n",
      "Epoch 1819, Loss 2.927647, Params tensor([  5.3674, -17.3032]), Grad tensor([-3.8445e-05,  2.7728e-04])\n",
      "Epoch 1820, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-6.8784e-05,  2.7084e-04])\n",
      "Epoch 1821, Loss 2.927645, Params tensor([  5.3674, -17.3032]), Grad tensor([-2.6405e-05,  2.7642e-04])\n",
      "Epoch 1822, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-4.9293e-05,  2.7117e-04])\n",
      "Epoch 1823, Loss 2.927645, Params tensor([  5.3674, -17.3032]), Grad tensor([-4.4882e-05,  2.7066e-04])\n",
      "Epoch 1824, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-4.5300e-05,  2.6929e-04])\n",
      "Epoch 1825, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-3.5107e-05,  2.6989e-04])\n",
      "Epoch 1826, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-5.8532e-05,  2.6426e-04])\n",
      "Epoch 1827, Loss 2.927646, Params tensor([  5.3674, -17.3032]), Grad tensor([-2.8372e-05,  2.6843e-04])\n",
      "Epoch 1828, Loss 2.927647, Params tensor([  5.3674, -17.3032]), Grad tensor([-5.0306e-05,  2.6301e-04])\n",
      "Epoch 1829, Loss 2.927647, Params tensor([  5.3674, -17.3032]), Grad tensor([-4.4644e-05,  2.6295e-04])\n",
      "Epoch 1830, Loss 2.927647, Params tensor([  5.3674, -17.3033]), Grad tensor([-3.9756e-05,  2.6241e-04])\n",
      "Epoch 1831, Loss 2.927644, Params tensor([  5.3674, -17.3033]), Grad tensor([-3.8624e-05,  2.6119e-04])\n",
      "Epoch 1832, Loss 2.927647, Params tensor([  5.3675, -17.3033]), Grad tensor([-5.9485e-05,  2.5648e-04])\n",
      "Epoch 1833, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-2.5213e-05,  2.6098e-04])\n",
      "Epoch 1834, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-5.3465e-05,  2.5472e-04])\n",
      "Epoch 1835, Loss 2.927648, Params tensor([  5.3675, -17.3033]), Grad tensor([-4.6313e-05,  2.5457e-04])\n",
      "Epoch 1836, Loss 2.927644, Params tensor([  5.3675, -17.3033]), Grad tensor([-3.1531e-05,  2.5561e-04])\n",
      "Epoch 1837, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-6.0022e-05,  2.4936e-04])\n",
      "Epoch 1838, Loss 2.927644, Params tensor([  5.3675, -17.3033]), Grad tensor([-2.4557e-05,  2.5415e-04])\n",
      "Epoch 1839, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-5.1141e-05,  2.4825e-04])\n",
      "Epoch 1840, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-4.3392e-05,  2.4828e-04])\n",
      "Epoch 1841, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-3.6538e-05,  2.4810e-04])\n",
      "Epoch 1842, Loss 2.927646, Params tensor([  5.3675, -17.3033]), Grad tensor([-6.4313e-05,  2.4173e-04])\n",
      "Epoch 1843, Loss 2.927646, Params tensor([  5.3675, -17.3034]), Grad tensor([-2.9206e-05,  2.4670e-04])\n",
      "Epoch 1844, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-5.3167e-05,  2.4134e-04])\n",
      "Epoch 1845, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-5.0008e-05,  2.4045e-04])\n",
      "Epoch 1846, Loss 2.927646, Params tensor([  5.3675, -17.3034]), Grad tensor([-3.7193e-05,  2.4140e-04])\n",
      "Epoch 1847, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-7.0453e-05,  2.3413e-04])\n",
      "Epoch 1848, Loss 2.927646, Params tensor([  5.3675, -17.3034]), Grad tensor([-3.1292e-05,  2.3982e-04])\n",
      "Epoch 1849, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-5.9307e-05,  2.3350e-04])\n",
      "Epoch 1850, Loss 2.927646, Params tensor([  5.3675, -17.3034]), Grad tensor([-2.5988e-05,  2.3800e-04])\n",
      "Epoch 1851, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-4.4823e-05,  2.3317e-04])\n",
      "Epoch 1852, Loss 2.927646, Params tensor([  5.3675, -17.3034]), Grad tensor([-4.0114e-05,  2.3279e-04])\n",
      "Epoch 1853, Loss 2.927645, Params tensor([  5.3675, -17.3034]), Grad tensor([-2.8789e-05,  2.3338e-04])\n",
      "Epoch 1854, Loss 2.927647, Params tensor([  5.3675, -17.3034]), Grad tensor([-5.8353e-05,  2.2677e-04])\n",
      "Epoch 1855, Loss 2.927645, Params tensor([  5.3675, -17.3034]), Grad tensor([-2.2590e-05,  2.3177e-04])\n",
      "Epoch 1856, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([-7.2777e-05,  2.2158e-04])\n",
      "Epoch 1857, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([5.6624e-06, 2.3463e-04])\n",
      "Epoch 1858, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([-7.3552e-05,  2.1940e-04])\n",
      "Epoch 1859, Loss 2.927647, Params tensor([  5.3675, -17.3035]), Grad tensor([6.5565e-06, 2.3252e-04])\n",
      "Epoch 1860, Loss 2.927647, Params tensor([  5.3675, -17.3035]), Grad tensor([-7.2002e-05,  2.1735e-04])\n",
      "Epoch 1861, Loss 2.927647, Params tensor([  5.3675, -17.3035]), Grad tensor([7.4506e-06, 2.3022e-04])\n",
      "Epoch 1862, Loss 2.927648, Params tensor([  5.3675, -17.3035]), Grad tensor([-7.1645e-05,  2.1496e-04])\n",
      "Epoch 1863, Loss 2.927648, Params tensor([  5.3675, -17.3035]), Grad tensor([1.2338e-05, 2.2867e-04])\n",
      "Epoch 1864, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([-9.3997e-05,  2.0880e-04])\n",
      "Epoch 1865, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([8.1658e-06, 2.2587e-04])\n",
      "Epoch 1866, Loss 2.927647, Params tensor([  5.3675, -17.3035]), Grad tensor([-9.4354e-05,  2.0641e-04])\n",
      "Epoch 1867, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([1.6987e-05, 2.2471e-04])\n",
      "Epoch 1868, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([-9.2924e-05,  2.0412e-04])\n",
      "Epoch 1869, Loss 2.927646, Params tensor([  5.3675, -17.3035]), Grad tensor([2.0564e-05, 2.2301e-04])\n",
      "Epoch 1870, Loss 2.927645, Params tensor([  5.3675, -17.3035]), Grad tensor([-8.9109e-05,  2.0242e-04])\n",
      "Epoch 1871, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([1.9610e-05, 2.2042e-04])\n",
      "Epoch 1872, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-6.6042e-05,  2.0450e-04])\n",
      "Epoch 1873, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-1.8418e-05,  2.1195e-04])\n",
      "Epoch 1874, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-4.6968e-05,  2.0593e-04])\n",
      "Epoch 1875, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-2.0027e-05,  2.0969e-04])\n",
      "Epoch 1876, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-5.0962e-05,  2.0313e-04])\n",
      "Epoch 1877, Loss 2.927645, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.0398e-05,  2.0584e-04])\n",
      "Epoch 1878, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.2008e-05,  2.0444e-04])\n",
      "Epoch 1879, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.3915e-05,  2.0316e-04])\n",
      "Epoch 1880, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.7074e-05,  2.0182e-04])\n",
      "Epoch 1881, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-4.2021e-05,  1.9974e-04])\n",
      "Epoch 1882, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-1.7762e-05,  2.0301e-04])\n",
      "Epoch 1883, Loss 2.927647, Params tensor([  5.3675, -17.3036]), Grad tensor([-5.1618e-05,  1.9628e-04])\n",
      "Epoch 1884, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-2.6405e-05,  1.9974e-04])\n",
      "Epoch 1885, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-2.7895e-05,  1.9827e-04])\n",
      "Epoch 1886, Loss 2.927645, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.0935e-05,  1.9655e-04])\n",
      "Epoch 1887, Loss 2.927646, Params tensor([  5.3675, -17.3036]), Grad tensor([-2.7299e-05,  1.9628e-04])\n",
      "Epoch 1888, Loss 2.927645, Params tensor([  5.3675, -17.3036]), Grad tensor([-3.2544e-05,  1.9410e-04])\n",
      "Epoch 1889, Loss 2.927645, Params tensor([  5.3675, -17.3037]), Grad tensor([-3.5048e-05,  1.9276e-04])\n",
      "Epoch 1890, Loss 2.927645, Params tensor([  5.3675, -17.3037]), Grad tensor([-4.4942e-05,  1.9011e-04])\n",
      "Epoch 1891, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-1.0729e-05,  1.9532e-04])\n",
      "Epoch 1892, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-4.7386e-05,  1.8787e-04])\n",
      "Epoch 1893, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-2.1577e-05,  1.9142e-04])\n",
      "Epoch 1894, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-5.3704e-05,  1.8474e-04])\n",
      "Epoch 1895, Loss 2.927645, Params tensor([  5.3675, -17.3037]), Grad tensor([-2.8014e-05,  1.8814e-04])\n",
      "Epoch 1896, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-3.3379e-05,  1.8632e-04])\n",
      "Epoch 1897, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-3.8862e-05,  1.8439e-04])\n",
      "Epoch 1898, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-3.9399e-05,  1.8334e-04])\n",
      "Epoch 1899, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-4.0650e-05,  1.8188e-04])\n",
      "Epoch 1900, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-2.0564e-05,  1.8454e-04])\n",
      "Epoch 1901, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-4.6790e-05,  1.7893e-04])\n",
      "Epoch 1902, Loss 2.927646, Params tensor([  5.3675, -17.3037]), Grad tensor([-2.0683e-05,  1.8266e-04])\n",
      "Epoch 1903, Loss 2.927648, Params tensor([  5.3675, -17.3037]), Grad tensor([-5.7638e-05,  1.7521e-04])\n",
      "Epoch 1904, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-2.1458e-06,  1.8376e-04])\n",
      "Epoch 1905, Loss 2.927647, Params tensor([  5.3675, -17.3037]), Grad tensor([-6.1750e-05,  1.7226e-04])\n",
      "Epoch 1906, Loss 2.927647, Params tensor([  5.3675, -17.3038]), Grad tensor([-6.1989e-06,  1.8078e-04])\n",
      "Epoch 1907, Loss 2.927647, Params tensor([  5.3675, -17.3038]), Grad tensor([-6.5625e-05,  1.6940e-04])\n",
      "Epoch 1908, Loss 2.927645, Params tensor([  5.3675, -17.3038]), Grad tensor([-1.0431e-05,  1.7801e-04])\n",
      "Epoch 1909, Loss 2.927647, Params tensor([  5.3675, -17.3038]), Grad tensor([-4.5419e-05,  1.7104e-04])\n",
      "Epoch 1910, Loss 2.927646, Params tensor([  5.3675, -17.3038]), Grad tensor([-2.0802e-05,  1.7437e-04])\n",
      "Epoch 1911, Loss 2.927646, Params tensor([  5.3675, -17.3038]), Grad tensor([-4.9293e-05,  1.6841e-04])\n",
      "Epoch 1912, Loss 2.927646, Params tensor([  5.3675, -17.3038]), Grad tensor([-3.0577e-05,  1.7062e-04])\n",
      "Epoch 1913, Loss 2.927645, Params tensor([  5.3675, -17.3038]), Grad tensor([-3.2902e-05,  1.6925e-04])\n",
      "Epoch 1914, Loss 2.927646, Params tensor([  5.3675, -17.3038]), Grad tensor([-3.0696e-05,  1.6856e-04])\n",
      "Epoch 1915, Loss 2.927645, Params tensor([  5.3675, -17.3038]), Grad tensor([-3.7670e-05,  1.6645e-04])\n",
      "Epoch 1916, Loss 2.927647, Params tensor([  5.3675, -17.3038]), Grad tensor([-4.2617e-05,  1.6463e-04])\n",
      "Epoch 1917, Loss 2.927646, Params tensor([  5.3675, -17.3038]), Grad tensor([-1.7285e-05,  1.6817e-04])\n",
      "Epoch 1918, Loss 2.927645, Params tensor([  5.3675, -17.3038]), Grad tensor([-4.7386e-05,  1.6180e-04])\n",
      "Epoch 1919, Loss 2.927647, Params tensor([  5.3675, -17.3038]), Grad tensor([-2.3544e-05,  1.6502e-04])\n",
      "Epoch 1920, Loss 2.927647, Params tensor([  5.3676, -17.3038]), Grad tensor([-5.5969e-05,  1.5828e-04])\n",
      "Epoch 1921, Loss 2.927646, Params tensor([  5.3676, -17.3038]), Grad tensor([2.0504e-05, 1.7098e-04])\n",
      "Epoch 1922, Loss 2.927645, Params tensor([  5.3676, -17.3038]), Grad tensor([-6.8367e-05,  1.5458e-04])\n",
      "Epoch 1923, Loss 2.927647, Params tensor([  5.3676, -17.3038]), Grad tensor([-5.9605e-08,  1.6594e-04])\n",
      "Epoch 1924, Loss 2.927646, Params tensor([  5.3676, -17.3038]), Grad tensor([-5.8830e-05,  1.5444e-04])\n",
      "Epoch 1925, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([1.4842e-05, 1.6671e-04])\n",
      "Epoch 1926, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-7.3552e-05,  1.5014e-04])\n",
      "Epoch 1927, Loss 2.927644, Params tensor([  5.3676, -17.3039]), Grad tensor([3.1292e-05, 1.6776e-04])\n",
      "Epoch 1928, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-8.1718e-05,  1.4684e-04])\n",
      "Epoch 1929, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([2.3067e-05, 1.6448e-04])\n",
      "Epoch 1930, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-6.5029e-05,  1.4809e-04])\n",
      "Epoch 1931, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([7.5698e-06, 1.6028e-04])\n",
      "Epoch 1932, Loss 2.927647, Params tensor([  5.3676, -17.3039]), Grad tensor([-5.2154e-05,  1.4877e-04])\n",
      "Epoch 1933, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-1.1563e-05,  1.5509e-04])\n",
      "Epoch 1934, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-2.1875e-05,  1.5274e-04])\n",
      "Epoch 1935, Loss 2.927647, Params tensor([  5.3676, -17.3039]), Grad tensor([-3.2246e-05,  1.5050e-04])\n",
      "Epoch 1936, Loss 2.927645, Params tensor([  5.3676, -17.3039]), Grad tensor([-1.8597e-05,  1.5223e-04])\n",
      "Epoch 1937, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-2.9147e-05,  1.4964e-04])\n",
      "Epoch 1938, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-1.3113e-05,  1.5157e-04])\n",
      "Epoch 1939, Loss 2.927645, Params tensor([  5.3676, -17.3039]), Grad tensor([-2.3723e-05,  1.4904e-04])\n",
      "Epoch 1940, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-3.9220e-05,  1.4582e-04])\n",
      "Epoch 1941, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-2.2292e-05,  1.4818e-04])\n",
      "Epoch 1942, Loss 2.927647, Params tensor([  5.3676, -17.3039]), Grad tensor([-3.1650e-05,  1.4588e-04])\n",
      "Epoch 1943, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-1.4186e-05,  1.4827e-04])\n",
      "Epoch 1944, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-3.1292e-05,  1.4469e-04])\n",
      "Epoch 1945, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-9.8944e-06,  1.4773e-04])\n",
      "Epoch 1946, Loss 2.927646, Params tensor([  5.3676, -17.3039]), Grad tensor([-2.2650e-05,  1.4478e-04])\n",
      "Epoch 1947, Loss 2.927645, Params tensor([  5.3676, -17.3039]), Grad tensor([-3.7074e-05,  1.4153e-04])\n",
      "Epoch 1948, Loss 2.927647, Params tensor([  5.3676, -17.3039]), Grad tensor([-1.7524e-05,  1.4430e-04])\n",
      "Epoch 1949, Loss 2.927647, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.9325e-05,  1.4162e-04])\n",
      "Epoch 1950, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.4544e-05,  1.4353e-04])\n",
      "Epoch 1951, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.4021e-05,  1.4129e-04])\n",
      "Epoch 1952, Loss 2.927645, Params tensor([  5.3676, -17.3040]), Grad tensor([-9.4175e-06,  1.4299e-04])\n",
      "Epoch 1953, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.0266e-05,  1.4040e-04])\n",
      "Epoch 1954, Loss 2.927647, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.4809e-05,  1.3742e-04])\n",
      "Epoch 1955, Loss 2.927645, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.3351e-05,  1.4025e-04])\n",
      "Epoch 1956, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.3782e-05,  1.3760e-04])\n",
      "Epoch 1957, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.2544e-05,  1.3545e-04])\n",
      "Epoch 1958, Loss 2.927644, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.6153e-05,  1.3766e-04])\n",
      "Epoch 1959, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.3796e-05,  1.3399e-04])\n",
      "Epoch 1960, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.2040e-05,  1.3715e-04])\n",
      "Epoch 1961, Loss 2.927648, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.0146e-05,  1.3492e-04])\n",
      "Epoch 1962, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.5524e-05,  1.3173e-04])\n",
      "Epoch 1963, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.4365e-05,  1.3483e-04])\n",
      "Epoch 1964, Loss 2.927647, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.3617e-05,  1.3074e-04])\n",
      "Epoch 1965, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.6570e-05,  1.3304e-04])\n",
      "Epoch 1966, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.7716e-05,  1.3039e-04])\n",
      "Epoch 1967, Loss 2.927644, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.0490e-05,  1.3289e-04])\n",
      "Epoch 1968, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.9491e-05,  1.3059e-04])\n",
      "Epoch 1969, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.3021e-05,  1.2764e-04])\n",
      "Epoch 1970, Loss 2.927645, Params tensor([  5.3676, -17.3040]), Grad tensor([-1.8179e-05,  1.2931e-04])\n",
      "Epoch 1971, Loss 2.927645, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.0637e-05,  1.2645e-04])\n",
      "Epoch 1972, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-9.5963e-06,  1.2961e-04])\n",
      "Epoch 1973, Loss 2.927645, Params tensor([  5.3676, -17.3040]), Grad tensor([-2.2233e-05,  1.2678e-04])\n",
      "Epoch 1974, Loss 2.927646, Params tensor([  5.3676, -17.3040]), Grad tensor([-3.6299e-05,  1.2374e-04])\n",
      "Epoch 1975, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([-1.7822e-05,  1.2630e-04])\n",
      "Epoch 1976, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-3.1352e-05,  1.2326e-04])\n",
      "Epoch 1977, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-1.2517e-05,  1.2580e-04])\n",
      "Epoch 1978, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-2.1935e-05,  1.2353e-04])\n",
      "Epoch 1979, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-4.1723e-05,  1.1948e-04])\n",
      "Epoch 1980, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([7.8678e-06, 1.2740e-04])\n",
      "Epoch 1981, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-3.1590e-05,  1.1984e-04])\n",
      "Epoch 1982, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([-1.4186e-05,  1.2237e-04])\n",
      "Epoch 1983, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-2.3723e-05,  1.2007e-04])\n",
      "Epoch 1984, Loss 2.927648, Params tensor([  5.3676, -17.3041]), Grad tensor([-4.1902e-05,  1.1602e-04])\n",
      "Epoch 1985, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([1.1265e-05, 1.2451e-04])\n",
      "Epoch 1986, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-6.5863e-05,  1.1039e-04])\n",
      "Epoch 1987, Loss 2.927644, Params tensor([  5.3676, -17.3041]), Grad tensor([1.1146e-05, 1.2332e-04])\n",
      "Epoch 1988, Loss 2.927644, Params tensor([  5.3676, -17.3041]), Grad tensor([-5.1975e-05,  1.1134e-04])\n",
      "Epoch 1989, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([-8.1062e-06,  1.1867e-04])\n",
      "Epoch 1990, Loss 2.927644, Params tensor([  5.3676, -17.3041]), Grad tensor([-2.4080e-05,  1.1492e-04])\n",
      "Epoch 1991, Loss 2.927645, Params tensor([  5.3676, -17.3041]), Grad tensor([-3.0994e-06,  1.1805e-04])\n",
      "Epoch 1992, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-4.1723e-05,  1.1045e-04])\n",
      "Epoch 1993, Loss 2.927645, Params tensor([  5.3676, -17.3041]), Grad tensor([-1.7881e-06,  1.1691e-04])\n",
      "Epoch 1994, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-4.1366e-05,  1.0923e-04])\n",
      "Epoch 1995, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([1.0312e-05, 1.1772e-04])\n",
      "Epoch 1996, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-5.8055e-05,  1.0505e-04])\n",
      "Epoch 1997, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([6.4969e-06, 1.1560e-04])\n",
      "Epoch 1998, Loss 2.927647, Params tensor([  5.3676, -17.3041]), Grad tensor([-3.3140e-05,  1.0797e-04])\n",
      "Epoch 1999, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-1.4126e-05,  1.1072e-04])\n",
      "Epoch 2000, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-2.4796e-05,  1.0812e-04])\n",
      "Epoch 2001, Loss 2.927646, Params tensor([  5.3676, -17.3041]), Grad tensor([-3.6359e-06,  1.1128e-04])\n",
      "Epoch 2002, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-4.2915e-05,  1.0377e-04])\n",
      "Epoch 2003, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-6.1393e-06,  1.0943e-04])\n",
      "Epoch 2004, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-4.5896e-05,  1.0183e-04])\n",
      "Epoch 2005, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([0.0000, 0.0001])\n",
      "Epoch 2006, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.9399e-05,  1.0183e-04])\n",
      "Epoch 2007, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.0862e-05,  1.0422e-04])\n",
      "Epoch 2008, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.5048e-05,  1.0127e-04])\n",
      "Epoch 2009, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-1.7762e-05,  1.0365e-04])\n",
      "Epoch 2010, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.7955e-05,  1.0106e-04])\n",
      "Epoch 2011, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([-1.0312e-05,  1.0353e-04])\n",
      "Epoch 2012, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.2590e-05,  1.0058e-04])\n",
      "Epoch 2013, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.7611e-05,  9.7394e-05])\n",
      "Epoch 2014, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.0742e-05,  9.9778e-05])\n",
      "Epoch 2015, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.9564e-05,  9.7543e-05])\n",
      "Epoch 2016, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([-1.2815e-05,  9.9301e-05])\n",
      "Epoch 2017, Loss 2.927644, Params tensor([  5.3676, -17.3042]), Grad tensor([-2.0146e-05,  9.7483e-05])\n",
      "Epoch 2018, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.5882e-05,  9.4086e-05])\n",
      "Epoch 2019, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([1.3113e-06, 1.0028e-04])\n",
      "Epoch 2020, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.8207e-05,  9.2834e-05])\n",
      "Epoch 2021, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([3.3379e-06, 9.9778e-05])\n",
      "Epoch 2022, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.5882e-05,  9.2238e-05])\n",
      "Epoch 2023, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([1.1921e-07, 9.8139e-05])\n",
      "Epoch 2024, Loss 2.927645, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.9399e-05,  9.0510e-05])\n",
      "Epoch 2025, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-1.0312e-05,  9.5308e-05])\n",
      "Epoch 2026, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([2.5034e-06, 9.7156e-05])\n",
      "Epoch 2027, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-3.7253e-05,  8.9645e-05])\n",
      "Epoch 2028, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-1.4901e-06,  9.5487e-05])\n",
      "Epoch 2029, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([-4.0531e-05,  8.7917e-05])\n",
      "Epoch 2030, Loss 2.927646, Params tensor([  5.3676, -17.3042]), Grad tensor([1.8299e-05, 9.8139e-05])\n",
      "Epoch 2031, Loss 2.927647, Params tensor([  5.3676, -17.3042]), Grad tensor([-4.1127e-05,  8.6844e-05])\n",
      "Epoch 2032, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([1.9968e-05, 9.7632e-05])\n",
      "Epoch 2033, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([-5.1796e-05,  8.4221e-05])\n",
      "Epoch 2034, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([1.7524e-05, 9.5874e-05])\n",
      "Epoch 2035, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-4.9233e-05,  8.3536e-05])\n",
      "Epoch 2036, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([1.2934e-05, 9.4146e-05])\n",
      "Epoch 2037, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.5346e-05,  8.5264e-05])\n",
      "Epoch 2038, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([1.4901e-06, 9.1583e-05])\n",
      "Epoch 2039, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.8954e-05,  8.7708e-05])\n",
      "Epoch 2040, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-9.9540e-06,  8.8930e-05])\n",
      "Epoch 2041, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.4571e-06,  8.9765e-05])\n",
      "Epoch 2042, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.2948e-05,  8.5920e-05])\n",
      "Epoch 2043, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.3828e-05,  8.7112e-05])\n",
      "Epoch 2044, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-6.2585e-06,  8.8036e-05])\n",
      "Epoch 2045, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.6107e-05,  8.4162e-05])\n",
      "Epoch 2046, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([5.2452e-06, 8.9645e-05])\n",
      "Epoch 2047, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.4365e-05,  8.5771e-05])\n",
      "Epoch 2048, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.7551e-06,  8.7321e-05])\n",
      "Epoch 2049, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.3544e-05,  8.3625e-05])\n",
      "Epoch 2050, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.5140e-05,  8.4877e-05])\n",
      "Epoch 2051, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-4.4107e-06,  8.6367e-05])\n",
      "Epoch 2052, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.4259e-05,  8.2612e-05])\n",
      "Epoch 2053, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([1.1981e-05, 8.8394e-05])\n",
      "Epoch 2054, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.6359e-05,  7.9632e-05])\n",
      "Epoch 2055, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([1.0729e-06, 8.5890e-05])\n",
      "Epoch 2056, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.8299e-05,  8.2016e-05])\n",
      "Epoch 2057, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.2100e-05,  8.2850e-05])\n",
      "Epoch 2058, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-4.4703e-06,  8.4132e-05])\n",
      "Epoch 2059, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.4557e-05,  8.0317e-05])\n",
      "Epoch 2060, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([1.3173e-05, 8.6457e-05])\n",
      "Epoch 2061, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.6299e-05,  7.7337e-05])\n",
      "Epoch 2062, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([8.9407e-07, 8.3447e-05])\n",
      "Epoch 2063, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.9252e-05,  7.9662e-05])\n",
      "Epoch 2064, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([-5.2452e-06,  8.1450e-05])\n",
      "Epoch 2065, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.5094e-05,  7.7695e-05])\n",
      "Epoch 2066, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([1.1981e-05, 8.3953e-05])\n",
      "Epoch 2067, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.3557e-05,  7.5489e-05])\n",
      "Epoch 2068, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.6093e-06,  8.1003e-05])\n",
      "Epoch 2069, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.0862e-05,  7.7218e-05])\n",
      "Epoch 2070, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.4126e-05,  7.7993e-05])\n",
      "Epoch 2071, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-3.0994e-06,  7.9632e-05])\n",
      "Epoch 2072, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.2948e-05,  7.5698e-05])\n",
      "Epoch 2073, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.3351e-05,  7.6950e-05])\n",
      "Epoch 2074, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-4.4107e-06,  7.8142e-05])\n",
      "Epoch 2075, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.3842e-05,  7.4387e-05])\n",
      "Epoch 2076, Loss 2.927644, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.1636e-05,  7.4565e-05])\n",
      "Epoch 2077, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.4722e-05,  7.5579e-05])\n",
      "Epoch 2078, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.7418e-06,  7.7546e-05])\n",
      "Epoch 2079, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-2.2352e-05,  7.3671e-05])\n",
      "Epoch 2080, Loss 2.927647, Params tensor([  5.3676, -17.3043]), Grad tensor([-1.1444e-05,  7.5251e-05])\n",
      "Epoch 2081, Loss 2.927646, Params tensor([  5.3676, -17.3043]), Grad tensor([-8.2850e-06,  7.5459e-05])\n",
      "Epoch 2082, Loss 2.927645, Params tensor([  5.3676, -17.3043]), Grad tensor([1.0133e-06, 7.6711e-05])\n",
      "Epoch 2083, Loss 2.927645, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.8477e-05,  7.2777e-05])\n",
      "Epoch 2084, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-8.1658e-06,  7.4387e-05])\n",
      "Epoch 2085, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([3.0994e-06, 7.6026e-05])\n",
      "Epoch 2086, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.6391e-05,  7.2092e-05])\n",
      "Epoch 2087, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.6332e-05,  7.1913e-05])\n",
      "Epoch 2088, Loss 2.927645, Params tensor([  5.3676, -17.3044]), Grad tensor([-5.7220e-06,  7.3463e-05])\n",
      "Epoch 2089, Loss 2.927645, Params tensor([  5.3676, -17.3044]), Grad tensor([-2.5392e-05,  6.9648e-05])\n",
      "Epoch 2090, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([1.0610e-05, 7.5668e-05])\n",
      "Epoch 2091, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-4.0054e-05,  6.6370e-05])\n",
      "Epoch 2092, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([2.8431e-05, 7.8171e-05])\n",
      "Epoch 2093, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([-4.8459e-05,  6.4164e-05])\n",
      "Epoch 2094, Loss 2.927645, Params tensor([  5.3676, -17.3044]), Grad tensor([1.9252e-05, 7.5966e-05])\n",
      "Epoch 2095, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-3.0935e-05,  6.6727e-05])\n",
      "Epoch 2096, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([5.1856e-06, 7.2956e-05])\n",
      "Epoch 2097, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.3828e-05,  6.9171e-05])\n",
      "Epoch 2098, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-9.4175e-06,  6.9737e-05])\n",
      "Epoch 2099, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.6689e-06,  7.0632e-05])\n",
      "Epoch 2100, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-2.0802e-05,  6.6757e-05])\n",
      "Epoch 2101, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.0014e-05,  6.8307e-05])\n",
      "Epoch 2102, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([-2.9802e-06,  6.9201e-05])\n",
      "Epoch 2103, Loss 2.927647, Params tensor([  5.3676, -17.3044]), Grad tensor([-2.2888e-05,  6.5386e-05])\n",
      "Epoch 2104, Loss 2.927645, Params tensor([  5.3676, -17.3044]), Grad tensor([-1.2338e-05,  6.6936e-05])\n",
      "Epoch 2105, Loss 2.927646, Params tensor([  5.3676, -17.3044]), Grad tensor([-3.8743e-06,  6.8128e-05])\n",
      "Epoch 2106, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.3544e-05,  6.4462e-05])\n",
      "Epoch 2107, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.9908e-05,  6.4880e-05])\n",
      "Epoch 2108, Loss 2.927648, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.0371e-05,  6.6161e-05])\n",
      "Epoch 2109, Loss 2.927648, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.5497e-06,  6.7472e-05])\n",
      "Epoch 2110, Loss 2.927648, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.0921e-05,  6.3717e-05])\n",
      "Epoch 2111, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.2279e-05,  6.4850e-05])\n",
      "Epoch 2112, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-4.9472e-06,  6.5714e-05])\n",
      "Epoch 2113, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.4378e-05,  6.1840e-05])\n",
      "Epoch 2114, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([1.4961e-05, 6.8516e-05])\n",
      "Epoch 2115, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-3.3796e-05,  5.9575e-05])\n",
      "Epoch 2116, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-3.0398e-06,  6.4850e-05])\n",
      "Epoch 2117, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.3127e-05,  6.1005e-05])\n",
      "Epoch 2118, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.3769e-05,  6.2197e-05])\n",
      "Epoch 2119, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.6822e-06,  6.3747e-05])\n",
      "Epoch 2120, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.2948e-05,  5.9903e-05])\n",
      "Epoch 2121, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.3828e-05,  6.1154e-05])\n",
      "Epoch 2122, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-7.7486e-06,  6.1989e-05])\n",
      "Epoch 2123, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-2.6524e-05,  5.8264e-05])\n",
      "Epoch 2124, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([1.9014e-05, 6.5684e-05])\n",
      "Epoch 2125, Loss 2.927645, Params tensor([  5.3677, -17.3044]), Grad tensor([-3.1710e-05,  5.6386e-05])\n",
      "Epoch 2126, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([3.5167e-06, 6.2406e-05])\n",
      "Epoch 2127, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.6093e-05,  5.8591e-05])\n",
      "Epoch 2128, Loss 2.927645, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.2338e-05,  5.8770e-05])\n",
      "Epoch 2129, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.1921e-07,  6.0618e-05])\n",
      "Epoch 2130, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.9789e-05,  5.6863e-05])\n",
      "Epoch 2131, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.2994e-05,  5.7638e-05])\n",
      "Epoch 2132, Loss 2.927646, Params tensor([  5.3677, -17.3044]), Grad tensor([1.3709e-06, 5.9962e-05])\n",
      "Epoch 2133, Loss 2.927647, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.8418e-05,  5.6118e-05])\n",
      "Epoch 2134, Loss 2.927645, Params tensor([  5.3677, -17.3044]), Grad tensor([-1.5438e-05,  5.6297e-05])\n",
      "Epoch 2135, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3113e-06,  5.8562e-05])\n",
      "Epoch 2136, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.1040e-05,  5.4836e-05])\n",
      "Epoch 2137, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.9372e-05,  5.4777e-05])\n",
      "Epoch 2138, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3530e-05,  5.5224e-05])\n",
      "Epoch 2139, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-3.8743e-06,  5.6714e-05])\n",
      "Epoch 2140, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.3723e-05,  5.2899e-05])\n",
      "Epoch 2141, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.5616e-05,  5.3912e-05])\n",
      "Epoch 2142, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.0133e-06,  5.6237e-05])\n",
      "Epoch 2143, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.0683e-05,  5.2363e-05])\n",
      "Epoch 2144, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3888e-05,  5.3257e-05])\n",
      "Epoch 2145, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3709e-05,  5.3078e-05])\n",
      "Epoch 2146, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.5630e-06,  5.4687e-05])\n",
      "Epoch 2147, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.1875e-05,  5.0783e-05])\n",
      "Epoch 2148, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.1384e-05,  5.2392e-05])\n",
      "Epoch 2149, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.6689e-06,  5.3883e-05])\n",
      "Epoch 2150, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.0802e-05,  5.0098e-05])\n",
      "Epoch 2151, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.6570e-05,  5.0664e-05])\n",
      "Epoch 2152, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-8.6427e-06,  5.1558e-05])\n",
      "Epoch 2153, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([1.7285e-06, 5.3048e-05])\n",
      "Epoch 2154, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.7405e-05,  4.9233e-05])\n",
      "Epoch 2155, Loss 2.927648, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3769e-05,  4.9770e-05])\n",
      "Epoch 2156, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-6.1393e-06,  5.0783e-05])\n",
      "Epoch 2157, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.5809e-05,  4.7088e-05])\n",
      "Epoch 2158, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([1.1265e-05, 5.3197e-05])\n",
      "Epoch 2159, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-3.7313e-05,  4.4405e-05])\n",
      "Epoch 2160, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([6.1989e-06, 5.1618e-05])\n",
      "Epoch 2161, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3471e-05,  4.7833e-05])\n",
      "Epoch 2162, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.0788e-05,  4.8131e-05])\n",
      "Epoch 2163, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([1.6689e-06, 4.9919e-05])\n",
      "Epoch 2164, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.8060e-05,  4.6194e-05])\n",
      "Epoch 2165, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.8120e-05,  4.5687e-05])\n",
      "Epoch 2166, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-6.9737e-06,  4.7237e-05])\n",
      "Epoch 2167, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.7061e-05,  4.3422e-05])\n",
      "Epoch 2168, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([7.6294e-06, 4.9263e-05])\n",
      "Epoch 2169, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.2159e-05,  4.5478e-05])\n",
      "Epoch 2170, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([8.3447e-07, 4.7326e-05])\n",
      "Epoch 2171, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.8537e-05,  4.3541e-05])\n",
      "Epoch 2172, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.1861e-05,  4.4614e-05])\n",
      "Epoch 2173, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.7418e-06,  4.5806e-05])\n",
      "Epoch 2174, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.2054e-05,  4.1962e-05])\n",
      "Epoch 2175, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.8120e-05,  4.2498e-05])\n",
      "Epoch 2176, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-9.5963e-06,  4.3750e-05])\n",
      "Epoch 2177, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.9802e-07,  4.4763e-05])\n",
      "Epoch 2178, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.9252e-05,  4.1187e-05])\n",
      "Epoch 2179, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-8.8215e-06,  4.2617e-05])\n",
      "Epoch 2180, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.6226e-06,  4.3511e-05])\n",
      "Epoch 2181, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-2.2292e-05,  3.9756e-05])\n",
      "Epoch 2182, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.0848e-05,  4.1306e-05])\n",
      "Epoch 2183, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([3.5763e-07, 4.2528e-05])\n",
      "Epoch 2184, Loss 2.927645, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.8895e-05,  3.8743e-05])\n",
      "Epoch 2185, Loss 2.927647, Params tensor([  5.3677, -17.3045]), Grad tensor([-1.3173e-05,  3.9577e-05])\n",
      "Epoch 2186, Loss 2.927646, Params tensor([  5.3677, -17.3045]), Grad tensor([-4.6492e-06,  4.0770e-05])\n",
      "Epoch 2187, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-2.5630e-05,  3.6985e-05])\n",
      "Epoch 2188, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([1.5080e-05, 4.3720e-05])\n",
      "Epoch 2189, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-3.6061e-05,  3.4511e-05])\n",
      "Epoch 2190, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9670e-06, 4.0799e-05])\n",
      "Epoch 2191, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-1.8120e-05,  3.6895e-05])\n",
      "Epoch 2192, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([-1.0073e-05,  3.7819e-05])\n",
      "Epoch 2193, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-9.5367e-07,  3.9071e-05])\n",
      "Epoch 2194, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-2.1160e-05,  3.5256e-05])\n",
      "Epoch 2195, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-1.5080e-05,  3.6031e-05])\n",
      "Epoch 2196, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.3777e-06,  3.7313e-05])\n",
      "Epoch 2197, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-2.6345e-05,  3.3528e-05])\n",
      "Epoch 2198, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([1.0848e-05, 3.9637e-05])\n",
      "Epoch 2199, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.1246e-05,  3.0190e-05])\n",
      "Epoch 2200, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([4.9114e-05, 4.5896e-05])\n",
      "Epoch 2201, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.0797e-05,  2.6464e-05])\n",
      "Epoch 2202, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([5.3883e-05, 4.6611e-05])\n",
      "Epoch 2203, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.8637e-05,  2.8253e-05])\n",
      "Epoch 2204, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([3.4392e-05, 4.2886e-05])\n",
      "Epoch 2205, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-3.7491e-05,  2.9862e-05])\n",
      "Epoch 2206, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([1.4603e-05, 3.8981e-05])\n",
      "Epoch 2207, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-3.0100e-05,  3.0786e-05])\n",
      "Epoch 2208, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([1.7762e-05, 3.9160e-05])\n",
      "Epoch 2209, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-2.4617e-05,  3.1501e-05])\n",
      "Epoch 2210, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([3.1471e-05, 4.1425e-05])\n",
      "Epoch 2211, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.4882e-05,  2.7597e-05])\n",
      "Epoch 2212, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.0829e-05, 4.2617e-05])\n",
      "Epoch 2213, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.4015e-05,  2.3782e-05])\n",
      "Epoch 2214, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.5240e-05, 4.3094e-05])\n",
      "Epoch 2215, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.1810e-05,  2.3842e-05])\n",
      "Epoch 2216, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([5.6446e-05, 4.4733e-05])\n",
      "Epoch 2217, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.0526e-05,  2.0206e-05])\n",
      "Epoch 2218, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([6.6221e-05, 4.5925e-05])\n",
      "Epoch 2219, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.7055e-05,  2.2352e-05])\n",
      "Epoch 2220, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.6134e-05, 4.2081e-05])\n",
      "Epoch 2221, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-5.7459e-05,  2.3603e-05])\n",
      "Epoch 2222, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([5.6803e-05, 4.3631e-05])\n",
      "Epoch 2223, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-7.7903e-05,  1.9670e-05])\n",
      "Epoch 2224, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([5.6684e-05, 4.3452e-05])\n",
      "Epoch 2225, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-7.2360e-05,  2.0295e-05])\n",
      "Epoch 2226, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([6.7890e-05, 4.5031e-05])\n",
      "Epoch 2227, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.1691e-05,  2.1845e-05])\n",
      "Epoch 2228, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.7565e-05, 4.1157e-05])\n",
      "Epoch 2229, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-5.2929e-05,  2.3067e-05])\n",
      "Epoch 2230, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([2.8074e-05, 3.7402e-05])\n",
      "Epoch 2231, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.2081e-05,  2.4676e-05])\n",
      "Epoch 2232, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([3.5405e-05, 3.8326e-05])\n",
      "Epoch 2233, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.2260e-05,  2.4498e-05])\n",
      "Epoch 2234, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.6134e-05, 4.0054e-05])\n",
      "Epoch 2235, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.1572e-05,  2.0653e-05])\n",
      "Epoch 2236, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([5.1916e-05, 4.0501e-05])\n",
      "Epoch 2237, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-5.1439e-05,  2.2203e-05])\n",
      "Epoch 2238, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([3.2306e-05, 3.6746e-05])\n",
      "Epoch 2239, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.3988e-05,  2.3097e-05])\n",
      "Epoch 2240, Loss 2.927645, Params tensor([  5.3677, -17.3046]), Grad tensor([4.0948e-05, 3.7998e-05])\n",
      "Epoch 2241, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.3121e-05,  1.9282e-05])\n",
      "Epoch 2242, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([5.1677e-05, 3.9488e-05])\n",
      "Epoch 2243, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-5.2512e-05,  2.1011e-05])\n",
      "Epoch 2244, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([3.1888e-05, 3.5793e-05])\n",
      "Epoch 2245, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.6968e-05,  2.1547e-05])\n",
      "Epoch 2246, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([3.7909e-05, 3.6627e-05])\n",
      "Epoch 2247, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-3.8564e-05,  2.2680e-05])\n",
      "Epoch 2248, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([1.7703e-05, 3.2753e-05])\n",
      "Epoch 2249, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-2.7239e-05,  2.4289e-05])\n",
      "Epoch 2250, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([2.4438e-05, 3.3677e-05])\n",
      "Epoch 2251, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.7088e-05,  2.0474e-05])\n",
      "Epoch 2252, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([3.2842e-05, 3.4928e-05])\n",
      "Epoch 2253, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.0829e-05,  2.1368e-05])\n",
      "Epoch 2254, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([4.4286e-05, 3.6418e-05])\n",
      "Epoch 2255, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-6.0618e-05,  1.7524e-05])\n",
      "Epoch 2256, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([5.1022e-05, 3.6836e-05])\n",
      "Epoch 2257, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-5.3823e-05,  1.8358e-05])\n",
      "Epoch 2258, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([3.1292e-05, 3.3230e-05])\n",
      "Epoch 2259, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-4.5657e-05,  1.9550e-05])\n",
      "Epoch 2260, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([3.9220e-05, 3.4451e-05])\n",
      "Epoch 2261, Loss 2.927646, Params tensor([  5.3677, -17.3046]), Grad tensor([-3.4511e-05,  2.1100e-05])\n",
      "Epoch 2262, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2263, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2264, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2265, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2266, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2267, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2268, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2269, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2270, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2271, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2272, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2273, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2274, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2275, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2276, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2277, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2278, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2279, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2280, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2281, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2282, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2283, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2284, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2285, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2286, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2287, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2288, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2289, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2290, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2291, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2292, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2293, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2294, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2295, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2296, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2297, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2298, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2299, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2300, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2301, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2302, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2303, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2304, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2305, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2306, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2307, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2308, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2309, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2310, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2311, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2312, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2313, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2314, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2315, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2316, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2317, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2318, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2319, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2320, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2321, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2322, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2323, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2324, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2325, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2326, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2327, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2328, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2329, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2330, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2331, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2332, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2333, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2334, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2335, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2336, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2337, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2338, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2339, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2340, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2341, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2342, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2343, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2344, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2345, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2346, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2347, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2348, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2349, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2350, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2351, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2352, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2353, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2354, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2355, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2356, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2357, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2358, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2359, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2360, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2361, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2362, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2363, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2364, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2365, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2366, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2367, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2368, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2369, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2370, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2371, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2372, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2373, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2374, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2375, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2376, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2377, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2378, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2379, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2380, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2381, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2382, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2383, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2384, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2385, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2386, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2387, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2388, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2389, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2390, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2391, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2392, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2393, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2394, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2395, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2396, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2397, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2398, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2399, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2400, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2401, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2402, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2403, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2404, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2405, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2406, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2407, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2408, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2409, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2410, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2411, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2412, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2413, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2414, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2415, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2416, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2417, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2418, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2419, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2420, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2421, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2422, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2423, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2424, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2425, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2426, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2427, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2428, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2429, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2430, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2431, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2432, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2433, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2434, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2435, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2436, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2437, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2438, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2439, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2440, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2441, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2442, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2443, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2444, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2445, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2446, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2447, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2448, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2449, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2450, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2451, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2452, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2453, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2454, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2455, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2456, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2457, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2458, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2459, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2460, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2461, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2462, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2463, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2464, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2465, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2466, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2467, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2468, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2469, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2470, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2471, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2472, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2473, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2474, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2475, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2476, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2477, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2478, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2479, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2480, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2481, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2482, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2483, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2484, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2485, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2486, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2487, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2488, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2489, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2490, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2491, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2492, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2493, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2494, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2495, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2496, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2497, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2498, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2499, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2500, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2501, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2502, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2503, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2504, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2505, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2506, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2507, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2508, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2509, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2510, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2511, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2512, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2513, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2514, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2515, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2516, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2517, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2518, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2519, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2520, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2521, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2522, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2523, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2524, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2525, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2526, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2527, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2528, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2529, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2530, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2531, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2532, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2533, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2534, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2535, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2536, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2537, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2538, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2539, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2540, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2541, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2542, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2543, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2544, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2545, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2546, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2547, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2548, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2549, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2550, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2551, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2552, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2553, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2554, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2555, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2556, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2557, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2558, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2559, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2560, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2561, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2562, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2563, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2564, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2565, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2566, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2567, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2568, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2569, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2570, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2571, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2572, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2573, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2574, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2575, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2576, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2577, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2578, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2579, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2580, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2581, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2582, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2583, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2584, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2585, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2586, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2587, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2588, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2589, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2590, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2591, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2592, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2593, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2594, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2595, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2596, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2597, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2598, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2599, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2600, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2601, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2602, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2603, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2604, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2605, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2606, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2607, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2608, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2609, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2610, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2611, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2612, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2613, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2614, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2615, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2616, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2617, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2618, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2619, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2620, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2621, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2622, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2623, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2624, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2625, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2626, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2627, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2628, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2629, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2630, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2631, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2632, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2633, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2634, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2635, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2636, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2637, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2638, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2639, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2640, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2641, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2642, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2643, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2644, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2645, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2646, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2647, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2648, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2649, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2650, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2651, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2652, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2653, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2654, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2655, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2656, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2657, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2658, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2659, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2660, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2661, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2662, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2663, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2664, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2665, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2666, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2667, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2668, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2669, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2670, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2671, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2672, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2673, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2674, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2675, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2676, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2677, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2678, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2679, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2680, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2681, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2682, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2683, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2684, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2685, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2686, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2687, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2688, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2689, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2690, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2691, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2692, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2693, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2694, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2695, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2696, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2697, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2698, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2699, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2700, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2701, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2702, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2703, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2704, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2705, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2706, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2707, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2708, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2709, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2710, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2711, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2712, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2713, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2714, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2715, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2716, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2717, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2718, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2719, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2720, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2721, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2722, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2723, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2724, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2725, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2726, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2727, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2728, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2729, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2730, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2731, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2732, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2733, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2734, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2735, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2736, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2737, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2738, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2739, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2740, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2741, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2742, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2743, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2744, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2745, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2746, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2747, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2748, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2749, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2750, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2751, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2752, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2753, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2754, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2755, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2756, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2757, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2758, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2759, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2760, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2761, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2762, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2763, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2764, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2765, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2766, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2767, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2768, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2769, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2770, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2771, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2772, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2773, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2774, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2775, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2776, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2777, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2778, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2779, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2780, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2781, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2782, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2783, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2784, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2785, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2786, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2787, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2788, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2789, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2790, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2791, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2792, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2793, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2794, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2795, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2796, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2797, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2798, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2799, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2800, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2801, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2802, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2803, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2804, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2805, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2806, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2807, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2808, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2809, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2810, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2811, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2812, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2813, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2814, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2815, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2816, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2817, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2818, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2819, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2820, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2821, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2822, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2823, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2824, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2825, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2826, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2827, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2828, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2829, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2830, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2831, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2832, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2833, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2834, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2835, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2836, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2837, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2838, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2839, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2840, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2841, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2842, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2843, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2844, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2845, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2846, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2847, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2848, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2849, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2850, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2851, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2852, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2853, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2854, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2855, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2856, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2857, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2858, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2859, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2860, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2861, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2862, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2863, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2864, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2865, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2866, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2867, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2868, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2869, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2870, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2871, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2872, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2873, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2874, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2875, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2876, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2877, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2878, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2879, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2880, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2881, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2882, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2883, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2884, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2885, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2886, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2887, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2888, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2889, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2890, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2891, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2892, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2893, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2894, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2895, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2896, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2897, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2898, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2899, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2900, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2901, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2902, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2903, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2904, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2905, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2906, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2907, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2908, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2909, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2910, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2911, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2912, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2913, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2914, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2915, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2916, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2917, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2918, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2919, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2920, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2921, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2922, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2923, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2924, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2925, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2926, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2927, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2928, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2929, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2930, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2931, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2932, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2933, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2934, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2935, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2936, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2937, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2938, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2939, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2940, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2941, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2942, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2943, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2944, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2945, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2946, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2947, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2948, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2949, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2950, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2951, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2952, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2953, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2954, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2955, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2956, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2957, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2958, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2959, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2960, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2961, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2962, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2963, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2964, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2965, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2966, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2967, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2968, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2969, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2970, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2971, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2972, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2973, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2974, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2975, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2976, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2977, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2978, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2979, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2980, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2981, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2982, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2983, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2984, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2985, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2986, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2987, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2988, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2989, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2990, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2991, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2992, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2993, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2994, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2995, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2996, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2997, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 2998, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 2999, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3000, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3001, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3002, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3003, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3004, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3005, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3006, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3007, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3008, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3009, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3010, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3011, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3012, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3013, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3014, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3015, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3016, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3017, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3018, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3019, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3020, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3021, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3022, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3023, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3024, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3025, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3026, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3027, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3028, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3029, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3030, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3031, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3032, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3033, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3034, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3035, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3036, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3037, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3038, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3039, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3040, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3041, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3042, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3043, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3044, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3045, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3046, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3047, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3048, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3049, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3050, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3051, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3052, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3053, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3054, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3055, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3056, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3057, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3058, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3059, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3060, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3061, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3062, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3063, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3064, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3065, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3066, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3067, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3068, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3069, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3070, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3071, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3072, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3073, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3074, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3075, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3076, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3077, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3078, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3079, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3080, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3081, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3082, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3083, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3084, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3085, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3086, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3087, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3088, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3089, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3090, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3091, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3092, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3093, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3094, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3095, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3096, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3097, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3098, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3099, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3100, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3101, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3102, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3103, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3104, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3105, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3106, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3107, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3108, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3109, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3110, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3111, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3112, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3113, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3114, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3115, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3116, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3117, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3118, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3119, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3120, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3121, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3122, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3123, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3124, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3125, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3126, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3127, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3128, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3129, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3130, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3131, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3132, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3133, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3134, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3135, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3136, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3137, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3138, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3139, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3140, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3141, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3142, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3143, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3144, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3145, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3146, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3147, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3148, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3149, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3150, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3151, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3152, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3153, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3154, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3155, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3156, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3157, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3158, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3159, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3160, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3161, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3162, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3163, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3164, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3165, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3166, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3167, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3168, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3169, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3170, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3171, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3172, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3173, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3174, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3175, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3176, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3177, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3178, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3179, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3180, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3181, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3182, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3183, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3184, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3185, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3186, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3187, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3188, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3189, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3190, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3191, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3192, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3193, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3194, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3195, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3196, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3197, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3198, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3199, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3200, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3201, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3202, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3203, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3204, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3205, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3206, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3207, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3208, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3209, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3210, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3211, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3212, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3213, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3214, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3215, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3216, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3217, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3218, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3219, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3220, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3221, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3222, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3223, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3224, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3225, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3226, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3227, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3228, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3229, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3230, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3231, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3232, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3233, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3234, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3235, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3236, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3237, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3238, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3239, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3240, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3241, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3242, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3243, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3244, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3245, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3246, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3247, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3248, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3249, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3250, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3251, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3252, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3253, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3254, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3255, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3256, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3257, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3258, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3259, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3260, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3261, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3262, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3263, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3264, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3265, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3266, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3267, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3268, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3269, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3270, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3271, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3272, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3273, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3274, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3275, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3276, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3277, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3278, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3279, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3280, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3281, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3282, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3283, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3284, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3285, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3286, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3287, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3288, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3289, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3290, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3291, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3292, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3293, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3294, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3295, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3296, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3297, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3298, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3299, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3300, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3301, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3302, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3303, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3304, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3305, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3306, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3307, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3308, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3309, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3310, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3311, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3312, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3313, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3314, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3315, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3316, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3317, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3318, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3319, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3320, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3321, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3322, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3323, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3324, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3325, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3326, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3327, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3328, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3329, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3330, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3331, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3332, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3333, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3334, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3335, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3336, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3337, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3338, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3339, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3340, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3341, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3342, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3343, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3344, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3345, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3346, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3347, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3348, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3349, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3350, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3351, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3352, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3353, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3354, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3355, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3356, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3357, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3358, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3359, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3360, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3361, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3362, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3363, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3364, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3365, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3366, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3367, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3368, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3369, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3370, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3371, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3372, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3373, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3374, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3375, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3376, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3377, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3378, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3379, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3380, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3381, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3382, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3383, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3384, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3385, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3386, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3387, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3388, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3389, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3390, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3391, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3392, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3393, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3394, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3395, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3396, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3397, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3398, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3399, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3400, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3401, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3402, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3403, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3404, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3405, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3406, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3407, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3408, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3409, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3410, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3411, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3412, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3413, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3414, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3415, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3416, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3417, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3418, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3419, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3420, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3421, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3422, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3423, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3424, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3425, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3426, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3427, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3428, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3429, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3430, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3431, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3432, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3433, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3434, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3435, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3436, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3437, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3438, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3439, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3440, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3441, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3442, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3443, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3444, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3445, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3446, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3447, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3448, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3449, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3450, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3451, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3452, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3453, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3454, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3455, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3456, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3457, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3458, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3459, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3460, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3461, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3462, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3463, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3464, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3465, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3466, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3467, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3468, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3469, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3470, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3471, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3472, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3473, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3474, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3475, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3476, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3477, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3478, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3479, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3480, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3481, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3482, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3483, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3484, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3485, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3486, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3487, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3488, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3489, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3490, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3491, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3492, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3493, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3494, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3495, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3496, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3497, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3498, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3499, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3500, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3501, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3502, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3503, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3504, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3505, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3506, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3507, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3508, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3509, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3510, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3511, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3512, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3513, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3514, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3515, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3516, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3517, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3518, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3519, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3520, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3521, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3522, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3523, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3524, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3525, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3526, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3527, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3528, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3529, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3530, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3531, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3532, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3533, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3534, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3535, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3536, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3537, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3538, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3539, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3540, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3541, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3542, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3543, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3544, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3545, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3546, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3547, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3548, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3549, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3550, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3551, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3552, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3553, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3554, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3555, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3556, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3557, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3558, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3559, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3560, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3561, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3562, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3563, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3564, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3565, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3566, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3567, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3568, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3569, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3570, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3571, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3572, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3573, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3574, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3575, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3576, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3577, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3578, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3579, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3580, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3581, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3582, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3583, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3584, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3585, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3586, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3587, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3588, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3589, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3590, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3591, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3592, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3593, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3594, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3595, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3596, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3597, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3598, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3599, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3600, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3601, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3602, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3603, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3604, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3605, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3606, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3607, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3608, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3609, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3610, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3611, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3612, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3613, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3614, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3615, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3616, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3617, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3618, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3619, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3620, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3621, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3622, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3623, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3624, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3625, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3626, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3627, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3628, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3629, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3630, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3631, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3632, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3633, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3634, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3635, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3636, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3637, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3638, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3639, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3640, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3641, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3642, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3643, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3644, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3645, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3646, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3647, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3648, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3649, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3650, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3651, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3652, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3653, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3654, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3655, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3656, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3657, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3658, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3659, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3660, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3661, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3662, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3663, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3664, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3665, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3666, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3667, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3668, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3669, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3670, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3671, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3672, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3673, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3674, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3675, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3676, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3677, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3678, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3679, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3680, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3681, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3682, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3683, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3684, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3685, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3686, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3687, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3688, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3689, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3690, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3691, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3692, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3693, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3694, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3695, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3696, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3697, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3698, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3699, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3700, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3701, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3702, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3703, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3704, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3705, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3706, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3707, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3708, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3709, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3710, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3711, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3712, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3713, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3714, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3715, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3716, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3717, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3718, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3719, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3720, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3721, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3722, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3723, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3724, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3725, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3726, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3727, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3728, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3729, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3730, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3731, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3732, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3733, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3734, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3735, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3736, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3737, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3738, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3739, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3740, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3741, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3742, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3743, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3744, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3745, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3746, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3747, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3748, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3749, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3750, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3751, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3752, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3753, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3754, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3755, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3756, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3757, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3758, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3759, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3760, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3761, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3762, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3763, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3764, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3765, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3766, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3767, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3768, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3769, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3770, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3771, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3772, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3773, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3774, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3775, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3776, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3777, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3778, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3779, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3780, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3781, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3782, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3783, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3784, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3785, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3786, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3787, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3788, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3789, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3790, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3791, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3792, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3793, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3794, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3795, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3796, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3797, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3798, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3799, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3800, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3801, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3802, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3803, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3804, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3805, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3806, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3807, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3808, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3809, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3810, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3811, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3812, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3813, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3814, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3815, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3816, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3817, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3818, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3819, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3820, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3821, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3822, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3823, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3824, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3825, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3826, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3827, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3828, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3829, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3830, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3831, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3832, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3833, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3834, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3835, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3836, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3837, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3838, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3839, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3840, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3841, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3842, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3843, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3844, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3845, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3846, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3847, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3848, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3849, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3850, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3851, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3852, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3853, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3854, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3855, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3856, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3857, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3858, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3859, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3860, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3861, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3862, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3863, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3864, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3865, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3866, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3867, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3868, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3869, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3870, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3871, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3872, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3873, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3874, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3875, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3876, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3877, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3878, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3879, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3880, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3881, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3882, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3883, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3884, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3885, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3886, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3887, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3888, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3889, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3890, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3891, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3892, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3893, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3894, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3895, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3896, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3897, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3898, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3899, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3900, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3901, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3902, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3903, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3904, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3905, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3906, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3907, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3908, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3909, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3910, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3911, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3912, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3913, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3914, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3915, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3916, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3917, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3918, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3919, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3920, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3921, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3922, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3923, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3924, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3925, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3926, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3927, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3928, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3929, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3930, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3931, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3932, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3933, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3934, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3935, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3936, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3937, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3938, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3939, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3940, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3941, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3942, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3943, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3944, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3945, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3946, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3947, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3948, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3949, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3950, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3951, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3952, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3953, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3954, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3955, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3956, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3957, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3958, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3959, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3960, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3961, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3962, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3963, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3964, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3965, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3966, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3967, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3968, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3969, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3970, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3971, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3972, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3973, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3974, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3975, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3976, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3977, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3978, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3979, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3980, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3981, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3982, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3983, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3984, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3985, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3986, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3987, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3988, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3989, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3990, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3991, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3992, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3993, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3994, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3995, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3996, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3997, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 3998, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 3999, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4000, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4001, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4002, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4003, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4004, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4005, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4006, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4007, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4008, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4009, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4010, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4011, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4012, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4013, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4014, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4015, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4016, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4017, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4018, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4019, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4020, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4021, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4022, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4023, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4024, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4025, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4026, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4027, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4028, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4029, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4030, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4031, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4032, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4033, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4034, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4035, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4036, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4037, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4038, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4039, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4040, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4041, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4042, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4043, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4044, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4045, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4046, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4047, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4048, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4049, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4050, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4051, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4052, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4053, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4054, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4055, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4056, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4057, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4058, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4059, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4060, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4061, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4062, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4063, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4064, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4065, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4066, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4067, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4068, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4069, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4070, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4071, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4072, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4073, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4074, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4075, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4076, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4077, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4078, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4079, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4080, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4081, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4082, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4083, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4084, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4085, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4086, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4087, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4088, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4089, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4090, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4091, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4092, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4093, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4094, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4095, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4096, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4097, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4098, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4099, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4100, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4101, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4102, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4103, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4104, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4105, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4106, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4107, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4108, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4109, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4110, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4111, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4112, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4113, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4114, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4115, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4116, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4117, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4118, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4119, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4120, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4121, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4122, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4123, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4124, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4125, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4126, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4127, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4128, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4129, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4130, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4131, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4132, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4133, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4134, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4135, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4136, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4137, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4138, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4139, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4140, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4141, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4142, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4143, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4144, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4145, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4146, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4147, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4148, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4149, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4150, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4151, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4152, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4153, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4154, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4155, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4156, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4157, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4158, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4159, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4160, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4161, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4162, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4163, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4164, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4165, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4166, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4167, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4168, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4169, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4170, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4171, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4172, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4173, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4174, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4175, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4176, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4177, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4178, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4179, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4180, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4181, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4182, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4183, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4184, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4185, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4186, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4187, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4188, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4189, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4190, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4191, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4192, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4193, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4194, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4195, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4196, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4197, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4198, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4199, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4200, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4201, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4202, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4203, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4204, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4205, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4206, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4207, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4208, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4209, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4210, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4211, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4212, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4213, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4214, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4215, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4216, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4217, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4218, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4219, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4220, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4221, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4222, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4223, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4224, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4225, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4226, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4227, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4228, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4229, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4230, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4231, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4232, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4233, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4234, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4235, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4236, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4237, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4238, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4239, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4240, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4241, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4242, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4243, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4244, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4245, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4246, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4247, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4248, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4249, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4250, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4251, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4252, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4253, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4254, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4255, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4256, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4257, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4258, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4259, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4260, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4261, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4262, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4263, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4264, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4265, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4266, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4267, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4268, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4269, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4270, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4271, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4272, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4273, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4274, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4275, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4276, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4277, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4278, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4279, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4280, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4281, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4282, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4283, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4284, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4285, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4286, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4287, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4288, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4289, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4290, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4291, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4292, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4293, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4294, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4295, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4296, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4297, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4298, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4299, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4300, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4301, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4302, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4303, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4304, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4305, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4306, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4307, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4308, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4309, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4310, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4311, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4312, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4313, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4314, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4315, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4316, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4317, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4318, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4319, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4320, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4321, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4322, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4323, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4324, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4325, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4326, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4327, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4328, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4329, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4330, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4331, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4332, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4333, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4334, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4335, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4336, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4337, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4338, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4339, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4340, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4341, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4342, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4343, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4344, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4345, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4346, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4347, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4348, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4349, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4350, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4351, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4352, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4353, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4354, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4355, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4356, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4357, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4358, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4359, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4360, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4361, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4362, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4363, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4364, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4365, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4366, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4367, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4368, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4369, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4370, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4371, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4372, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4373, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4374, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4375, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4376, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4377, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4378, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4379, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4380, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4381, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4382, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4383, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4384, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4385, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4386, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4387, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4388, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4389, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4390, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4391, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4392, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4393, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4394, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4395, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4396, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4397, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4398, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4399, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4400, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4401, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4402, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4403, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4404, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4405, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4406, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4407, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4408, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4409, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4410, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4411, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4412, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4413, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4414, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4415, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4416, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4417, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4418, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4419, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4420, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4421, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4422, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4423, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4424, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4425, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4426, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4427, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4428, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4429, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4430, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4431, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4432, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4433, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4434, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4435, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4436, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4437, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4438, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4439, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4440, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4441, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4442, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4443, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4444, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4445, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4446, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4447, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4448, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4449, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4450, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4451, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4452, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4453, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4454, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4455, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4456, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4457, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4458, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4459, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4460, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4461, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4462, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4463, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4464, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4465, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4466, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4467, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4468, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4469, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4470, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4471, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4472, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4473, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4474, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4475, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4476, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4477, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4478, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4479, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4480, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4481, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4482, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4483, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4484, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4485, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4486, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4487, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4488, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4489, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4490, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4491, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4492, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4493, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4494, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4495, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4496, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4497, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4498, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4499, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4500, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4501, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4502, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4503, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4504, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4505, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4506, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4507, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4508, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4509, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4510, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4511, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4512, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4513, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4514, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4515, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4516, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4517, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4518, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4519, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4520, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4521, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4522, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4523, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4524, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4525, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4526, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4527, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4528, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4529, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4530, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4531, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4532, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4533, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4534, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4535, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4536, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4537, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4538, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4539, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4540, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4541, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4542, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4543, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4544, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4545, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4546, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4547, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4548, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4549, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4550, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4551, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4552, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4553, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4554, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4555, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4556, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4557, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4558, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4559, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4560, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4561, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4562, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4563, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4564, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4565, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4566, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4567, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4568, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4569, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4570, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4571, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4572, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4573, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4574, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4575, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4576, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4577, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4578, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4579, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4580, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4581, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4582, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4583, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4584, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4585, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4586, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4587, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4588, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4589, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4590, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4591, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4592, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4593, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4594, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4595, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4596, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4597, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4598, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4599, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4600, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4601, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4602, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4603, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4604, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4605, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4606, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4607, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4608, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4609, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4610, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4611, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4612, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4613, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4614, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4615, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4616, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4617, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4618, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4619, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4620, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4621, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4622, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4623, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4624, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4625, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4626, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4627, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4628, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4629, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4630, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4631, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4632, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4633, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4634, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4635, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4636, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4637, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4638, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4639, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4640, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4641, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4642, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4643, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4644, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4645, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4646, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4647, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4648, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4649, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4650, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4651, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4652, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4653, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4654, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4655, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4656, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4657, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4658, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4659, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4660, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4661, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4662, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4663, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4664, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4665, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4666, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4667, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4668, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4669, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4670, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4671, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4672, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4673, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4674, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4675, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4676, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4677, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4678, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4679, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4680, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4681, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4682, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4683, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4684, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4685, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4686, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4687, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4688, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4689, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4690, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4691, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4692, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4693, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4694, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4695, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4696, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4697, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4698, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4699, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4700, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4701, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4702, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4703, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4704, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4705, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4706, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4707, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4708, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4709, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4710, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4711, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4712, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4713, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4714, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4715, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4716, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4717, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4718, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4719, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4720, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4721, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4722, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4723, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4724, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4725, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4726, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4727, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4728, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4729, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4730, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4731, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4732, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4733, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4734, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4735, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4736, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4737, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4738, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4739, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4740, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4741, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4742, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4743, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4744, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4745, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4746, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4747, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4748, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4749, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4750, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4751, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4752, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4753, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4754, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4755, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4756, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4757, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4758, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4759, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4760, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4761, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4762, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4763, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4764, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4765, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4766, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4767, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4768, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4769, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4770, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4771, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4772, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4773, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4774, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4775, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4776, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4777, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4778, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4779, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4780, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4781, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4782, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4783, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4784, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4785, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4786, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4787, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4788, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4789, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4790, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4791, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4792, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4793, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4794, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4795, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4796, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4797, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4798, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4799, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4800, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4801, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4802, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4803, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4804, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4805, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4806, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4807, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4808, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4809, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4810, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4811, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4812, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4813, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4814, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4815, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4816, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4817, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4818, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4819, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4820, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4821, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4822, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4823, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4824, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4825, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4826, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4827, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4828, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4829, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4830, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4831, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4832, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4833, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4834, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4835, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4836, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4837, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4838, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4839, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4840, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4841, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4842, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4843, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4844, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4845, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4846, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4847, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4848, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4849, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4850, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4851, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4852, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4853, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4854, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4855, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4856, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4857, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4858, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4859, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4860, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4861, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4862, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4863, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4864, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4865, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4866, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4867, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4868, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4869, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4870, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4871, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4872, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4873, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4874, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4875, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4876, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4877, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4878, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4879, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4880, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4881, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4882, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4883, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4884, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4885, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4886, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4887, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4888, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4889, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4890, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4891, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4892, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4893, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4894, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4895, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4896, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4897, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4898, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4899, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4900, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4901, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4902, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4903, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4904, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4905, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4906, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4907, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4908, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4909, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4910, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4911, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4912, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4913, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4914, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4915, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4916, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4917, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4918, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4919, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4920, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4921, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4922, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4923, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4924, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4925, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4926, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4927, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4928, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4929, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4930, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4931, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4932, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4933, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4934, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4935, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4936, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4937, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4938, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4939, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4940, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4941, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4942, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4943, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4944, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4945, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4946, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4947, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4948, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4949, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4950, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4951, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4952, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4953, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4954, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4955, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4956, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4957, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4958, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4959, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4960, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4961, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4962, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4963, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4964, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4965, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4966, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4967, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4968, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4969, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4970, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4971, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4972, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4973, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4974, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4975, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4976, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4977, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4978, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4979, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4980, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4981, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4982, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4983, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4984, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4985, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4986, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4987, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4988, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4989, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4990, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4991, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4992, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4993, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4994, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4995, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4996, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4997, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 4998, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n",
      "Epoch 4999, Loss 2.927647, Params tensor([  5.3677, -17.3046]), Grad tensor([-8.2254e-06,  2.5451e-05])\n",
      "Epoch 5000, Loss 2.927644, Params tensor([  5.3677, -17.3046]), Grad tensor([1.9848e-05, 3.0577e-05])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 2500,\n",
    "    learning_rate = 3e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un, t_cel = t_cel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21970bc7190>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x2400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_p = model(t_un, *params)      # 技巧: 变量解包 *var 能分离出其内部的变量\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel('Temperature (Fahrenheit)')\n",
    "plt.ylabel('Temperature (Celsius)')\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_cel.numpy(), 'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('disco-diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d154572db74bf447d8ef00b8088999b75aecc8ee0cff4dd3dcc88c51a55f698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

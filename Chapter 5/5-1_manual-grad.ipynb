{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorded temperature data, c - celsius, u - unknown\n",
    "t_cel = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_un = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_cel = torch.tensor(t_cel)\n",
    "t_un = torch.tensor(t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear model\n",
    "def model(t_un, w, b):\n",
    "    t_est = w * t_un + b\n",
    "    return t_est\n",
    "\n",
    "# use square loss function\n",
    "def loss_fn(t_cel, t_est):\n",
    "    sqr_dif = (t_est - t_cel)**2\n",
    "    return sqr_dif.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_est = model(t_un, w, b)\n",
    "loss = loss_fn(t_cel, t_est)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2964,   82.6000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain rule of gradient\n",
    "def dloss_dmodel(t_cel, t_est):\n",
    "    dsqr_dif = (t_est - t_cel) / t_est.size(0) * 2  # 均值的导数, 因为后面return [dloss_dw.sum(), dloss_db.sum()]\n",
    "    return dsqr_dif\n",
    "def dmodel_w(t_un, w, b):\n",
    "    return t_un\n",
    "def dmodel_b(t_un, w, b):\n",
    "    return 1.0\n",
    "\n",
    "def grad_fn(t_un, t_cel, t_est, w, b):\n",
    "    dloss_dmodel = dloss_dmodel(t_cel, t_est)\n",
    "    dloss_dw = dloss_dmodel * dmodel_w(t_un, w, b)\n",
    "    dloss_db = dloss_dmodel * dmodel_b(t_un, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "grad_fn(t_un, t_cel, t_est, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代以适应模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_un, t_cel):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params   # input tensor[w, b]\n",
    "        t_est = model(t_un, w, b)\n",
    "        loss = loss_fn(t_cel, t_est)\n",
    "        grad = grad_fn(t_un, t_cel, t_est, w, b)\n",
    "        params = params - learning_rate * grad\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "Epoch 2, Loss 1044.412231\n",
      "Epoch 3, Loss 623.328979\n",
      "Epoch 4, Loss 376.882294\n",
      "Epoch 5, Loss 232.644302\n",
      "Epoch 6, Loss 148.225693\n",
      "Epoch 7, Loss 98.817238\n",
      "Epoch 8, Loss 69.899025\n",
      "Epoch 9, Loss 52.973038\n",
      "Epoch 10, Loss 43.065723\n",
      "Epoch 11, Loss 37.266159\n",
      "Epoch 12, Loss 33.870735\n",
      "Epoch 13, Loss 31.882368\n",
      "Epoch 14, Loss 30.717512\n",
      "Epoch 15, Loss 30.034624\n",
      "Epoch 16, Loss 29.633808\n",
      "Epoch 17, Loss 29.398085\n",
      "Epoch 18, Loss 29.258989\n",
      "Epoch 19, Loss 29.176441\n",
      "Epoch 20, Loss 29.126989\n",
      "Epoch 21, Loss 29.096901\n",
      "Epoch 22, Loss 29.078163\n",
      "Epoch 23, Loss 29.066053\n",
      "Epoch 24, Loss 29.057833\n",
      "Epoch 25, Loss 29.051880\n",
      "Epoch 26, Loss 29.047260\n",
      "Epoch 27, Loss 29.043415\n",
      "Epoch 28, Loss 29.040031\n",
      "Epoch 29, Loss 29.036909\n",
      "Epoch 30, Loss 29.033949\n",
      "Epoch 31, Loss 29.031073\n",
      "Epoch 32, Loss 29.028259\n",
      "Epoch 33, Loss 29.025471\n",
      "Epoch 34, Loss 29.022705\n",
      "Epoch 35, Loss 29.019945\n",
      "Epoch 36, Loss 29.017195\n",
      "Epoch 37, Loss 29.014446\n",
      "Epoch 38, Loss 29.011707\n",
      "Epoch 39, Loss 29.008961\n",
      "Epoch 40, Loss 29.006220\n",
      "Epoch 41, Loss 29.003487\n",
      "Epoch 42, Loss 29.000744\n",
      "Epoch 43, Loss 28.998003\n",
      "Epoch 44, Loss 28.995264\n",
      "Epoch 45, Loss 28.992525\n",
      "Epoch 46, Loss 28.989784\n",
      "Epoch 47, Loss 28.987047\n",
      "Epoch 48, Loss 28.984308\n",
      "Epoch 49, Loss 28.981573\n",
      "Epoch 50, Loss 28.978832\n",
      "Epoch 51, Loss 28.976099\n",
      "Epoch 52, Loss 28.973360\n",
      "Epoch 53, Loss 28.970625\n",
      "Epoch 54, Loss 28.967888\n",
      "Epoch 55, Loss 28.965151\n",
      "Epoch 56, Loss 28.962416\n",
      "Epoch 57, Loss 28.959686\n",
      "Epoch 58, Loss 28.956947\n",
      "Epoch 59, Loss 28.954212\n",
      "Epoch 60, Loss 28.951479\n",
      "Epoch 61, Loss 28.948742\n",
      "Epoch 62, Loss 28.946012\n",
      "Epoch 63, Loss 28.943275\n",
      "Epoch 64, Loss 28.940546\n",
      "Epoch 65, Loss 28.937813\n",
      "Epoch 66, Loss 28.935076\n",
      "Epoch 67, Loss 28.932348\n",
      "Epoch 68, Loss 28.929613\n",
      "Epoch 69, Loss 28.926882\n",
      "Epoch 70, Loss 28.924150\n",
      "Epoch 71, Loss 28.921419\n",
      "Epoch 72, Loss 28.918690\n",
      "Epoch 73, Loss 28.915956\n",
      "Epoch 74, Loss 28.913231\n",
      "Epoch 75, Loss 28.910498\n",
      "Epoch 76, Loss 28.907768\n",
      "Epoch 77, Loss 28.905035\n",
      "Epoch 78, Loss 28.902311\n",
      "Epoch 79, Loss 28.899580\n",
      "Epoch 80, Loss 28.896852\n",
      "Epoch 81, Loss 28.894121\n",
      "Epoch 82, Loss 28.891396\n",
      "Epoch 83, Loss 28.888666\n",
      "Epoch 84, Loss 28.885942\n",
      "Epoch 85, Loss 28.883211\n",
      "Epoch 86, Loss 28.880487\n",
      "Epoch 87, Loss 28.877758\n",
      "Epoch 88, Loss 28.875036\n",
      "Epoch 89, Loss 28.872307\n",
      "Epoch 90, Loss 28.869581\n",
      "Epoch 91, Loss 28.866854\n",
      "Epoch 92, Loss 28.864130\n",
      "Epoch 93, Loss 28.861403\n",
      "Epoch 94, Loss 28.858681\n",
      "Epoch 95, Loss 28.855955\n",
      "Epoch 96, Loss 28.853233\n",
      "Epoch 97, Loss 28.850506\n",
      "Epoch 98, Loss 28.847784\n",
      "Epoch 99, Loss 28.845057\n",
      "Epoch 100, Loss 28.842335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2338, -0.1041])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 3e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_un = t_un, t_cel = t_cel\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('disco-diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d154572db74bf447d8ef00b8088999b75aecc8ee0cff4dd3dcc88c51a55f698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

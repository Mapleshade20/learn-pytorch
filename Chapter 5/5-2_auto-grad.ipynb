{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorded temperature data, c - celsius, u - unknown\n",
    "t_cel = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_cel = torch.tensor(t_cel)\n",
    "t_u = torch.tensor(t_u)\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用自动求导, 使用 grad 属性\n",
    "\n",
    "requires_grad=True 将使系统跟踪params张量进行操作后产生的所有张量的系谱树, 它们对params的导数将自动填充为params张量的grad属性."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear model\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "# use square loss function\n",
    "def loss_fn(t_cel, t_est):\n",
    "    sqr_dif = (t_est - t_cel)**2\n",
    "    return sqr_dif.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize gradient-required parameters\n",
    "params = torch.tensor([1., 0.], requires_grad=True)\n",
    "\n",
    "t_est = model(t_u, *params)\n",
    "loss = loss_fn(t_cel, t_est)\n",
    "\n",
    "loss.backward()     # 调用backward()的是待导函数\n",
    "\n",
    "params.grad     # 系统将进行反向遍历, 计算待导函数对requires_grad者的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 每次迭代需要归零梯度\n",
    "\n",
    "注意! 每次反向传播, 系统将把求得的导数**累加**在grad上. 为防止这种情况, 需要在每次迭代时明确地将梯度归零."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_cel):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_est = model(t_u, *params)\n",
    "        loss = loss_fn(t_cel, t_est)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        if epoch % 500 == 0 or epoch == 1:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 500, Loss 3.091884\n",
      "Epoch 1000, Loss 2.928639\n",
      "Epoch 1500, Loss 2.927653\n",
      "Epoch 2000, Loss 2.927646\n",
      "Epoch 2500, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3677, -17.3046], requires_grad=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 2500,\n",
    "    learning_rate = 3e-2,\n",
    "    params = torch.tensor([1., 0.], requires_grad=True),\n",
    "    t_u = t_un, t_cel = t_cel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用梯度下降优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# dir(optim)\n",
    "\n",
    "params = torch.tensor([1., 0.], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_est = model(t_un, *params)\n",
    "loss = loss_fn(t_cel, t_est)\n",
    "\n",
    "optimizer.zero_grad()       # 一定要归零梯度!\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()        # 已修改params\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3236, -17.0549], requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_cel):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_est = model(t_u, *params)\n",
    "        loss = loss_fn(t_cel, t_est)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0 or epoch <= 3:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1., 0.], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "training_loop(\n",
    "    n_epochs = 2500,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un, t_cel = t_cel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练, 验证, 防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集和验证集的索引\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集和验证集\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_cel = t_cel[train_indices]\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_cel = t_cel[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 72.7406, Validation loss 114.6712\n",
      "Epoch 2, Training loss 40.2682, Validation loss 46.4569\n",
      "Epoch 3, Training loss 33.9582, Validation loss 27.9884\n",
      "Epoch 500, Training loss 7.0645, Validation loss 6.8510\n",
      "Epoch 1000, Training loss 3.4611, Validation loss 4.2857\n",
      "Epoch 1500, Training loss 2.9574, Validation loss 3.5832\n",
      "Epoch 2000, Training loss 2.8870, Validation loss 3.3564\n",
      "Epoch 2500, Training loss 2.8772, Validation loss 3.2767\n",
      "Epoch 3000, Training loss 2.8758, Validation loss 3.2475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.2908, -16.8837], requires_grad=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_cel, val_t_cel):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_est = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_cel, train_t_est)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_est = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_cel, val_t_est)\n",
    "            assert val_loss.requires_grad == False      # if not True then throw an error\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0 or epoch <= 3:\n",
    "            print(f'Epoch {epoch}, Training loss {train_loss.item():.4f}, Validation loss {val_loss.item():.4f}')\n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1., 0.], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "training_loop(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un, val_t_u = val_t_un, train_t_cel = train_t_cel, val_t_cel = val_t_cel\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('disco-diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d154572db74bf447d8ef00b8088999b75aecc8ee0cff4dd3dcc88c51a55f698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
